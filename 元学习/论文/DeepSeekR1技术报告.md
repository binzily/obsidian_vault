DeepSeek 把这一代推理模型分成三类东西：

1. **DeepSeek-R1-Zero**：直接在 **DeepSeek-V3-Base** 上做大规模 RL，不先做 SFT（监督微调）。核心作用：证明“纯 RL 也能把推理能力拉起来”，但会出现可读性差、语言混杂、重复等问题。
    
2. **DeepSeek-R1**：在 Zero 的基础上，引入“冷启动（cold start）数据 + 两阶段 RL + 两阶段 SFT”，把**推理能力**和**可用性/对齐**一起做出来。
    
3. **蒸馏版 Distill 模型**：用 R1 生成的数据去微调更小的开源底座（Qwen、Llama），让小模型也学到大模型的推理模式。
    

另外，R1 本体是 **MoE（混合专家）**：模型卡给出的口径是 **总参数约 671B、激活参数约 37B、上下文长度 128K**。  
（你可以先简单理解：MoE 像“很多专家网络”，每个 token 只调用其中一小部分专家，所以“总参数很大但实际算力消耗更接近激活参数”。）