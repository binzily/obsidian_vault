### 1. ==说一下你对kl散度的理解？kl散度和交叉熵的区别和联系是什么？== 

统一符号 
$X$：离散随机变量，取值集合 $\mathcal{X}$ ，
$p(x)$：**真实分布**（现实/数据生成机制） ，
$q(x)$：**模型分布**（你用来近似真实的分布） ，
$\log_b$：以 $b$ 为底的对数（底数决定单位）

#### 1.1 自信息（Surprisal / 信息量）：先有“单次的惊讶” 
**定义**：某个结果 $x$ 的“惊讶度/信息量” $$ I_p(x) = -\log_b p(x) $$直觉：越不可能发生（$p(x)$ 越小），发生时越“意外”，信息量越大。维基百科也用这个观点来引出熵：把事件的信息量看成随机变量，取期望就是平均信息量（熵）。

#### 1.2 熵 $H(p)$：真实世界本身的“不确定性” 
**定义**（平均自信息）： $$ H(p)=\mathbb{E}_{x\sim p}\big[-\log_b p(x)\big]=-\sum_{x\in\mathcal{X}} p(x)\log_b p(x) $$ 它只由 $p$ 决定：描述“真实信源”平均有多难预测/多不确定。

#### 1.3 交叉熵 $H(p,q)$：真实是 $p$，但你用 $q$ 去“解释/编码” 
**定义**： $$ H(p,q)=\mathbb{E}_{x\sim p}\big[-\log_b q(x)\big]=-\sum_{x\in\mathcal{X}} p(x)\log_b q(x) $$直觉：数据是按 $p$ 产生的，但你用模型 $q$ 来给这些数据算“惊讶度”（$-\log q(x)$），再取平均——这就是**模型在真实数据上的平均“被打脸程度”**。
很重要：交叉熵**不对称**，一般 $H(p,q)\neq H(q,p)$。
 
#### 1.4 KL 散度 $D_{\mathrm{KL}}(p|q)$：模型错配带来的“额外代价” 
 **定义**： $$ D_{\mathrm{KL}}(p|q)=\sum_{x\in\mathcal{X}} p(x)\log \frac{p(x)}{q(x)} $$它衡量：用 $q$ 近似 $p$ 时“差了多少”。
 KL 有一个**超级重要的等价分解**： $$ D_{\mathrm{KL}}(p|q)=H(p,q)-H(p) $$ 也就是下面这条“总关系式”： $$ \boxed{H(p,q)=H(p)+D_{\mathrm{KL}}(p|q)} $$
* $H(p)$：真实世界的“基本难度”（你改不了，因为真实分布固定）
* $D_{\mathrm{KL}}(p|q)$：你模型选错导致的“额外难度/额外惊讶” 
* $H(p,q)$：两者相加＝你用模型 $q$ 在真实数据上实际承受的平均代价
因此： 
在固定 $p$ 时，**最小化交叉熵 $H(p,q)$ ⇔ 最小化 KL 散度 $D_{\mathrm{KL}}(p|q)$**（因为 $H(p)$ 与 $q$ 无关）。
#### 两个关键性质（理解“为什么它们合理”） 
A. 交叉熵永远不小于熵 $$ H(p,q)\ge H(p) $$ 等号当且仅当 $p=q$。这常被称为 Gibbs 不等式/严格正确性（properness）：真实是 $p$，你用别的 $q$ 总要多付代价。

B. KL 散度非负 $$ D_{\mathrm{KL}}(p|q)\ge 0 $$ 且为 0 当且仅当 $p=q$。

PS:对数底数为什么是“单位选择”？ 熵/交叉熵/KL 都是“$\log$”的期望或差。换底只会乘一个常数： $$ \log_b x=\frac{\log_k x}{\log_k b} $$ 所以数值整体缩放，最小值位置不变，但**单位变了**： 
* $b=2$：单位是 **bit** 
* $b=e$：单位是 **nat** 维基百科在熵的定义里也明确写了这一点。 
### 2. ==如何选取模型中的超参数，有什么办法？==

- **随机搜索**：为每个超参数定义一个**分布**，在预设的尝试次数内（比如100次），**每次都从这些分布中随机采样**一组超参数组合来进行训练和评估。

- **贝叶斯优化**：随机选择几组（比如3-5组）超参数 `x`，分别用这几组超参数完整地训练模型，得到对应的性能得分 `f(x)`。我们现在有了一个小小的初始数据集 `D = {(x₁, f(x₁)), (x₂, f(x₂)), ...}`。 使用当前所有的数据 `D`，拟合一个高斯过程模型：普通函数在每个输入点 `x` 只会输出一个确定的值 `y`。而高斯过程在每个输入点 `x` 输出的是一个**概率分布**——具体来说是一个**高斯（正态）分布**。这个分布由**均值和方差定义**。我们得到了一个覆盖整个超参数空间的“信念地图”（均值和方差）。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。更新，重复。

- 基于**早停**的方法：代表算法:是**Successive Halving** 和 **Hyperband**。随机生成大量的超参数组合。

  给所有组合分配一个**最小的资源**（比如只训练1个epoch）。淘汰掉表现最差的一半组合。给剩下的一半组合分配**更多的资源**（比如再训练2个epoch）。再淘汰掉一半...重复这个“分配资源-淘汰差生”的过程，直到只剩下一个“冠军”组合，并对其进行完整训练。

### 3. ==正则化==

**正则化防止过拟合**：让很多个特征都贡献一点点（拥有较小的权重），而不是让少数几个特征拥有巨大的权重，使得权重分散。

| 特性     | L2 正则化 (Ridge)  | L1 正则化 (Lasso)       |
| ------ | --------------- | -------------------- |
| 惩罚项公式  | λ * ∑ wᵢ² (平方和) | λ * ∑                |
| 对权重的影响 | 使权重变小，趋近于0      | 使权重变小，很多会精确等于0       |
| 模型稀疏性  | 否，权重通常是小的非零值    | 是，产生稀疏的权重矩阵          |
| 核心思想   | 鼓励权重分散、平滑       | 特征选择，保留最重要的特征        |
| 主要用途   | 通用的、首选的正则化方法    | 当特征数量巨大且怀疑很多特征是无用的时候 |

L1正则化：**惩罚是“一视同仁”的**：惩罚的力度与权重大小成正比。一个权重从 0.1 变为 0，和从 10.1 变为 10，所获得的“奖励”（惩罚的减少量）是相同的。

### 4. ==在参数初始化时，为什么不能全零初始化==

![500](assets/数学python+深度学习/file-20251204084206136.png)
全0初始化会一直是0；全1初始化网络变动会一直一样。可以把两层的连线看成权重W矩阵。
### 5. ==死亡ReLU==

在训练过程中，如果某一次梯度更新的幅度特别大（比如学习率设置得太高），可能会导致权重 `W` 和偏置 `b` 被更新成一组“糟糕”的值。这组“糟糕”的值，使得对于**之后所有输入到这个神经元的数据 `x`**，计算出的进入激活函数前的 `z` **始终小于0**。反向传播的梯度计算是**一路连乘**的。ReLU函数在负数区间的**导数（斜率）恰好为0**。

### 6. ==1×1卷积核的作用==

首先：一个卷积核，无论它的高和宽是多少（`1x1`, `3x3`, `5x5`），它的唯一使命就是：作为一个特征探测器，扫描整个输入数据，并将所有输入通道的信息“浓缩”成一张代表“该特征出现位置”的单通道输出图。对于多通道的图像，卷积核最后会把所有通道都加起来。**卷积核是3D的**，**一个卷积核卷完后得到的特征图是1D的**。

那么1×1卷积核的作用：

- **降维或者升维**。

- **增加非线性**：一个 1x1 的卷积，后面再接一个 ReLU，相当于在不改变特征图尺寸的情况下，对输入的每一个像素点进行了一次非线性的变换。

- **跨通道信息交互**：将输入的所有 `C` 个通道的值进行一次**加权求和**。

### 7. ==深度可分离卷积==

| 特性   | 标准卷积             | 深度可分离卷积                                      |
| ---- | ---------------- | -------------------------------------------- |
| 工作方式 | 一步到位，同时处理空间和通道   | 分两步走，先处理空间，再处理通道                             |
| 步骤   | 单一的标准卷积          | 1. 深度卷积 (Depthwise) 2. 逐点卷积 (Pointwise, 1x1) |
| 核心思想 | 耦合操作             | 解耦操作                                         |
| 主要优势 | 表达能力强            | 参数少，计算快，极其高效                                 |
| 代表模型 | VGG, ResNet (早期) | MobileNet, Xception, EfficientNet            |

### 8.==GAN是怎么训练的==

生成对抗网络(GAN)的训练过程包括两个部分:生成器的训练和判别器的训练。这两个部分交替进行,形成一
种“对抗”的训练过程。

**判别器**训练:首先,我们固定生成器的参数,训练判别器。具体来说,我们首先从真实数据集中采样一批真实样
本,然后从生成器中生成一批假样本。我们希望判别器能够正确地区分这两批样本,因此我们通过最大化判别器对
真实样本的输出和最小化判别器对假样本的输出来更新判别器的参数。

**生成器**训练:然后,我们固定判别器的参数,训练生成器。我们希望生成器能够生成能够“欺骗”判别器的假样
本,因此我们通过最大化判别器对生成器生成的假样本的输出来更新生成器的参数。

循环训练。

### 9.==word2vec提出了负采样策略，他的原理是什么，解决了什么样的问题？==

Word2Vec不是一个单一的算法，而是 Google 在 2013 年提出的一个**模型工具集**，其主要目的是将词语转换成向量。这种由模型产生的词语向量，通常被称为**词嵌入 (Word Embedding)**。

```
Word2Vec 怎么训练：

1. 输入： 也就是一对词（中心词，上下文词）。
    
2. 动作：
    
    - 去矩阵里查出这两个词的向量（一开始是乱数）。
        
    - 算这两个向量的内积（相似度）。
        
3. 目的：
    
    - 如果是真邻居，就拉近这两个向量（修改矩阵数值）。
        
    - 如果是假邻居（负采样），就推远这两个向量。
        
4. 产出： 训练结束后，那个被改了无数遍的矩阵（表格），就是我们梦寐以求的 Word2Vec 词向量。
```

Word2Vec 工具集中包含两种主要的训练模型：

- **CBOW (Continuous Bag-of-Words)**：**根据上下文词预测中心词**。比如，给出 `["the", "cat", "on", "the", "mat"]`，模型需要预测中间的词是 `sat`。
- **Skip-gram**: **根据中心词预测上下文词**。比如，给出中心词 `sat`，模型需要预测出周围的词可能是 `the`, `cat`, `on`, `the`, `mat`。Skip-gram 在处理罕见词时效果通常更好。

|**特性**|**Skip-Gram (中心 → 周围)**|**CBOW (周围 → 中心)**|
|---|---|---|
|**方向**|**一对多** (One-to-Many)|**多对一** (Many-to-One)|
|**逻辑**|给定当前词，猜它可能出现在什么环境里。|给定环境，猜中间缺了什么词。|
|**处理方式**|每个上下文词都独立产生误差，**独立更新**中心词。|上下文词先**求平均**，再一起去预测中心词。|
|**生僻词效果**|**更好**。因为每个样本都独立起作用，生僻词能被充分训练。|**较差**。因为上下文被平均了，生僻词的独特信息容易被淹没。|
|**训练速度**|**慢**。计算量大 (样本多)。|**快**。计算量小 (合并计算)。|
|**适用场景**|**大部分场景的首选**，特别是语料不够多时，或者对生僻词精度要求高时。|语料库超级巨大，且追求极速训练时。|

### 10.==如果让你设计一个命名实体识别任务，你会怎么设计？==

