## 1.Double DQN
Double DQN（Double Deep Q-Network）是对 DQN 的一个小改动，但能显著缓解 **Q 值“系统性高估（overestimation bias）”**，训练更稳、策略更靠谱。

DQN 的 TD 目标（target）通常是： $$y = r + \gamma \max_{a'} Q_{\text{target}}(s', a')$$ 如果 $Q(\cdot)$ 的估计里带噪声（神经网络几乎必然有），那么“max”会倾向挑到“被高估”的那个动作 —— 即便真实最优动作不是它。挑到哪个偏高的动作后，DQN是直接用这个动作的偏高Q值来更新，Double DQN也是用这个动作，但是不用这个Q值，用的是另一个网络估计的Q值，缓解“二次伤害”的问题。
- 真实训练里Q不是直接监督的，而是靠bootstrap：前一状态的值靠下一状态的max Q回传； 
- 下一状态动作多、估计有噪声时，max很容易挑中被高估动作；
- 这个偏高target会一路传回更早状态，造成策略偏向次优分支； 
- Double DQN用“两张相关但不完全相同的估计器”把“选”和“评”拆开，显著减少这种系统性高估。