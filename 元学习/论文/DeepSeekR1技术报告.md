## 0.R1 系列的“产品形态”

DeepSeek 把这一代推理模型分成三类东西：

1. **DeepSeek-R1-Zero**：直接在 **DeepSeek-V3-Base** 上做大规模 RL，不先做 SFT（监督微调）。核心作用：证明“纯 RL 也能把推理能力拉起来”，但会出现可读性差、语言混杂、重复等问题。
    
2. **DeepSeek-R1**：在 Zero 的基础上，引入“冷启动（cold start）数据 + 两阶段 RL + 两阶段 SFT”，把**推理能力**和**可用性/对齐**一起做出来。
    
3. **蒸馏版 Distill 模型**：用 R1 生成的数据去微调更小的开源底座（Qwen、Llama），让小模型也学到大模型的推理模式。
    
另外，R1 本体是 **MoE（混合专家）**：模型卡给出的口径是 **总参数约 671B、激活参数约 37B、上下文长度 128K**。  

## 1.这篇报告最核心的“训练路径”到底是什么

**DeepSeek-V3-Base**  
→ **（A）冷启动 Long CoT 的 SFT** → **R1 Dev-1**  
→ **（B）第一阶段 RL：规则奖励 + 语言一致性奖励** → **R1 Dev-2**  
→ **（C）拒绝采样（rejection sampling）造数据 + 第二阶段 SFT（推理 + 非推理）** → **R1 Dev-3**  
→ **（D）第二阶段 RL：规则奖励 + 偏好/安全奖励（对齐）** → **DeepSeek-R1**

并行还有一条支线：  
**DeepSeek-V3-Base →（纯 RL，准确率+格式奖励）→ R1-Zero**，R1-Zero 既是研究结论，也会被用来生成/筛选冷启动与后续推理数据。

## 2. GRPO：这篇报告最关键的算法点
