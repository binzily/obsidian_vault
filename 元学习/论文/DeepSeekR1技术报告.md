## 0.R1 系列的“产品形态”

DeepSeek 把这一代推理模型分成三类东西：

1. **DeepSeek-R1-Zero**：直接在 **DeepSeek-V3-Base** 上做大规模 RL，不先做 SFT（监督微调）。核心作用：证明“纯 RL 也能把推理能力拉起来”，但会出现可读性差、语言混杂、重复等问题。
    
2. **DeepSeek-R1**：在 Zero 的基础上，引入“冷启动（cold start）数据 + 两阶段 RL + 两阶段 SFT”，把**推理能力**和**可用性/对齐**一起做出来。
    
3. **蒸馏版 Distill 模型**：用 R1 生成的数据去微调更小的开源底座（Qwen、Llama），让小模型也学到大模型的推理模式。
    
另外，R1 本体是 **MoE（混合专家）**：模型卡给出的口径是 **总参数约 671B、激活参数约 37B、上下文长度 128K**。  

## 1.这篇报告最核心的“训练路径”到底是什么

**DeepSeek-V3-Base**  
→ **（A）冷启动 Long CoT 的 SFT** → **R1 Dev-1**  
→ **（B）第一阶段 RL：规则奖励 + 语言一致性奖励** → **R1 Dev-2**  
→ **（C）拒绝采样（rejection sampling）造数据 + 第二阶段 SFT（推理 + 非推理）** → **R1 Dev-3**  
→ **（D）第二阶段 RL：规则奖励 + 偏好/安全奖励（对齐）** → **DeepSeek-R1**

并行还有一条支线：  
**DeepSeek-V3-Base →（纯 RL，准确率+格式奖励）→ R1-Zero**，R1-Zero 既是研究结论，也会被用来生成/筛选冷启动与后续推理数据。

## 2. GRPO：这篇报告最关键的算法点
对同一个问题 qqq，不只采样 1 个回答，而是采样一组 GGG 个回答（论文里常用 **16 个**）。然后：
1. 给这 GGG 个回答各算一个奖励 $r_i$
2. 用“组内相对好坏”做优势函数（advantage）：$[ A_i = \frac{r_i - \text{mean}(r_{1..G})}{\text{std}(r_{1..G})} ]$
3. 再用类似 PPO 的“clip 目标 + KL 正则”更新策略

GRPO 不需要训练一个“价值网络/critic”去估计优势函数（PPO 常见做法），而是用“同题多样本的平均值”当 baseline，显著降低系统复杂度和资源消耗。
长 CoT 会让序列很长、方差更大，critic 的训练也更难、更费显存。报告里明确提到：PPO 对某些超参（比如 GAE 的 $λ$）很敏感，而 GRPO 更实用，尤其在大模型资源受限时。

## 3. 奖励设计：他们是怎么“让模型学会推理”的

### 3.1 R1-Zero 的奖励：尽量不用神经奖励模型

对数学/代码/逻辑推理这种“可验证”的任务，R1-Zero 用的是 **规则奖励**，主要两部分：
- **准确率奖励（accuracy）**：答案对就是 1，不对就是 0（或通过/不通过测试）
- **格式奖励（format）**：强制模型把推理写在 `<think>` 标签里，答案写在 `<answer>` 里，便于解析和后续处理

并且明确说：**对推理任务不使用神经奖励模型（无论 outcome 还是 process）**，理由是大规模 RL 下容易 reward hacking，而且重训 RM 也很贵、会让流程更复杂。

> 小白理解：能用“判卷机”就不要用“另一个大模型当裁判”。判卷机更稳定、更不容易被钻空子。

## 4. R1-Zero：纯 RL 训练到底怎么跑的（关键超参+现象）

报告把 R1-Zero 的训练细节写得很具体，抓住下面这几个点：

- 学习率 **3e-6**，KL 系数 **0.001**，rollout 温度 **1**
    
- 每题采样 **16** 个输出
    
- 最大生成长度：**8.2k step 之前 32,768 tokens；之后 65,536 tokens**
    
- 训练总步数 **10,400 steps（约 1.6 epochs）**
    
- 每 step 32 个问题 → batch size 512
    
- **每 400 steps** 用最新策略替换参考模型（reference policy）
    
- 为加速：每次 rollout 生成 **8192** 条输出，打散成 16 个 mini-batch，只训练 1 个 inner epoch
    

一个很重要的观察是：当允许更长输出（65k）之后，**模型的输出长度和效果在 8.2k step 处出现明显跃迁**。  
这就是在很多解读里看到的“aha moment/自发变强”的基础条件之一：**给足够的推理 token 预算 + 用可验证奖励驱动探索**。

## 5. 为什么还要做 DeepSeek-R1：从“会推理”到“可用、可控、可读”

报告非常直白：R1-Zero 虽然推理强，但会出现 **可读性差、语言混杂** 等问题。于是 R1 的 pipeline 目标是把这些工程化问题解决掉。

下面按阶段讲每一步在干什么。

## 6. 阶段 A：Cold Start SFT（R1 Dev-1）——先把“思考风格”扶正

### 6.1 冷启动数据在解决什么

他们收集“小规模的 long CoT 数据”做 SFT，动机更偏“产品体验”：
- 让推理过程更像第一人称、更自然（“I” vs “we”）
- 强调语言一致性（避免中文问题输出中英混写）

同时他们也专门提醒：这种“更像人”的推理叙述，是工程化启发式（heuristics），不代表模型真的获得了人类式智能或自主能力。
### 6.2 冷启动数据怎么做出来（你关心的“训练路径公开”就在这里）

一个非常关键的细节是：冷启动并不是人工从零写 CoT，而是“**R1-Zero 生成 → 过滤 → DeepSeek-V3 改写/润色 → 人工复核**”的流水线：
- 先收集数千条高质量推理 prompt
- 用 R1-Zero 以较高温度（1.0）采样多条推理轨迹
- 过滤：只保留最终答案正确且格式可读的轨迹
    - 数学部分用 **sympy** 做解析与表达式比对
    - 还做重复检测、语言混杂过滤等规则处理
- 再调用 DeepSeek-V3 做“同语言翻译/格式化/更人类可读的表达”，并进行二次人工验证

## 7. 阶段 B：第一阶段 RL（R1 Dev-2）——在“可读风格”上继续把推理做强

第一阶段 RL 的超参基本沿用 R1-Zero，并且加入了一个非常具体的工程奖励：**语言一致性奖励**。
### 7.1 第一阶段 RL 的关键配置

- 学习率 **3e-6**，KL **0.001**
- GRPO clip ratio ϵ=10（他们强调 clip ratio 很关键：太低会截断大量 token 的梯度、掉性能；太高会训练不稳定）
- rollout 温度 **1.0**
- 每题采样 **16** 个输出，最大长度 **32,768**
- 每 step 32 题 → batch 512
- 每 400 steps 更新 reference model
- rollout 仍然是一次 8192 输出、16 minibatch、1 inner epoch
### 7.2 语言一致性奖励（language consistency reward）

他们把它定义成：CoT 里“目标语言词”的占比，例如：

$R_lang = (target\ language\  words) / (all\ words)$

并且把它直接加到最终奖励里，同时对推理和非推理数据都生效。

## 8. 阶段 C：拒绝采样 + 第二阶段 SFT（R1 Dev-3）——把能力面拓宽，并把输出清洗干净

这是 v2 报告里非常“可复现”的部分：他们公开了 **SFT 数据的规模、构成、生成方式、以及训练超参**。
### 8.1 第二阶段 SFT 数据：总计约 80 万条

报告给出统计：总共约 **804,745** 条监督样本，域分布大致是：
- Math 395,285
- Code 211,129
- STEM 10,124
- Logic 10,395
- General 177,812
并且也承认：大部分是单轮对话，多轮能力可能受限，未来再扩展。
### 8.2 这 80 万怎么来的：600k 推理 + 200k 非推理

- **推理数据约 600k**：从第一阶段 RL 的 checkpoint 做 rejection sampling 得到；相比前一阶段只用可规则验证的数据，这里进一步引入“生成式裁判”（把参考答案和模型答案喂给 DeepSeek-V3 做正确性判定）。同时还会过滤掉语言混杂、长段落、代码块等“难读”的 CoT。
- **非推理数据约 200k**：复用 DeepSeek-V3 的 SFT pipeline 的一部分数据，并加入软件工程数据（程序修复、前端开发等）。某些非推理任务会先用 DeepSeek-V3 生成潜在 CoT 再回答；但简单问候类不会强行加 CoT。
### 8.3 SFT 的训练超参（也写明了）

冷启动 SFT 和第二阶段 SFT：
- 训练 **2–3 epochs**
- cosine 学习率：**5e-5 → 5e-6**
- 最大上下文长度 **32,768**
- batch size **128**

## 9. 阶段 D：第二阶段 RL（最终 R1）——把“好用/安全”也纳入奖励

### 9.1 为什么要引入奖励模型（RM）

对开放式任务（写作、编辑、角色扮演、开放问答），很难像数学那样“规则判卷”。于是他们引入 **model-based reward**：
- **helpfulness RM**：偏好有用回答
- **safety RM**：惩罚不安全回答

特别关键的一个设计取舍：
- helpfulness 只评估“最终总结/答案部分”，尽量避免干扰底层推理过程
- harmlessness（安全）评估整个回答（包括推理过程和总结），以发现潜在风险
### 9.2 RM 怎么训练的（这也是 v2 报告补齐的细节）

- **helpfulness RM**：
    - 用 DeepSeek-V3 生成偏好对（每个 query 两个候选回答）
    - 同一对样本让 DeepSeek-V3 评 4 次，随机交换 A/B 位置减轻位置偏差
    - 取 4 次平均分，并只保留分差 Δ>1\Delta>1Δ>1 的样本
    - 同时控制 chosen/rejected 长度相近，减轻“越长越高分”的偏差
    - 总计 **66,000** 对
    - RM 结构与 R1 相同但加 reward head
    - 训练：batch 256，lr 6e-6，1 epoch，训练 max seq 8192；推理时不强制长度上限
        
- **safety RM**：
    - **106,000** 条 prompt + 模型回答，被标注 safe/unsafe
    - 用 point-wise（逐样本分类）方式训练
    - 超参同 helpfulness RM
### 9.3 第二阶段 RL 的奖励怎么组合

他们最终把奖励写成加和形式：
- 推理任务：规则奖励（正确性/格式等）
- 通用任务：RM 奖励（helpful 或 safety）
- 再加格式奖励和语言一致性奖励

并且给了一个很“防翻车”的工程细节：**总共 1700 steps，偏好/通用奖励只在最后 400 steps 引入**，理由是避免 reward hacking 和推理能力被破坏

## 10. “训练路径首次详细公开”的另一个关键：RL 基础设施怎么支撑大规模训练

这次 v2 里把 RL 框架结构写得很清楚。他们把 pipeline 拆成 4 个模块：
1. **Rollout 模块**：用多个 vLLM workers 跑 actor 模型采样；针对 MoE 做 expert parallelism，并复制热点专家平衡负载；还用 MTP（multi-token prediction）做 self-speculative decoding 加速长输出采样。
2. **Inference 模块**：加载 reward model 和 reference model，对采样结果做前向，算 model-based reward 等。
3. **Rule-based reward 模块**：跑代码执行器/答案匹配/格式检查等，通常耗时；用异步调度，把它和 rollout/inference 重叠执行以隐藏延迟。
4. **Training 模块**：加载 actor（+可选 critic），计算 loss 更新参数；做数据 packing（按长度排序+Best-Fit packing）减少 padding 浪费；并集成 DualPipe 做高效 pipeline 并行。

另外一个对大模型训练很关键的工程点：除 rule-based reward 外，各模块结束后会把模型从显存 offload 到内存或磁盘，给下一阶段腾显存。