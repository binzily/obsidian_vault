## 0.Policy Gradient
### 0.1 先把LLM 放进 RL 记号里：token = action，前缀 = state
对一个固定 prompt $x$： 
- 状态：$s_t = (x, y_{<t})$（prompt + 已生成前缀） 
- 动作：$a_t = y_t$（第 $t$ 个 token） 
- 轨迹：$\tau = (s_1, a_1, \dots, s_T, a_T)$ 等价于整段 $y$ 
语言模型策略就是 $\pi_\theta(a_t | s_t) = \pi_\theta(y_t | x, y_{<t})$

### 0.2 原始公式推导
令策略（actor）是参数分布 $\pi_\theta(a | s)$。优化目标写成“轨迹回报的期望”： $$J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)]$$其中 $\tau = (s_0,a_0,\dots,s_T,a_T,s_{T+1})$ 是一条轨迹（在 LLM 里就是：给定 prompt 后采样出一整段序列；$a_t$ 对应 token，$s_t$ 对应“prompt+已生成前缀”）。 
轨迹概率可以展开为（环境转移不含 $\theta$）： $$P(\tau | \theta) = \rho_0(s_0) \prod_{t=0}^T P(s_{t+1} | s_t,a_t) \pi_\theta(a_t | s_t)$$
用log-derivative trick 把 $\nabla J(\theta)$ 写成期望（积分）：$$J(\theta) = \int P(\tau | \theta) R(\tau) d\tau$$ 求梯度： $$\nabla_\theta J(\theta) = \int \nabla_\theta P(\tau | \theta) R(\tau) d\tau$$ 用 log-derivative trick: $$\nabla_\theta P(\tau | \theta) = P(\tau | \theta) \nabla_\theta \log P(\tau | \theta)$$ 代入得: $$\nabla_\theta J(\theta) = \int P(\tau | \theta) \nabla_\theta \log P(\tau | \theta) R(\tau) d\tau = \mathbb{E}_{\tau\sim\pi_\theta}\left[\nabla_\theta \log P(\tau | \theta) R(\tau)\right]$$ 而因为环境部分不依赖 $\theta$, 所以 $$\nabla_\theta \log P(\tau | \theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t)$$ 于是得到最“原始”的 REINFORCE 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_\tau\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]$$
**还完全是 on-policy**：期望是对“当前策略 $π_{θ}$​ 诱导出的轨迹分布 $p_\theta(\tau)$”取的。
“重要性采样（IS）”出现的唯一原因是：**你想用别的分布采到的样本来估计这个期望**——典型就是用旧策略 $\pi_{\text{old}}​$ 采到的轨迹来更新新策略 $\pi_\theta$​。
1) 从 on-policy 期望到 IS：就是一个换测度恒等式 对任意函数 $f(\tau)$，只要 $p_{\text{old}}(\tau) > 0 \Rightarrow p_\theta(\tau) > 0$（支持集覆盖），都有： $$\mathbb{E}_{\tau\sim p_\theta}[f(\tau)] = \sum_\tau p_\theta(\tau)f(\tau) = \sum_\tau p_{\text{old}}(\tau)\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)}f(\tau) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[w(\tau)f(\tau)\right]$$ 这一步就是重要性采样的定义：用权重 $w(\tau) = \frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}$ 把“在 $p_{\text{old}}$ 下的期望”变成“在 $p_\theta$ 下的期望”。
2) 把 $f(\tau)$ 换成你的 policy-gradient integrand 令 $$f(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)$$ 代回去就得到 off-policy 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)} \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)\right]$$ 这就是“从 on-policy policy gradient 到重要性采样”的那一步：只是把期望的采样分布从 $p_\theta$ 换成 $p_{\text{old}}$。
3) 为什么 $w(\tau)$ 会变成“连乘的 ratio”（而且环境转移会抵消） 
       轨迹分布可写成（环境转移不依赖 $\theta$）： $$p_\theta(\tau) = \rho_0(s_0) \prod_{t=0}^T \pi_\theta(a_t | s_t) P(s_{t+1} | s_t,a_t)$$因此比值： $$\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)} = \prod_{t=0}^T \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$所有 $P(\cdot)$ 都消掉了，所以 IS 权重只依赖两套策略，不依赖环境动态。Sutton & Barto 在讲 off-policy MC 时把这个“转移概率抵消”写得很明确，并给出了乘积形式的 ratio（他们用 $\mu$ 表示行为策略/旧策略）。 映射到  LLM: $s_t = (x,y_{<t})$，$a_t = y_t$，于是 $$w_{\text{seq}}(y) = \prod_t \frac{\pi_\theta(y_t | x,y_{<t})}{\pi_{\text{old}}(y_t | x,y_{<t})}$$这就是你一直写的“序列级重要性比率”。问题在于：**用这个 $w_{seq}(y)$ 做梯度估计方差巨大**（尤其长序列时），**TRPO/PPO 走的是另一条路：改目标函数、做“局部近似 + 信赖域”**，而不是严格轨迹 IS。
4) TRPO 的关键一步：用“性能差异恒等式”把目标改写成 advantage 的累积 
       TRPO 先给出一个恒等式：新策略 $\pi$ 的性能可以写成旧策略 $\pi_{\text{old}}$ 的性能加上“在新策略下累积旧策略 advantage”的期望。然后引入局部近似：把“新策略的状态访问分布”用旧策略的替换，得到一个 local surrogate $L$，并且说明这个 surrogate 在 $\pi = \pi_{\text{old}}$ 处与真实目标一阶一致（first-order match）。 
       直觉翻译成一句话就是： 
       - 真正的 $J(\pi)$ 很难直接优化（因为状态分布会随策略变） 
       - 于是 TRPO 优化一个用旧策略状态分布构造的 surrogate $L_{\pi_{\text{old}}}(\pi)$，并靠 trust region 保证“别走太远”，这样一阶近似就可靠

       






## 1.RLHF + PPO训练LLM的全流程：

### 1.1训练前的准备

**SFT** 阶段：首先需要一个已经完成预训练的 LLM ，它的训练任务是“Next Token Prediction”，SFT就是在这个 LLM 的基础上用“指令-回答”数据集监督微调，最后得到的模型就是 SFT 模型。
**RM** 阶段：复用SFT模型的架构来构建Reward Model。和Critic一样，它也有一个特殊的“奖励头”，输出一个标量值。训练数据都是一个 `(prompt, SFT模型回答_1, SFT模型回答_2, 排序结果)` 的组合。

| 模型                        | 来源                         | 在PPO阶段的状态           | 训练阶段            |
| ------------------------- | -------------------------- | ------------------- | --------------- |
| Actor $π_θ$               | 复制自SFT模型                   | 可训练，在PPO中不断更新       | PPO             |
| Critic $V_φ$              | 架构和权重来自SFT模型，但有一个随机初始化的价值头 | 可训练，在PPO中与Actor同步更新 | PPO             |
| Reference Model $π_{SFT}$ | 就是SFT模型本身                  | 冻结，参数固定不变           | SFT             |
| Reward Model (RM)         | 架构来自SFT模型，在人类排序数据上单独训练     | 冻结，参数固定不变           | Reward Modeling |
### 1.2训练主循环

###### 步骤一：数据收集 (Rollout / 与环境交互)
一句话：我们让当前的Actor模型 $π_θ$ 生成一大批对话数据。具体流程：
1. 从一个数据集中随机抽取一批 `prompt`。
2. 对于每一个 `prompt`，让Actor $π_θ$ 自回归地（一个词一个词地）生成一个完整的回答。
3. 在生成每一个词 $a_t$ 的过程中，我们需要记录下一整条“轨迹 (trajectory)”所需的所有信息：
    - **状态 $s_t$**: 生成 $a_t$ 之前的文本序列。
    - **动作 $a_t$**: 实际生成的那个词 (token)。
    - **动作概率 $log(π_θ(a_t|s_t))$**: Actor模型在状态 $s_t$ 下，选择动作 $a_t$ 的对数概率。
    - **价值估计 $V_φ(s_t)$**: Critic模型对当前状态 $s_t$ 的打分。
    - **KL散度奖励 $r_{KL}$**: 这是我们之前讲的“过程分”，用于稳定训练。
4. 当一个回答生成完毕（遇到结束符 $<eos>$），我们计算它的最终奖励：
    - 将完整的 `prompt + 回答` 输入给 RM，得到一个最终分数 $R_{final}$。
    - 将 KL散度奖励 和 最终奖励 结合。通常是这样：
        - 对于中间的每一步 `t`，奖励 $r_t = -β * KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$。
        - 对于最后一步 `T`，奖励 $r_T = R_{final} + r_{KL}$。
5. 重复以上过程，直到收集到足够多的轨迹数据（例如，几千条完整的对话）。现在，我们手上有一个巨大的经验池，里面装满了 $(s_t, a_t, log(π_θ(a_t|s_t)), V_φ(s_t), r_t)$ 这些数据点。
###### 步骤二：优势计算
==“构造优势”本身就是在做方差降低：把回报做“去均值/中心化”的 baseline（控制变量） 技巧。==
==优势的定义是 `A(s,a) = Q(s,a) - V(s)`。问题在于，真实的`Q`值我们永远不知道。我们只能用不同的方法去**估计**它。==
两种极端的估计方法：
1. TD估计 (只看一步): $Â_{TD}(t) = r_t + γV(s_{t+1}) - V(s_t) = δ_t$
    - 优点: 方差低。因为它主要依赖于相对稳定的函数`V`，而不是充满随机性的真实奖励。
    - 缺点: 偏差高。因为$V(s_{t+1})$身就是一个不准确的估计，这个偏差会引入到$Â$的计算中。
2. 蒙特卡洛估计 (看到底): $Â_{MC}(t) = (r_t + γr_{t+1} + γ^2r_{t+2} + ...) - V(s_t)$
    - 优点: 无偏。因为它完全基于真实发生过的奖励序列，没有任何估计成分。
    - 缺点: 方差高。因为真实的奖励序列每次都可能因随机性而剧烈波动。
我们可以定义一个“2步优势估计”：$Â^2(t) = r_t + γr_{t+1} + γ^2V(s_{t+2}) - V(s_t)$。  
可以定义一个“k步优势估计”：$Â^{k}(t) = (Σ_{l=0}^{k-1} γ^l r_{t+l}) + γ^k V(s_{t+k}) - V(s_t)$
GAE的公式 $Â_t = δ_t + (γλ)δ_{t+1} + (γλ)^2δ_{t+2} + ...$ 
                  $Â_t = δ_t + (γλ) * Â_{t+1}$
                  $\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_{\text{old}}(s_{t+1}) - V_{\text{old}}(s_t)$
                  PS：λ=1意思是把所有长度的 TD 残差都权重一样地累加
- `t`时刻的GAE优势，不仅仅是`t`时刻的TD-Error $δ_t$。
- 它还包含了对未来`t+1`时刻TD-Error $δ_{t+1}$的一次折扣考虑。
- 它还包含了对未来`t+2`时刻TD-Error $δ_{t+2}$的二次折扣考虑。
- ...以此类推，直到序列结束。
###### 步骤三：模型优化
1. Actor Loss：$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$
2. Critic Loss：$L_{VF}(φ) = E[ (V_φ(s_t) - V_t^{target})^2]$ = $\mathbb{E}\left[\left(V_φ(s_t) - R_t\right)^2\right]$
			   $V_t^{\text{target}} = \hat{R}_t = \hat{A}_t + V_{old}(s_t)$
3. Entropy of the policy Bonus：$S = E[ -Σ_a π_θ(a|s_t) * log(π_θ(a|s_t)) ]$，最大化熵（鼓励探索）。

>重要性采样：$$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$
>Q:理论上用 $\mathbb{E}_{a\sim\pi_\theta(\cdot|s_t)}[\hat{A}(s_t,a)] = \sum_a \pi_\theta(a|s_t)\hat{A}(s_t,a)$ （没有重要性采样）当作Actor的优化目标是完全可以的。那为什么不这样写而是引入重要性采样呢？
>A：$\hat{A}(s_t,a)$ 是 $\pi_{old}$ 得到的，$\hat{A}(s_t,a)$ 里所有数据（reward, value, log‑probs）都是**基于旧策略分布**，但是目标函数应该是**对当前策略分布的期望**。

>Q：LLM用到的RLHF是policy based还是value based？
>A: 本质上是policy-based（策略梯度）为主，但训练时通常带一个 **critic/value function** 当 baseline ——也就是 **actor–critic**。
>**用到了 value**，但不是 “value-based RL” 的那种（以学 Q 为主、用 Q 决策）。

>Q:为什么LLM很少用纯value-based（比如Q-learning）？
>A：动作空间是“词表大小（几万）× 每个位置”的巨大离散空间，且序列很长，直接学全动作的 Q(s,a)并做 max/bootstrapping 通常更难、更不稳定；策略梯度更自然。
>Q：什么是bootstraping？
>A：bootstraping（自举），用自己当前学到的价值函数预测，来当作“未来回报”的一部分目标，而不是等到把整段轨迹都走完再算真实回报。比如​用$r_t + \gamma V(s_{t+1})$ 或 GAE 来构造$\hat{R}_t$和$\hat{A}_t$。
>bootstraping的优点就是方差小；缺点就是有偏差。

> **on-policy**：你用“当前策略 $\pi_\theta$​”采样的数据，也用来更新“当前策略”。指**不长期用老数据**，不是指“每个 SGD step 都完全 on-policy”。比如在一轮rollout中，采样到的数据会被分为很多mini-batch。$\pi_{\text{old}}$ 就是rollout时的策略，$\pi_\theta$ 在一个mini-batch就会更新一次，第一个mini-batch时，重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ = 1。**第一轮更新后， $\pi_\theta$​ 已经变了，但数据还是旧策略采的**。因此后面那些更新，从严格意义上讲就“off-policy 了”，GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”
> **off-policy**：你用“别的策略”（旧策略/行为策略）采样的数据，拿来更新“当前策略”。
## 2.GRPO

在 **GRPO（Group Relative Policy Optimization）** 里确实**没有 critic / value function**，所以它不走 “$A=\hat R - V(s)$” 这条路，而是用 **“同一个 prompt 下多次采样得到的一组回答（group）”** 来构造一个 **baseline**，从而得到优势（advantage）。
对同一个prompt x，从当前策略π_θ采样K个回答y₁,…,y_K。 
1. 先算每个回答的标量奖励（通常含KL惩罚）： $r_i = R(x,y_i) - \beta \text{KL}(\pi_\theta(\cdot|x) \parallel \pi_{\text{ref}}(\cdot|x))$
2. 用组内baseline（最常见是均值）： $b = \frac{1}{K} \sum_{j=1}^K r_j$ 
3. 定义优势（相对同组其他回答“好多少”）： $\hat{A}_i = r_i - b$ 很多实现还会做 组内标准化（更稳）： $\hat{A}_i = \frac{r_i - \mu}{\sigma + \epsilon}, \quad \mu = \frac{1}{K} \sum_{j=1}^K r_j, \quad \sigma = \sqrt{\frac{1}{K} \sum (r_j - \mu)^2}$ 
>直觉：同一个prompt的这组回答里，比平均好就正优势，比平均差就负优势。这样就不需要一个单独的critic来预测V(s)当baseline了。

## 3.DPO
#### DPO 的核心：用“成对偏好”替代 advantage
数据是一对回答：$(x, y^+, y^-)$，表示在prompt x下人更喜欢$y^+$而不是$y^-$。 
DPO定义一个“偏好logit”（可以把它当成一种相对优势）： $\Delta_\theta = \left[\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x)\right] - \left[\log \pi_{\text{ref}}(y^+|x) - \log \pi_{\text{ref}}(y^-|x)\right]$ 
然后用logistic loss： $L_{\text{DPO}}(\theta) = -\mathbb{E}\left[\log \sigma(\beta \Delta_\theta)\right]$ 
- $\sigma$: sigmoid 
- $\beta$: 温度/强度超参（控制偏好推动的力度） 
- ref项让更新不要偏离参考模型太多（效果上类似RLHF里的KL约束）

#### “那优势在哪？”
如果你非要找一个“优势”的影子，它就是这个 成对差值的logit: 
- 在PPO/GRPO里：优势是“这个动作/回答比baseline好多少” 
- 在DPO里：你不需要baseline，因为标签直接告诉你$y^+$应该比$y^-$好 所以DPO直接优化“偏好差”: $\Delta_\theta$ 越大越好 
- 可以把$\Delta_\theta$看成一种 pairwise advantage（“偏好优势”），但它不是$Q-V$那种per-state的优势，也不需要critic去估$V$。

==强化学习中神经网络本质：把你在环境里交互得到的经验（状态、动作、奖励、下一个状态）压缩成一个可微分的函数，用来**决策**（怎么选动作）和/或**评估**（这个状态/动作有多好），从而最大化长期回报。==

## 4.强化学习主流算法总结
现在主流的大模型RL微调，基本都跑在PPO家族上：一开始是标准PPO，再到DPO这类“去掉value、走偏好”的方向，后面DeepSeek提了GRPO，Qwen又在这个基础上提出了DAPO、GSPO，再往前一步就是现在的SAPO。整个演进有一个共同的主线，就是在解决两件事：**训练稳定性和样本效率**，尤其是在长CoT、MoE这种比较折腾的场景下。

GRPO 的创新是用组内相对比较 + token 级 PPO 框架，去掉 value 和 RM；它的问题是 **reward 粒度和更新粒度不一致**，尤其是在长序列和 MoE 上不稳定。
>用于“任务好坏”的主奖励（RM/正确性）是序列级的，而参数更新却要对每个 token 的 logprob 做梯度；
>于是主奖励会被“广播”到所有 token，上下文里哪个 token 真正导致成功/失败很难归因。
>KL/熵这些 token 级项，更多是“正则/约束/探索”，不是“任务信用分配”。

### 4.1 PPO>DPO>GRPO>DAPO>GSPO>SAPO
DAPO、GSPO、SAPO都在解决同一类痛点：**长序列 + 多个 mini-batch 更新会带来 off-policy；如果用“token 级重要性比率 + token 级裁剪”，方差会沿序列累积，容易不稳定甚至崩溃**。GSPO 论文把这个问题说得很直：GRPO 把重要性权重用在每个 token 上，本质上“**用单一样本去做分布校正**”会引入高方差噪声，并随长序列累积；而奖励是整段给的，所以“优化单位应匹配奖励单位”。

>"分布校正"：你手里有的数据是按 **旧分布/旧策略** $\pi_{\text{old}}$​ 采出来的，但你想优化的是 **新分布/新策略** $\pi_\theta$。重要性采样的做法“**用权重把分布从 old 校正到 new**”，所以又RLHF中PPO算法的重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ 。
>
>"单一样本做分布校正"：在每个 token 位置只采到一个 token，就试图用它的概率比率去代表整个位点的分布差异；理论上需要大量样本平均才稳定，现实里会变成高方差噪声。
>在 LLM-RL（GRPO 那套）里，每个 token 位置 t 的“动作”就是生成的那个 $token \ a_t$​。但在某个前缀状态 $s_t$ 下，你通常只采到 **一个** token（一次只走了一条路径）。于是你用 **这一条路径上这一个 token** 的比率$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$来做“分布校正”，**只采样一次就用重要性采样本身就会带来偏差（或者说是引入了噪声）。**

在 LLM 的 RL 里，一次 rollout 得到的是整段序列 $y_{1:T}$，而你常见的训练做法是：**一大批 rollout 数据切成多个 mini-batch，做多次梯度更新**——这一步直接把学习推向 off-policy：数据来自旧策略 $\pi_{\text{old}}$​，但你在优化新策略 $\pi_\theta$​。GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”。

#### 4.1.1 从最基本的理论：重要性采样在“随机变量本身的分布”上做换测度 
1) 设你要的目标是（prompt $x$ 固定）： $$J(\theta) = \mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[R(x,y)]$$这里 $y = y_{1:T}$ 是整段序列，奖励 $R(x,y)$ 也是整段函数。 如果你手里只有旧策略 $\pi_{\text{old}}$ 采样来的序列 $y$，那要把期望换成在旧分布下： $$\mathbb{E}_{y\sim\pi_\theta}[R] = \sum_y \pi_\theta(y|x)R(x,y) = \sum_y \pi_{\text{old}}(y|x)\underbrace{\frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}}_{\text{正确的IS权重}}R(x,y) = \mathbb{E}_{y\sim\pi_{\text{old}}}\left[w_{\text{seq}}(y)R(x,y)\right]$$所以理论上对应的权重就是 $$w_{\text{seq}}(y) = \frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}$$这一步就是经典重要性采样恒等式，没有额外假设。 在 RL 文献里，这个 $w$ 叫 trajectory-wise / ordinary importance sampling ratio：用“轨迹在目标策略下发生的概率”除以“轨迹在行为策略下发生的概率”。Thomas & Brunskill 的 high-confidence OPE 论文就把无偏估计写成：对每条轨迹用 $\Pr(\tau|\theta)/\Pr(\tau|\theta_i)$ 乘以回报，并进一步展开成沿时间步的概率比连乘。

2) 为什么它在序列生成里会变成“token 比率的连乘”（也就是你说的序列级 ratio） LLM 自回归生成的序列概率本来就按链式法则分解： $$\pi_\theta(y|x) = \prod_{t=1}^T \pi_\theta(y_t | x, y_{<t})$$ 因此 $$w_{\text{seq}}(y) = \frac{\prod_t \pi_\theta(y_t|x,y_{<t})}{\prod_t \pi_{\text{old}}(y_t|x,y_{<t})} = \prod_{t=1}^T \underbrace{\frac{\pi_\theta(y_t|x,y_{<t})}{\pi_{\text{old}}(y_t|x,y_{<t})}}_{r_t}$$ 这就是“序列级重要性比率 = token 比率连乘”的严格推导。 Sutton & Barto 在讲 off-policy Monte Carlo 时也明确把重要性采样比率写成沿轨迹的动作概率比连乘（$\prod \pi/\mu$），并讨论它会导致方差极端甚至无穷。
3) PPO/GRPO 的 loss 不是把“整段一个标量权重 $w_{\text{seq}}$”乘到整段梯度上；它是把每个 token 的梯度分别乘上各自的 $r_t$（以及各自的 clip/min 分支）再求和：$$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$
4) 