## 1.RLHF + PPO训练LLM的全流程：

### 1.1训练前的准备

**SFT** 阶段：首先需要一个已经完成预训练的 LLM ，它的训练任务是“Next Token Prediction”，SFT就是在这个 LLM 的基础上用“指令-回答”数据集监督微调，最后得到的模型就是 SFT 模型。
**RM** 阶段：复用SFT模型的架构来构建Reward Model。和Critic一样，它也有一个特殊的“奖励头”，输出一个标量值。训练数据都是一个 `(prompt, SFT模型回答_1, SFT模型回答_2, 排序结果)` 的组合。

| 模型                        | 来源                         | 在PPO阶段的状态           | 训练阶段            |
| ------------------------- | -------------------------- | ------------------- | --------------- |
| Actor $π_θ$               | 复制自SFT模型                   | 可训练，在PPO中不断更新       | PPO             |
| Critic $V_φ$              | 架构和权重来自SFT模型，但有一个随机初始化的价值头 | 可训练，在PPO中与Actor同步更新 | PPO             |
| Reference Model $π_{SFT}$ | 就是SFT模型本身                  | 冻结，参数固定不变           | SFT             |
| Reward Model (RM)         | 架构来自SFT模型，在人类排序数据上单独训练     | 冻结，参数固定不变           | Reward Modeling |
### 1.2训练主循环

###### 步骤一：数据收集 (Rollout / 与环境交互)
一句话：我们让当前的Actor模型 $π_θ$ 生成一大批对话数据。具体流程：
1. 从一个数据集中随机抽取一批 `prompt`。
2. 对于每一个 `prompt`，让Actor $π_θ$ 自回归地（一个词一个词地）生成一个完整的回答。
3. 在生成每一个词 $a_t$ 的过程中，我们需要记录下一整条“轨迹 (trajectory)”所需的所有信息：
    - **状态 $s_t$**: 生成 $a_t$ 之前的文本序列。
    - **动作 $a_t$**: 实际生成的那个词 (token)。
    - **动作概率 $log(π_θ(a_t|s_t))$**: Actor模型在状态 $s_t$ 下，选择动作 $a_t$ 的对数概率。
    - **价值估计 $V_φ(s_t)$**: Critic模型对当前状态 $s_t$ 的打分。
    - **KL散度奖励 $r_{KL}$**: 这是我们之前讲的“过程分”，用于稳定训练。
4. 当一个回答生成完毕（遇到结束符 $<eos>$），我们计算它的最终奖励：
    - 将完整的 `prompt + 回答` 输入给 RM，得到一个最终分数 $R_{final}$。
    - 将 KL散度奖励 和 最终奖励 结合。通常是这样：
        - 对于中间的每一步 `t`，奖励 $r_t = -β * KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$。
        - 对于最后一步 `T`，奖励 $r_T = R_{final} + r_{KL}$。
5. 重复以上过程，直到收集到足够多的轨迹数据（例如，几千条完整的对话）。现在，我们手上有一个巨大的经验池，里面装满了 $(s_t, a_t, log(π_θ(a_t|s_t)), V_φ(s_t), r_t)$ 这些数据点。
###### 步骤二：优势计算
==“构造优势”本身就是在做方差降低：把回报做“去均值/中心化”的 baseline（控制变量） 技巧。==
==优势的定义是 `A(s,a) = Q(s,a) - V(s)`。问题在于，真实的`Q`值我们永远不知道。我们只能用不同的方法去**估计**它。==
两种极端的估计方法：
1. TD估计 (只看一步): $Â_{TD}(t) = r_t + γV(s_{t+1}) - V(s_t) = δ_t$
    - 优点: 方差低。因为它主要依赖于相对稳定的函数`V`，而不是充满随机性的真实奖励。
    - 缺点: 偏差高。因为$V(s_{t+1})$身就是一个不准确的估计，这个偏差会引入到$Â$的计算中。
2. 蒙特卡洛估计 (看到底): $Â_{MC}(t) = (r_t + γr_{t+1} + γ^2r_{t+2} + ...) - V(s_t)$
    - 优点: 无偏。因为它完全基于真实发生过的奖励序列，没有任何估计成分。
    - 缺点: 方差高。因为真实的奖励序列每次都可能因随机性而剧烈波动。
我们可以定义一个“2步优势估计”：$Â^2(t) = r_t + γr_{t+1} + γ^2V(s_{t+2}) - V(s_t)$。  
可以定义一个“k步优势估计”：$Â^{k}(t) = (Σ_{l=0}^{k-1} γ^l r_{t+l}) + γ^k V(s_{t+k}) - V(s_t)$
GAE的公式 $Â_t = δ_t + (γλ)δ_{t+1} + (γλ)^2δ_{t+2} + ...$ 
                  $Â_t = δ_t + (γλ) * Â_{t+1}$
                  $\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_{\text{old}}(s_{t+1}) - V_{\text{old}}(s_t)$
                  PS：λ=1意思是把所有长度的 TD 残差都权重一样地累加
- `t`时刻的GAE优势，不仅仅是`t`时刻的TD-Error $δ_t$。
- 它还包含了对未来`t+1`时刻TD-Error $δ_{t+1}$的一次折扣考虑。
- 它还包含了对未来`t+2`时刻TD-Error $δ_{t+2}$的二次折扣考虑。
- ...以此类推，直到序列结束。
###### 步骤三：模型优化
1. Actor Loss：$L_{CLIP}(θ) = E[ min( r_t(θ) * Â_t, clip(r_t(θ), 1-ε, 1+ε) * Â_t ) ]$
2. Critic Loss：$L_{VF}(φ) = E[ (V_φ(s_t) - V_t^{target})^2]$ = $\mathbb{E}\left[\left(V_φ(s_t) - R_t\right)^2\right]$
			   $V_t^{\text{target}} = \hat{R}_t = \hat{A}_t + V_{old}(s_t)$
3. Entropy of the policy Bonus：$S = E[ -Σ_a π_θ(a|s_t) * log(π_θ(a|s_t)) ]$，最大化熵（鼓励探索）。

>Q：LLM用到的RLHF是policy based还是value based？
>A: 本质上是policy-based（策略梯度）为主，但训练时通常带一个 **critic/value function** 当 baseline ——也就是 **actor–critic**。
>**用到了 value**，但不是 “value-based RL” 的那种（以学 Q 为主、用 Q 决策）。

>Q:为什么LLM很少用纯value-based（比如Q-learning）？
>A：动作空间是“词表大小（几万）× 每个位置”的巨大离散空间，且序列很长，直接学全动作的 Q(s,a)并做 max/bootstrapping 通常更难、更不稳定；策略梯度更自然。
>Q：什么是bootstraping？
>A：bootstraping（自举），用自己当前学到的价值函数预测，来当作“未来回报”的一部分目标，而不是等到把整段轨迹都走完再算真实回报。比如​用$r_t + \gamma V(s_{t+1})$ 或 GAE 来构造$\hat{R}_t$和$\hat{A}_t$。
>bootstraping的优点就是方差小；缺点就是有偏差。

> **on-policy**：你用“当前策略 $\pi_\theta$​”采样的数据，也用来更新“当前策略”。
> **off-policy**：你用“别的策略”（旧策略/行为策略）采样的数据，拿来更新“当前策略”。
## 2.GRPO

在 **GRPO（Group Relative Policy Optimization）** 里确实**没有 critic / value function**，所以它不走 “$A=\hat R - V(s)$” 这条路，而是用 **“同一个 prompt 下多次采样得到的一组回答（group）”** 来构造一个 **baseline**，从而得到优势（advantage）。
对同一个prompt x，从当前策略π_θ采样K个回答y₁,…,y_K。 
1. 先算每个回答的标量奖励（通常含KL惩罚）： $r_i = R(x,y_i) - \beta \text{KL}(\pi_\theta(\cdot|x) \parallel \pi_{\text{ref}}(\cdot|x))$
2. 用组内baseline（最常见是均值）： $b = \frac{1}{K} \sum_{j=1}^K r_j$ 
3. 定义优势（相对同组其他回答“好多少”）： $\hat{A}_i = r_i - b$ 很多实现还会做 组内标准化（更稳）： $\hat{A}_i = \frac{r_i - \mu}{\sigma + \epsilon}, \quad \mu = \frac{1}{K} \sum_{j=1}^K r_j, \quad \sigma = \sqrt{\frac{1}{K} \sum (r_j - \mu)^2}$ 
>直觉：同一个prompt的这组回答里，比平均好就正优势，比平均差就负优势。这样就不需要一个单独的critic来预测V(s)当baseline了。

## 3.DPO
#### DPO 的核心：用“成对偏好”替代 advantage
数据是一对回答：$(x, y^+, y^-)$，表示在prompt x下人更喜欢$y^+$而不是$y^-$。 
DPO定义一个“偏好logit”（可以把它当成一种相对优势）： $\Delta_\theta = \left[\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x)\right] - \left[\log \pi_{\text{ref}}(y^+|x) - \log \pi_{\text{ref}}(y^-|x)\right]$ 
然后用logistic loss： $L_{\text{DPO}}(\theta) = -\mathbb{E}\left[\log \sigma(\beta \Delta_\theta)\right]$ 
- $\sigma$: sigmoid 
- $\beta$: 温度/强度超参（控制偏好推动的力度） 
- ref项让更新不要偏离参考模型太多（效果上类似RLHF里的KL约束）

#### “那优势在哪？”
如果你非要找一个“优势”的影子，它就是这个 成对差值的logit: 
- 在PPO/GRPO里：优势是“这个动作/回答比baseline好多少” 
- 在DPO里：你不需要baseline，因为标签直接告诉你$y^+$应该比$y^-$好 所以DPO直接优化“偏好差”: $\Delta_\theta$ 越大越好 
- 可以把$\Delta_\theta$看成一种 pairwise advantage（“偏好优势”），但它不是$Q-V$那种per-state的优势，也不需要critic去估$V$。

==强化学习中神经网络本质：把你在环境里交互得到的经验（状态、动作、奖励、下一个状态）压缩成一个可微分的函数，用来**决策**（怎么选动作）和/或**评估**（这个状态/动作有多好），从而最大化长期回报。==

## 4.强化学习主流算法总结
现在主流的大模型RL微调，基本都跑在PPO家族上：一开始是标准PPO，再到DPO这类“去掉value、走偏好”的方向，后面DeepSeek提了GRPO，Qwen又在这个基础上提出了DAPO、GSPO，再往前一步就是现在的SAPO。整个演进有一个共同的主线，就是在解决两件事：**训练稳定性和样本效率**，尤其是在长CoT、MoE这种比较折腾的场景下。

GRPO 的创新是用组内相对比较 + token 级 PPO 框架，去掉 value 和 RM；它的问题是 **reward 粒度和更新粒度不一致**，尤其是在长序列和 MoE 上不稳定。
>用于“任务好坏”的主奖励（RM/正确性）是序列级的，而参数更新却要对每个 token 的 logprob 做梯度；
>于是主奖励会被“广播”到所有 token，上下文里哪个 token 真正导致成功/失败很难归因。
>KL/熵这些 token 级项，更多是“正则/约束/探索”，不是“任务信用分配”。

### 4.1 PPO>DPO>GRPO>DAPO>GSPO>SAPO

