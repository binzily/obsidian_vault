先说结论：**Pre-Norm更易优化但会表示塌缩；Post-Norm表示更不塌但更易梯度消失**


先把两种残差块写成最常见的形式（忽略 Dropout 等细节）：
- **Pre-Norm**：$x_{l+1}=x_{l}+F(LN(x_{l}))$
- **Post-Norm**：$x_{l+1}=LN(x_{l}+F(x_{l}))$

**Pre-Norm** 即便主分支那项的雅可比很小，梯度也至少能沿着残差支路$I$“直接穿过去”，**以至于到了下一层至少有”1“的梯度**。
但是就是这个”1“也导致了**表示塌缩**：Pre-Norm 的残差流是“累加器”，Pre-Norm 每层做 $x_{l+1}=x_{l}+Δl$（其中 $Δl=F(LN(x_{l}))$。如果随着深度增加，$∥xl∥$ 变得越来越大，而每层新增的 $∥Δl∥$相对变小，那么$x_{l}+1≈x_{l}$⇒相邻层表示会越来越像⇒“层变深但表示不变”，这就对应了“塌缩”。

**Post-Norm** 每层输出都被 LN 包了一次。直观上，梯度往回传要**连续穿过很多个 LN 的雅可比**，而且浅层的信号还会被“多次归一化”，导致梯度范数随深度指数级衰减。
Post-Norm 因为每层都对 $x_{l}+F(⋅)x_l$ 做 LN，相当于持续“重标定”，**降低某个 hidden state 对后续层的支配**，所以更不容易出现“一个大残差流把后面层淹没”的现象，从而能缓解塌缩；但它又把梯度消失问题带回来了。