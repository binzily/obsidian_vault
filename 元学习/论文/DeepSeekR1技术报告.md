## 0.R1 系列的“产品形态”

DeepSeek 把这一代推理模型分成三类东西：

1. **DeepSeek-R1-Zero**：直接在 **DeepSeek-V3-Base** 上做大规模 RL，不先做 SFT（监督微调）。核心作用：证明“纯 RL 也能把推理能力拉起来”，但会出现可读性差、语言混杂、重复等问题。
    
2. **DeepSeek-R1**：在 Zero 的基础上，引入“冷启动（cold start）数据 + 两阶段 RL + 两阶段 SFT”，把**推理能力**和**可用性/对齐**一起做出来。
    
3. **蒸馏版 Distill 模型**：用 R1 生成的数据去微调更小的开源底座（Qwen、Llama），让小模型也学到大模型的推理模式。
    
另外，R1 本体是 **MoE（混合专家）**：模型卡给出的口径是 **总参数约 671B、激活参数约 37B、上下文长度 128K**。  

## 1.这篇报告最核心的“训练路径”到底是什么

**DeepSeek-V3-Base**  
→ **（A）冷启动 Long CoT 的 SFT** → **R1 Dev-1**  
→ **（B）第一阶段 RL：规则奖励 + 语言一致性奖励** → **R1 Dev-2**  
→ **（C）拒绝采样（rejection sampling）造数据 + 第二阶段 SFT（推理 + 非推理）** → **R1 Dev-3**  
→ **（D）第二阶段 RL：规则奖励 + 偏好/安全奖励（对齐）** → **DeepSeek-R1**

并行还有一条支线：  
**DeepSeek-V3-Base →（纯 RL，准确率+格式奖励）→ R1-Zero**，R1-Zero 既是研究结论，也会被用来生成/筛选冷启动与后续推理数据。

## 2. GRPO：这篇报告最关键的算法点
对同一个问题 qqq，不只采样 1 个回答，而是采样一组 GGG 个回答（论文里常用 **16 个**）。然后：
1. 给这 GGG 个回答各算一个奖励 $r_i$
2. 用“组内相对好坏”做优势函数（advantage）：$[ A_i = \frac{r_i - \text{mean}(r_{1..G})}{\text{std}(r_{1..G})} ]$
3. 再用类似 PPO 的“clip 目标 + KL 正则”更新策略

GRPO 不需要训练一个“价值网络/critic”去估计优势函数（PPO 常见做法），而是用“同题多样本的平均值”当 baseline，显著降低系统复杂度和资源消耗。
长 CoT 会让序列很长、方差更大，critic 的训练也更难、更费显存。报告里明确提到：PPO 对某些超参（比如 GAE 的 $λ$）很敏感，而 GRPO 更实用，尤其在大模型资源受限时。

## 3. 奖励设计：他们是怎么“让模型学会推理”的

### 3.1 R1-Zero 的奖励：尽量不用神经奖励模型

对数学/代码/逻辑推理这种“可验证”的任务，R1-Zero 用的是 **规则奖励**，主要两部分：
- **准确率奖励（accuracy）**：答案对就是 1，不对就是 0（或通过/不通过测试）
- **格式奖励（format）**：强制模型把推理写在 `<think>` 标签里，答案写在 `<answer>` 里，便于解析和后续处理

并且明确说：**对推理任务不使用神经奖励模型（无论 outcome 还是 process）**，理由是大规模 RL 下容易 reward hacking，而且重训 RM 也很贵、会让流程更复杂。

> 小白理解：能用“判卷机”就不要用“另一个大模型当裁判”。判卷机更稳定、更不容易被钻空子。

## 4. R1-Zero：纯 RL 训练到底怎么跑的（关键超参+现象）

报告把 R1-Zero 的训练细节写得很具体，抓住下面这几个点：

- 学习率 **3e-6**，KL 系数 **0.001**，rollout 温度 **1**
    
- 每题采样 **16** 个输出
    
- 最大生成长度：**8.2k step 之前 32,768 tokens；之后 65,536 tokens**
    
- 训练总步数 **10,400 steps（约 1.6 epochs）**
    
- 每 step 32 个问题 → batch size 512
    
- **每 400 steps** 用最新策略替换参考模型（reference policy）
    
- 为加速：每次 rollout 生成 **8192** 条输出，打散成 16 个 mini-batch，只训练 1 个 inner epoch
    

一个很重要的观察是：当允许更长输出（65k）之后，**模型的输出长度和效果在 8.2k step 处出现明显跃迁**。  
这就是在很多解读里看到的“aha moment/自发变强”的基础条件之一：**给足够的推理 token 预算 + 用可验证奖励驱动探索**。

## 6. 为什么还要做 DeepSeek-R1：从“会推理”到“可用、可控、可读”

报告非常直白：R1-Zero 虽然推理强，但会出现 **可读性差、语言混杂** 等问题。于是 R1 的 pipeline 目标是把这些工程化问题解决掉。

下面按阶段讲每一步在干什么。

## 7. 阶段 A：Cold Start SFT（R1 Dev-1）——先把“思考风格”扶正

