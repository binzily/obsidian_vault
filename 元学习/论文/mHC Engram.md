## 主线其实很前沿：一个管“深度信息流”，一个管“时间跨度信息流”

- **mHC** 解决的问题：当你把残差连接“升级成更复杂、更宽的残差流”后，模型在大规模训练时会不稳定（loss 突然炸、梯度范数异常），怎么让它重新稳定、还能保住收益。
    
- **ENGRAM** 解决的问题：LLM 的上下文窗口有限，长对话/多会话就会遗忘；很多记忆系统做得很复杂（图结构、多级检索、类似操作系统调度），ENGRAM 主张“用更简单、可复现的 typed memory + dense retrieval 也能做到 SOTA/接近 SOTA”。

如果把它们抽象成一句话：
- mHC 是在 **“网络深度方向”** 做稳定的信息守恒；
- ENGRAM 是在 **“交互时间方向”** 做稳定的信息保存与检索。
---
# Part A：mHC（Manifold-Constrained Hyper-Connections）——把“超连接”拉回可规模化训练

## A1. 从 0 到 1：为什么残差连接这么重要

经典残差连接（ResNet/Transformer 的残差思想）可以写成：$x_{l+1} = x_l + F(x_l, W_l)$
关键点是它有一条“身份映射”（identity mapping）通路：即使 $F(\cdot)$ 学不好，信息也能沿着 $x_l \to x_{l+1}$ 直接流动，这对深层网络的稳定训练至关重要。mHC 论文在引言里也明确把稳定性归因于 residual 的 identity mapping。

---
## A2. Hyper-Connections 在做什么：把残差流变“多车道 + 可学习互通”

ByteDance 的 **Hyper-Connections (HC)**（ICLR 2025）提出：传统残差连接在 **梯度消失** 与 **表示塌缩之间存在“跷跷板”权衡（Pre-Norm 更稳梯度但易塌缩；Post-Norm 反之），那能不能让连接强度变成可学习的？

HC 的关键操作是：

1. 把原本 (C) 维的残差流扩成 $n\times C$（可以理解成 **n 条并行残差“流/车道”**）。
    
2. 引入三类可学习映射：
    - $H^{pre}_l$：把 (nC) 聚合成 (C) 作为层输入
        
    - $H^{post}_l$：把层输出从 (C) 再映射回 (nC)
        
    - **$H^{res}_l \in \mathbb{R}^{n\times n}$**：在这 n 条残差流之间做“混合/交换”（这是最关键也最危险的部分）
        

在 mHC 论文里，HC 单层传播写为：$x_{l+1} = H^{res}_l x_l + (H^{post}_l)^{\top} F(H^{pre}_l x_l, W_l)$

其中 $x_l$ 维度是 (nC)。

直觉：HC 让不同深度、不同“车道”的信息可以更灵活地交换，理论上表达力更强，实验上也经常更好。

---
## A3. HC 为什么会在大规模训练里崩：问题集中在 “$H^{res}$” 的连乘

mHC 论文做了一个很清晰的诊断：

- 当你把 HC 堆很多层时，残差流的“身份映射”不再是简单的 $I$，而变成很多层的 $H^{res}$ 连乘（类似 $(\prod H^{res})$。
    
- 因为 $H^{res}_l$是**完全不受约束**的可学习矩阵，这个连乘几乎必然偏离 identity，导致信号在前向/反向传播中出现 **爆炸或消失**。
    
- 论文用一个“增益幅度”指标（基于复合映射的行/列和的最大绝对值）量化这种放大效应，在 27B 实验里，复合映射的增益峰值能到 3000，这就是“残差流爆炸”的直接证据。
    
- 现象层面：HC 训练会出现 loss 在某个 step（论文示例约 12k step）突然飙升，并且与梯度范数异常相关。

一句话总结：**HC 把“残差的稳定身份通路”改成了“很多不受约束的混合矩阵连乘”，于是稳定性没了。**

---
## A4. mHC 的核心技术：把 (H^{res}) 投影到“可守恒”的矩阵流形上

mHC 的做法很“硬核但干净”：

- 他们把 $H^{res}_l$不再当成任意矩阵，而是投影到一个特定流形：**Birkhoff polytope（双随机矩阵集合）**。
    
- **双随机矩阵（doubly stochastic matrix）** 的性质：每一行和每一列的元素和都等于 1。

这带来两个关键好处（也是论文抓住的“稳定性来源”）：

1. **凸组合解释**：$H^{res}_l x_l$相当于对 n 条残差流做凸组合（不会凭空把“平均强度”放大很多倍），因此能更好地控制信号尺度，避免爆炸/消失。（PS：①凸组合：加权系数非负且总和为1；②残差流指F(·)）
    
2. **乘法闭包（closure）**：双随机矩阵相乘仍是双随机矩阵，所以即使跨很多层连乘，复合映射仍保有这种“守恒/稳定”结构，等价于把 identity mapping 的稳定性“结构化地找回来了”。
    
---
## A5. 关键算法：Sinkhorn–Knopp 把任意矩阵“拉回”双随机

mHC 用 **Sinkhorn–Knopp** 做投影（经典 1967 算法，但在这里用得很恰当）：

1. 先把未约束矩阵 $\tilde{H}^{res}_l$ 做指数变换得到正矩阵 $M^{(0)} = \exp(\tilde{H}^{res}_l)$。
    
2. 然后迭代地做“列归一化 + 行归一化”：$M^{(t)} = T_r( T_c(M^{(t-1)})$
$T_r$表示把每一行归一到和为 1，$T_c$ 表示把每一列归一到和为 1。迭代收敛后就是双随机矩阵。

3. 论文里实际用 $t_{\max}=20$ 次迭代作为工程可用的折中。

同时，mHC 也对 $H^{pre}$、$H^{post}$ 做了可控的约束形式（sigmoid 等），但最核心的“稳定性闸门”就是 **$H^{res}$ 的双随机约束**。

---
## A6. 工程化部分

mHC 不只是提出一个数学约束，还明确回答了一个现实问题：

> 你把残差流扩大了 n 倍，多了矩阵运算、归一化、投影迭代，训练会不会慢到不可用？

他们给了三类系统优化：

- **Kernel Fusion**：把多个操作融合，减少内存读写与 kernel launch 开销，并针对 mHC 的大维度隐藏态优化 RMSNorm 的执行顺序等。
    
- **Recomputing（选择性重计算）**：通过分块保存/重算中间激活，控制显存峰值（表 3 讨论了哪些激活存、哪些重算，以及分块大小如何影响峰值）。
    
- **DualPipe 通信-计算重叠**：扩展 pipeline schedule，把 mHC 带来的额外算子塞进通信空隙里，减少“纯等待”。

最终他们报告：在 expansion rate (n=4) 时，大规模训练额外时间开销约 **6.7%**。
这点很重要：mHC 不是“只在 toy setting 有用”，而是明确奔着大模型训练去的。

---
## A7. 作为学习者，读 mHC 最应该“抓住并复现”的 3 个点

1. **不稳定性的数学来源**：HC 的问题不是“某个 trick 没调好”，而是 $\prod H^{res}$破坏 identity mapping 导致信号增益失控。
    
2. **双随机约束为什么有效**：凸组合 + 乘法闭包，让“跨层复合映射”仍保有守恒结构。
    
3. **Sinkhorn 投影怎么实现**：$\exp$ 保正、行列交替归一、固定迭代次数（20）是可落地的工程解。
    
---
# Part B：ENGRAM —— 用“typed memory + dense retrieval”做长期记忆，不靠复杂系统

## B1. Engram 本质是什么（一句话）

**Engram 是插在 Transformer 里的一个“可训练、可扩展、按需查表”的记忆模块**：每个 token 只触发固定次数的查表（所以每 token 计算量几乎不随记忆规模增长），但你可以把“表”做得非常大（所以总参数量/知识容量可以继续扩）。

把它想成：模型除了“算”（attention/MLP）之外，多了一个“**查**”（lookup）的原语；查到的是一组**静态嵌入向量**，再用当前上下文对这些向量做“动态融合”。

---
## B2. 为什么要 Engram（它解决的痛点）

### 痛点：Transformer 没有“原生检索”，只能用计算硬模拟

很多语言模式是**局部、静态、模式化**的（命名实体、多词短语、公式化表达等）。传统 Transformer 往往需要用多层 attention/MLP 去“重建”这种近似查表的行为——等于用昂贵的算力做本该 O(1) 查表的事。

### Engram 的定位：与 MoE 互补的“第二种稀疏”

- **MoE（条件计算）**：通过稀疏激活专家来扩容量，但本质还是“算”。
    
- **Engram（条件记忆）**：通过稀疏查表来扩容量，让模型把“静态模式/局部依赖”的负担交给记忆模块，从而把主干网络的有效深度和注意力容量“解放出来”去做更复杂的推理与全局建模。

论文/报道里也给了这种机制解释：Engram **减轻浅层对静态模式的重建负担**、**释放注意力容量**，并观察到对长上下文检索和推理任务有增益。

---
## B3. Engram 怎么工作（两阶段：检索 Retrieval + 融合 Fusion）

### A. 检索阶段：把“局部上下文”映射到“静态记忆条目”

1. **提取后缀 N-gram**：对序列中当前位置，取若干个后缀 N-gram（例如最近 1/2/3/… 个 token 组成的片段）。
    
2. **分词器压缩 / 词表投影（vocabulary projection）**：把原始 token id 映射到“规范化标识”（例如 NFKC、大小写归一等），减少有效词表规模、提高记忆单元语义密度。
    
3. **多头哈希（multi-head hashing）做确定性寻址**：对每种 N-gram 阶数 _n_，用 _K_ 个相互独立的哈希头把 N-gram 映射到表里的槽位（slot）。多头的目的主要是降低哈希冲突影响。
    

> 结果：对每个 token，你会得到固定数量的“索引”，从超大表里 **O(1)** 抓回一组嵌入向量。

### B. 融合阶段：把“静态向量”变成“对当前语境有用的信息”

4. **上下文感知门控（context-aware gating）**：因为查到的向量本质是“静态先验”，会受多义性/哈希冲突噪声影响，所以再用当前隐藏状态去调制（类似注意力思想的门控）。
    
5. **轻量卷积精炼**：对检索到的表示做一个轻量级卷积操作进一步精炼。
    
6. **与多分支架构集成**：把 Engram 分支的输出并回主干网络（你可以把它理解为“除了 attention 分支、MLP 分支，又多了一个 memory 分支”）。

---
## B4. “怎么用”分两种：作为研究者用、作为应用工程师用
### 4.1 作为研究者：你要“把它接进 Transformer 并训练”

Engram 不是一个“推理时外挂插件”（不像 RAG/向量库那样可以不改模型就直接加），它是**模型结构的一部分**：需要在预训练/继续训练时端到端学习记忆表和门控等参数。

你可以按以下最小落地路径做（不需要一开始就复刻全部系统优化）：

**步骤 1：选一个基座模型（Dense 或 MoE 都可）**  
论文强调它与 MoE 互补，等参数/等 FLOPs 条件下做“稀疏预算分配”，并观察到存在“U 形”的最优分配区间（大约把一部分稀疏预算分给 Engram 更好）。

**步骤 2：实现最小 Engram（先跑通功能）**  
最小实现只需：

- 后缀 N-gram 提取
    
- 哈希到 slot → embedding table gather（稀疏查表）
    
- 一个 gating（用 hidden state 做缩放/加权）
    
- 残差加回主干
    
先不做复杂卷积、不做多级缓存。

**步骤 3：决定关键超参（你可以用“固定激活、扩总量”的思路记）**

- N-gram 最大阶数 N
    
- 每阶哈希头数 K（冲突-开销权衡）
    
- 槽位数 M（决定记忆规模/总参数量）
    
- 记忆向量维度 d_mem（通常对齐模型维度或可投影）
    
关键点是：**每 token 检索槽位数固定**，所以推理/训练 FLOPs 近似不随 M 增长。

**步骤 4：做 Ablation（你会更快建立直觉）**

- 只开 1-gram vs 开多阶 N-gram
    
- 只开 1 个哈希头 vs 多头
    
- 有无 vocabulary projection
    
- 有无 gating / 有无卷积精炼
    
这些都对应报道中点到的结构件。

### 4.2 作为工程师：你通常“用带 Engram 的模型”，而不是自己加

如果未来 DeepSeek（或其他团队）发布“集成 Engram 的 checkpoint/API”，你作为应用方最现实的用法是：**直接选用该模型**，它的“记忆”是参数级的，并不等价于“聊天记忆/个人记忆”。

---
## B5. 容易误解的点：Engram ≠ RAG ≠ 对话记忆

- **Engram**：参数内的条件记忆（查的是“训练出来的表”），强调 O(1) 查表与可扩展。
    
- **RAG/向量库**：外部检索，查的是你给的文档库；不改模型也能加，但系统链路更复杂。
    
- **对话记忆**：产品层面的会话状态管理；和论文的“条件记忆”不是一回事。

## B6. 总结提炼

传统 Transformer 只有“算”，没有“查”，所以很多局部静态知识只能靠多层计算去重建，效率低。Engram 提出一种与 MoE 条件计算互补的“条件记忆”：对每个 token 提取后缀 N-gram，用确定性多头哈希 O(1) 查表取回静态嵌入，再用当前隐藏状态做门控融合并回主干。这样每 token 只做固定次数 lookup，计算量不随记忆规模增长，但记忆表可以做得很大，还能在系统上把大表放到主机内存做预取，从而把 attention/网络深度释放出来做更复杂推理与长上下文建模。

---
## C1. mHC：建议复现到什么程度

### 最小原型（你一周内能做完的版本）

- 用 numpy 实现 Sinkhorn‑Knopp：随机矩阵 → exp → 行列交替归一 20 次 → 验证行和列和≈1。
    
- 在一个 toy transformer block 里把 residual stream 扩成 n=2 或 n=4，并只实现 (H^{res}) 的混合（先别管复杂 kernel）。对比：
    
    - 不约束的 $H^{res}$
        
    - Sinkhorn 约束后的 $H^{res}$

观察训练稳定性（loss/grad norm）是不是更接近论文描述的现象。
### 必须讲清的因果链（面试/组会要用）

- HC 的收益来自“多流 + 可学习混合”，但不约束会导致 $\prod H^{res}$ 偏离 identity → 信号增益失控 → 不稳定。
    
- mHC 通过把 $H^{res}$ 约束到双随机矩阵（凸组合 + 乘法闭包）恢复守恒性质 → 稳定。