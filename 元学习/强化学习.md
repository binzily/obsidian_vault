## 0.Policy Gradient
### 0.1 先把LLM 放进 RL 记号里：token = action，前缀 = state
对一个固定 prompt $x$： 
- 状态：$s_t = (x, y_{<t})$（prompt + 已生成前缀） 
- 动作：$a_t = y_t$（第 $t$ 个 token） 
- 轨迹：$\tau = (s_1, a_1, \dots, s_T, a_T)$ 等价于整段 $y$ 
语言模型策略就是 $\pi_\theta(a_t | s_t) = \pi_\theta(y_t | x, y_{<t})$

### 0.2 原始公式推导
令策略（actor）是参数分布 $\pi_\theta(a | s)$。优化目标写成“轨迹回报的期望”： $$J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)]$$其中 $\tau = (s_0,a_0,\dots,s_T,a_T,s_{T+1})$ 是一条轨迹（在 LLM 里就是：给定 prompt 后采样出一整段序列；$a_t$ 对应 token，$s_t$ 对应“prompt+已生成前缀”）。 
轨迹概率可以展开为（环境转移不含 $\theta$）： $$P(\tau | \theta) = \rho_0(s_0) \prod_{t=0}^T P(s_{t+1} | s_t,a_t) \pi_\theta(a_t | s_t)$$
用log-derivative trick 把 $\nabla J(\theta)$ 写成期望（积分）：$$J(\theta) = \int P(\tau | \theta) R(\tau) d\tau$$ 求梯度： $$\nabla_\theta J(\theta) = \int \nabla_\theta P(\tau | \theta) R(\tau) d\tau$$ 用 log-derivative trick: $$\nabla_\theta P(\tau | \theta) = P(\tau | \theta) \nabla_\theta \log P(\tau | \theta)$$ 代入得: $$\nabla_\theta J(\theta) = \int P(\tau | \theta) \nabla_\theta \log P(\tau | \theta) R(\tau) d\tau = \mathbb{E}_{\tau\sim\pi_\theta}\left[\nabla_\theta \log P(\tau | \theta) R(\tau)\right]$$ 而因为环境部分不依赖 $\theta$, 所以 $$\nabla_\theta \log P(\tau | \theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t)$$ 于是得到最“原始”的 REINFORCE 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_\tau\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]$$
**还完全是 on-policy**：期望是对“当前策略 $π_{θ}$​ 诱导出的轨迹分布 $p_\theta(\tau)$”取的。
“重要性采样（IS）”出现的唯一原因是：**你想用别的分布采到的样本来估计这个期望**——典型就是用旧策略 $\pi_{\text{old}}​$ 采到的轨迹来更新新策略 $\pi_\theta$​。
1) 从 on-policy 期望到 IS：就是一个换测度恒等式 对任意函数 $f(\tau)$，只要 $p_{\text{old}}(\tau) > 0 \Rightarrow p_\theta(\tau) > 0$（支持集覆盖），都有： $$\mathbb{E}_{\tau\sim p_\theta}[f(\tau)] = \sum_\tau p_\theta(\tau)f(\tau) = \sum_\tau p_{\text{old}}(\tau)\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)}f(\tau) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[w(\tau)f(\tau)\right]$$ 这一步就是重要性采样的定义：用权重 $w(\tau) = \frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}$ 把“在 $p_{\text{old}}$ 下的期望”变成“在 $p_\theta$ 下的期望”。
2) 把 $f(\tau)$ 换成你的 policy-gradient integrand 令 $$f(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)$$ 代回去就得到 off-policy 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)} \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)\right]$$ 这就是“从 on-policy policy gradient 到重要性采样”的那一步：只是把期望的采样分布从 $p_\theta$ 换成 $p_{\text{old}}$。
3) 为什么 $w(\tau)$ 会变成“连乘的 ratio”（而且环境转移会抵消） 
       轨迹分布可写成（环境转移不依赖 $\theta$）： $$p_\theta(\tau) = \rho_0(s_0) \prod_{t=0}^T \pi_\theta(a_t | s_t) P(s_{t+1} | s_t,a_t)$$因此比值： $$\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)} = \prod_{t=0}^T \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$所有 $P(\cdot)$ 都消掉了，所以 IS 权重只依赖两套策略，不依赖环境动态。Sutton & Barto 在讲 off-policy MC 时把这个“转移概率抵消”写得很明确，并给出了乘积形式的 ratio（他们用 $\mu$ 表示行为策略/旧策略）。 映射到  LLM: $s_t = (x,y_{<t})$，$a_t = y_t$，于是 $$w_{\text{seq}}(y) = \prod_t \frac{\pi_\theta(y_t | x,y_{<t})}{\pi_{\text{old}}(y_t | x,y_{<t})}$$这就是你一直写的“序列级重要性比率”。问题在于：**用这个 $w_{seq}(y)$ 做梯度估计方差巨大**（尤其长序列时），**TRPO/PPO 走的是另一条路：改目标函数、做“局部近似 + 信赖域”**，而不是严格轨迹 IS。
4) TRPO/PPO 的目标是：**不要用这个高方差的轨迹级 $w(\tau)$**，改用更稳定的“局部近似 + 信赖域”。
       TRPO 先定义策略表现（折扣回报）$\eta(\pi)$，以及 $Q^\pi, V^\pi, A^\pi$（优势） 然后给出一个关键恒等式（performance difference）： $$\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{\tau\sim\tilde{\pi}}\left[\sum_{t\ge0} \gamma^t A^\pi(s_t,a_t)\right]$$
       直觉：**新策略的期望回报 = 旧策略期望回报 + 在新策略采样到的轨迹上，旧策略优势函数的折扣和，我们需要做的是优化后者**。
       并进一步“把沿时间步求和”改写成“按状态出现次数加权”： $$\eta(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A^\pi(s,a)$$直接优化这个式子很难（策略一变，后续会走到哪些状态的分布就全变了）。
       符号解释： 
    - $\tilde{\pi}$：你想更新得到的新策略 
    - $\pi$：当前/旧策略（后面常写 $\pi_{\text{old}}$）
    - $\rho_{\tilde{\pi}}(s)$：折扣访问频率，直观就是“在 $\tilde{\pi}$ 下状态 $s$ 被访问到的加权次数” 
    - $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$：动作相对平均水平的好坏 
	到这里为止，TRPO 还没引入任何重要性采样比率。
       
   5) TRPO 的“局部近似 surrogate”从哪来？ 
         TRPO 说：我先忽略“状态访问分布变化”，把 $\rho_{\tilde{\pi}}$ 用 $\rho_\pi$ 近似（$\rho_\pi$ 是“旧策略跑出来的数据分布”，你手里真的有样本可以估计它，所以 surrogate 可算、可做 SGD。），得到局部 surrogate： $$L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s) A_\pi(s,a)$$并且它证明：这个 surrogate 在 $\theta = \theta_0$ 处与真目标一阶相同（梯度相同、函数值相同）。 这就是理论支撑：TRPO 优化的不是“严格等于真目标”的东西，而是“在旧策略附近一阶正确”的 surrogate。 所以只要你每次更新别走太远，优化 surrogate 就能稳定提升真目标。 这也解释了“trust region / KL 约束”为啥必要：它是用来保证“别走太远”。
>这两个式子唯一差别是把 $\rho_{\tilde{\pi}}$ 换成了 $\rho_\pi$；而在 $\tilde{\pi} = \pi$ 处，$\rho_{\tilde{\pi}} = \rho_\pi$，并且“$\rho$ 对 $\theta$ 的导数项”会乘上一个在每个状态都为 0 的量：$\sum_a \pi(a|s) A^\pi(s,a) = 0$，所以导数差异消失。

6) 从 TRPO surrogate 到 PPO 里熟悉的 $r_t \hat{A}_t$ 现在看 surrogate 里的关键项： $$\sum_a \tilde{\pi}(a|s)A_\pi(s,a)$$这里的s特指某一个s，但是在TRPO的真等式本来外层就对状态加权。
       整理一下（$\tilde{\pi} \equiv \pi_\theta$）：$$\sum_a \pi_\theta(a|s)A(s,a) = \sum_a \pi_{\text{old}}(a|s)\frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)}A(s,a) = \mathbb{E}_{a\sim\pi_{\text{old}}(\cdot|s)}\left[r(a,s)A(s,a)\right]$$ PPO 论文在讲 TRPO 背景时就直接写了：TRPO 最大化 $\hat{\mathbb{E}}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \hat{A}_t\right]$ 并加 KL 约束。 并且 PPO 论文也把这个 surrogate 写成： $$L^{\text{CPI}}(\theta) = \hat{\mathbb{E}}_t\left[r_t(\theta)\hat{A}_t\right]$$**这里的“t”本质就是把 rollout 收集到的一堆 $(s_t, a_t)$（LLM 里就是 token 位点）摊平后的样本索引，这里可以token-level loss（GRPO）也可以sample level loss（DAPO）。** 
       然后再引入 clip。 到这里你应该能看到：PPO 的 $r_t \hat{A}_t$ 不是从 $\prod r_t$ 直接“化简”出来的；**它来自 TRPO 的“局部 surrogate + 在每个 state 上对 action 的期望用单步 IS 改写”**。

## 1.RLHF + PPO训练LLM的全流程：

### 1.1训练前的准备

**SFT** 阶段：首先需要一个已经完成预训练的 LLM ，它的训练任务是“Next Token Prediction”，SFT就是在这个 LLM 的基础上用“指令-回答”数据集监督微调，最后得到的模型就是 SFT 模型。
**RM** 阶段：复用SFT模型的架构来构建Reward Model。和Critic一样，它也有一个特殊的“奖励头”，输出一个标量值。训练数据都是一个 `(prompt, SFT模型回答_1, SFT模型回答_2, 排序结果)` 的组合。

| 模型                        | 来源                         | 在PPO阶段的状态           | 训练阶段            |
| ------------------------- | -------------------------- | ------------------- | --------------- |
| Actor $π_θ$               | 复制自SFT模型                   | 可训练，在PPO中不断更新       | PPO             |
| Critic $V_φ$              | 架构和权重来自SFT模型，但有一个随机初始化的价值头 | 可训练，在PPO中与Actor同步更新 | PPO             |
| Reference Model $π_{SFT}$ | 就是SFT模型本身                  | 冻结，参数固定不变           | SFT             |
| Reward Model (RM)         | 架构来自SFT模型，在人类排序数据上单独训练     | 冻结，参数固定不变           | Reward Modeling |
### 1.2训练主循环

###### 步骤一：数据收集 (Rollout / 与环境交互)
一句话：我们让当前的Actor模型 $π_θ$ 生成一大批对话数据。具体流程：
1. 从一个数据集中随机抽取一批 `prompt`。
2. 对于每一个 `prompt`，让Actor $π_θ$ 自回归地（一个词一个词地）生成一个完整的回答。
3. 在生成每一个词 $a_t$ 的过程中，我们需要记录下一整条“轨迹 (trajectory)”所需的所有信息：
    - **状态 $s_t$**: 生成 $a_t$ 之前的文本序列。
    - **动作 $a_t$**: 实际生成的那个词 (token)。
    - **动作概率 $log(π_θ(a_t|s_t))$**: Actor模型在状态 $s_t$ 下，选择动作 $a_t$ 的对数概率。
    - **价值估计 $V_φ(s_t)$**: Critic模型对当前状态 $s_t$ 的打分。
    - **KL散度奖励 $r_{KL}$**: 这是我们之前讲的“过程分”，用于稳定训练。
4. 当一个回答生成完毕（遇到结束符 $<eos>$），我们计算它的最终奖励：
    - 将完整的 `prompt + 回答` 输入给 RM，得到一个最终分数 $R_{final}$。
    - 将 KL散度奖励 和 最终奖励 结合。通常是这样：
        - 对于中间的每一步 `t`，奖励 $r_t = -β * KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$。
        - 对于最后一步 `T`，奖励 $r_T = R_{final} + r_{KL}$。
5. 重复以上过程，直到收集到足够多的轨迹数据（例如，几千条完整的对话）。现在，我们手上有一个巨大的经验池，里面装满了 $(s_t, a_t, log(π_θ(a_t|s_t)), V_φ(s_t), r_t)$ 这些数据点。
###### 步骤二：优势计算
==“构造优势”本身就是在做方差降低：把回报做“去均值/中心化”的 baseline（控制变量） 技巧。==
==优势的定义是 `A(s,a) = Q(s,a) - V(s)`。问题在于，真实的`Q`值我们永远不知道。我们只能用不同的方法去**估计**它。==
两种极端的估计方法：
1. TD估计 (只看一步): $Â_{TD}(t) = r_t + γV(s_{t+1}) - V(s_t) = δ_t$
    - 优点: 方差低。因为它主要依赖于相对稳定的函数`V`，而不是充满随机性的真实奖励。
    - 缺点: 偏差高。因为$V(s_{t+1})$身就是一个不准确的估计，这个偏差会引入到$Â$的计算中。
2. 蒙特卡洛估计 (看到底): $Â_{MC}(t) = (r_t + γr_{t+1} + γ^2r_{t+2} + ...) - V(s_t)$
    - 优点: 无偏。因为它完全基于真实发生过的奖励序列，没有任何估计成分。
    - 缺点: 方差高。因为真实的奖励序列每次都可能因随机性而剧烈波动。
我们可以定义一个“2步优势估计”：$Â^2(t) = r_t + γr_{t+1} + γ^2V(s_{t+2}) - V(s_t)$。  
可以定义一个“k步优势估计”：$Â^{k}(t) = (Σ_{l=0}^{k-1} γ^l r_{t+l}) + γ^k V(s_{t+k}) - V(s_t)$
GAE的公式 $Â_t = δ_t + (γλ)δ_{t+1} + (γλ)^2δ_{t+2} + ...$ 
                  $Â_t = δ_t + (γλ) * Â_{t+1}$
                  $\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_{\text{old}}(s_{t+1}) - V_{\text{old}}(s_t)$
                  PS：λ=1意思是把所有长度的 TD 残差都权重一样地累加
- `t`时刻的GAE优势，不仅仅是`t`时刻的TD-Error $δ_t$。
- 它还包含了对未来`t+1`时刻TD-Error $δ_{t+1}$的一次折扣考虑。
- 它还包含了对未来`t+2`时刻TD-Error $δ_{t+2}$的二次折扣考虑。
- ...以此类推，直到序列结束。
###### 步骤三：模型优化
1. Actor Loss：$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$
2. Critic Loss：$L_{VF}(φ) = E[ (V_φ(s_t) - V_t^{target})^2]$ = $\mathbb{E}\left[\left(V_φ(s_t) - R_t\right)^2\right]$
			   $V_t^{\text{target}} = \hat{R}_t = \hat{A}_t + V_{old}(s_t)$
3. Entropy of the policy Bonus：$S = E[ -Σ_a π_θ(a|s_t) * log(π_θ(a|s_t)) ]$，最大化熵（鼓励探索）。

>重要性采样（==RLHF的PPO中用到的重要性采样是”序列级“==）：$$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$
>Q:理论上用 $\mathbb{E}_{a\sim\pi_\theta(\cdot|s_t)}[\hat{A}(s_t,a)] = \sum_a \pi_\theta(a|s_t)\hat{A}(s_t,a)$ （没有重要性采样）当作Actor的优化目标是完全可以的。那为什么不这样写而是引入重要性采样呢？
>A：$\hat{A}(s_t,a)$ 是 $\pi_{old}$ 得到的，$\hat{A}(s_t,a)$ 里所有数据（reward, value, log‑probs）都是**基于旧策略分布**，但是目标函数应该是**对当前策略分布的期望**。

>Q：LLM用到的RLHF是policy based还是value based？
>A: 本质上是policy-based（策略梯度）为主，但训练时通常带一个 **critic/value function** 当 baseline ——也就是 **actor–critic**。
>**用到了 value**，但不是 “value-based RL” 的那种（以学 Q 为主、用 Q 决策）。

>Q:为什么LLM很少用纯value-based（比如Q-learning）？
>A：动作空间是“词表大小（几万）× 每个位置”的巨大离散空间，且序列很长，直接学全动作的 Q(s,a)并做 max/bootstrapping 通常更难、更不稳定；策略梯度更自然。
>Q：什么是bootstraping？
>A：bootstraping（自举），用自己当前学到的价值函数预测，来当作“未来回报”的一部分目标，而不是等到把整段轨迹都走完再算真实回报。比如​用$r_t + \gamma V(s_{t+1})$ 或 GAE 来构造$\hat{R}_t$和$\hat{A}_t$。
>bootstraping的优点就是方差小；缺点就是有偏差。

> **on-policy**：你用“当前策略 $\pi_\theta$​”采样的数据，也用来更新“当前策略”。指**不长期用老数据**，不是指“每个 SGD step 都完全 on-policy”。比如在一轮rollout中，采样到的数据会被分为很多mini-batch。$\pi_{\text{old}}$ 就是rollout时的策略，$\pi_\theta$ 在一个mini-batch就会更新一次，第一个mini-batch时，重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ = 1。**第一轮更新后， $\pi_\theta$​ 已经变了，但数据还是旧策略采的**。因此后面那些更新，从严格意义上讲就“off-policy 了”，GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”
> **off-policy**：你用“别的策略”（旧策略/行为策略）采样的数据，拿来更新“当前策略”。
## 2.GRPO

在 **GRPO（Group Relative Policy Optimization）** 里确实**没有 critic / value function**，所以它不走 “$A=\hat R - V(s)$” 这条路，而是用 **“同一个 prompt 下多次采样得到的一组回答（group）”** 来构造一个 **baseline**，从而得到优势（advantage）。
对同一个prompt x，从当前策略π_θ采样K个回答y₁,…,y_K。 
1. 先算每个回答的标量奖励（通常含KL惩罚）： $r_i = R(x,y_i) - \beta \text{KL}(\pi_\theta(\cdot|x) \parallel \pi_{\text{ref}}(\cdot|x))$
2. 用组内baseline（最常见是均值）： $b = \frac{1}{K} \sum_{j=1}^K r_j$ 
3. 定义优势（相对同组其他回答“好多少”）： $\hat{A}_i = r_i - b$ 很多实现还会做 组内标准化（更稳）： $\hat{A}_i = \frac{r_i - \mu}{\sigma + \epsilon}, \quad \mu = \frac{1}{K} \sum_{j=1}^K r_j, \quad \sigma = \sqrt{\frac{1}{K} \sum (r_j - \mu)^2}$ 
>直觉：同一个prompt的这组回答里，比平均好就正优势，比平均差就负优势。这样就不需要一个单独的critic来预测V(s)当baseline了。

## 3.DPO
#### DPO 的核心：用“成对偏好”替代 advantage
数据是一对回答：$(x, y^+, y^-)$，表示在prompt x下人更喜欢$y^+$而不是$y^-$。 
DPO定义一个“偏好logit”（可以把它当成一种相对优势）： $\Delta_\theta = \left[\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x)\right] - \left[\log \pi_{\text{ref}}(y^+|x) - \log \pi_{\text{ref}}(y^-|x)\right]$ 
然后用logistic loss： $L_{\text{DPO}}(\theta) = -\mathbb{E}\left[\log \sigma(\beta \Delta_\theta)\right]$ 
- $\sigma$: sigmoid 
- $\beta$: 温度/强度超参（控制偏好推动的力度） 
- ref项让更新不要偏离参考模型太多（效果上类似RLHF里的KL约束）

#### “那优势在哪？”
如果你非要找一个“优势”的影子，它就是这个 成对差值的logit: 
- 在PPO/GRPO里：优势是“这个动作/回答比baseline好多少” 
- 在DPO里：你不需要baseline，因为标签直接告诉你$y^+$应该比$y^-$好 所以DPO直接优化“偏好差”: $\Delta_\theta$ 越大越好 
- 可以把$\Delta_\theta$看成一种 pairwise advantage（“偏好优势”），但它不是$Q-V$那种per-state的优势，也不需要critic去估$V$。

==强化学习中神经网络本质：把你在环境里交互得到的经验（状态、动作、奖励、下一个状态）压缩成一个可微分的函数，用来**决策**（怎么选动作）和/或**评估**（这个状态/动作有多好），从而最大化长期回报。==

## 4.强化学习主流算法总结
现在主流的大模型RL微调，基本都跑在PPO家族上：一开始是标准PPO，再到DPO这类“去掉value、走偏好”的方向，后面DeepSeek提了GRPO，Qwen又在这个基础上提出了DAPO、GSPO，再往前一步就是现在的SAPO。整个演进有一个共同的主线，就是在解决两件事：**训练稳定性和样本效率**，尤其是在长CoT、MoE这种比较折腾的场景下。

GRPO 的创新是用组内相对比较 + token 级 PPO 框架，去掉 value 和 RM；它的问题是 **reward 粒度和更新粒度不一致**，尤其是在长序列和 MoE 上不稳定。
>用于“任务好坏”的主奖励（RM/正确性）是序列级的，而参数更新却要对每个 token 的 logprob 做梯度；
>于是主奖励会被“广播”到所有 token，上下文里哪个 token 真正导致成功/失败很难归因。
>KL/熵这些 token 级项，更多是“正则/约束/探索”，不是“任务信用分配”。

### 4.1 PPO>DPO>GRPO>DAPO>GSPO>SAPO
DAPO、GSPO、SAPO都在解决同一类痛点：**长序列 + 多个 mini-batch 更新会带来 off-policy；如果用“token 级重要性比率 + token 级裁剪”，方差会沿序列累积，容易不稳定甚至崩溃**。GSPO 论文把这个问题说得很直：GRPO 把重要性权重用在每个 token 上，本质上“**用单一样本去做分布校正**”会引入高方差噪声，并随长序列累积；而奖励是整段给的，所以“优化单位应匹配奖励单位”。

>"分布校正"：你手里有的数据是按 **旧分布/旧策略** $\pi_{\text{old}}$​ 采出来的，但你想优化的是 **新分布/新策略** $\pi_\theta$。重要性采样的做法“**用权重把分布从 old 校正到 new**”，所以又RLHF中PPO算法的重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ 。
>
>"单一样本做分布校正"：在每个 token 位置只采到一个 token，就试图用它的概率比率去代表整个位点的分布差异；理论上需要大量样本平均才稳定，现实里会变成高方差噪声。
>在 LLM-RL（GRPO 那套）里，每个 token 位置 t 的“动作”就是生成的那个 $token \ a_t$​。但在某个前缀状态 $s_t$ 下，你通常只采到 **一个** token（一次只走了一条路径）。于是你用 **这一条路径上这一个 token** 的比率$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$来做“分布校正”，**只采样一次就用重要性采样本身就会带来偏差（或者说是引入了噪声）。**

在 LLM 的 RL 里，一次 rollout 得到的是整段序列 $y_{1:T}$，而你常见的训练做法是：**一大批 rollout 数据切成多个 mini-batch，做多次梯度更新**——这一步直接把学习推向 off-policy：数据来自旧策略 $\pi_{\text{old}}$​，但你在优化新策略 $\pi_\theta$​。GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”。
#### 4.1.1 从最基本的理论：重要性采样在“随机变量本身的分布”上做换测度 
1) 设你要的目标是（prompt $x$ 固定）： $$J(\theta) = \mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[R(x,y)]$$这里 $y = y_{1:T}$ 是整段序列，奖励 $R(x,y)$ 也是整段函数。 如果你手里只有旧策略 $\pi_{\text{old}}$ 采样来的序列 $y$，那要把期望换成在旧分布下： $$\mathbb{E}_{y\sim\pi_\theta}[R] = \sum_y \pi_\theta(y|x)R(x,y) = \sum_y \pi_{\text{old}}(y|x)\underbrace{\frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}}_{\text{正确的IS权重}}R(x,y) = \mathbb{E}_{y\sim\pi_{\text{old}}}\left[w_{\text{seq}}(y)R(x,y)\right]$$所以理论上对应的权重就是 $$w_{\text{seq}}(y) = \frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}$$这一步就是经典重要性采样恒等式，没有额外假设。 在 RL 文献里，这个 $w$ 叫 trajectory-wise / ordinary importance sampling ratio：用“轨迹在目标策略下发生的概率”除以“轨迹在行为策略下发生的概率”。Thomas & Brunskill 的 high-confidence OPE 论文就把无偏估计写成：对每条轨迹用 $\Pr(\tau|\theta)/\Pr(\tau|\theta_i)$ 乘以回报，并进一步展开成沿时间步的概率比连乘。

2) 为什么它在序列生成里会变成“token 比率的连乘”（也就是你说的序列级 ratio） LLM 自回归生成的序列概率本来就按链式法则分解： $$\pi_\theta(y|x) = \prod_{t=1}^T \pi_\theta(y_t | x, y_{<t})$$ 因此 $$w_{\text{seq}}(y) = \frac{\prod_t \pi_\theta(y_t|x,y_{<t})}{\prod_t \pi_{\text{old}}(y_t|x,y_{<t})} = \prod_{t=1}^T \underbrace{\frac{\pi_\theta(y_t|x,y_{<t})}{\pi_{\text{old}}(y_t|x,y_{<t})}}_{r_t}$$ 这就是“序列级重要性比率 = token 比率连乘”的严格推导。 Sutton & Barto 在讲 off-policy Monte Carlo 时也明确把重要性采样比率写成沿轨迹的动作概率比连乘（$\prod \pi/\mu$），并讨论它会导致方差极端甚至无穷。
3) PPO/GRPO 的 loss 不是把“整段一个标量权重 $w_{\text{seq}}$”乘到整段梯度上；它是把每个 token 的梯度分别乘上各自的 $r_t$（以及各自的 clip/min 分支）再求和：$$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$
### 4.2 DAPO：不改“token-level ratio”的骨架，而是用一套工程化手段把它“撑稳、撑久”
它是在 token-level PPO/GRPO 框架不变的情况下，把“熵塌缩、有效梯度减少、长序列权重失真、截断奖励噪声”这些最容易把系统推向崩溃的因素逐个拆弹。
	1) Decoupled Clip / Clip-Higher：上界放宽，保住探索与熵 
	DAPO观察到：PPO常用的对称clip（如ε = 0.2）对“低概率探索token”极不友好——上界1+ε会把它们的概率提升死死卡住，从而让熵过快塌陷、样本趋同。于是它把clip下界ε low和上界ε high解耦，并提高上界给探索留空间。 
	本质：不是在“纠正off-policy”，而是在避免clip机制把探索通道掐死，从而让长链推理的策略不会早早变成确定性、走向崩。 
	2) Dynamic Sampling：把“零梯度/弱梯度batch”从数据管道里清掉 
	在verifiable reward（对/错）里，经常出现某些prompt的一组采样全对或全错→组内标准化优势变成0→整个prompt对梯度没贡献。DAPO选择“多采样+过滤”把这些样本从batch里剔除，保证每个batch里有效梯度数量稳定，降低噪声敏感性。 
	本质：你解决的不是“ratio方差”，而是有效样本数下降导致的梯度方差飙升。 
	3) Token-level Policy Gradient Loss：让长样本该有的权重回到梯度里 
	DAPO指出GRPO的sample-level loss（先对token平均再对样本平均）会让长序列里每个token的贡献被稀释：高质量长推理学不进去，低质量超长胡言乱语又惩罚不够，导致熵和长度不健康膨胀。于是它改成token-level方式聚合，让长序列在梯度里有更合理的影响，并能更强地压制样本里的坏模式。 
	本质：这是在修“长序列下的梯度分配/信用分配失真”，让训练动力学更健康。 
	4) Overlong Reward Shaping：减少截断带来的奖励噪声 
	生成被截断时，如果直接给惩罚，会把“本来正确但太长”的推理也当作错误，产生强噪声扰动；DAPO用过滤/软惩罚等方式降低这种噪声，显著稳定训练。

### 4.3 GSPO：直接把“off-policy 校正单位”改成序列级，让优化单位对齐奖励单位
它认为 GRPO 的崩溃来自更根本的“算法目标 ill-posed”：token-level importance weight 在每个位置只用单样本，无法完成分布校正，噪声随长度累积并被 clip 放大，导致崩溃。
因此 GSPO 做了一个原则性改变：**重要性比率用序列似然定义，clip/优化也在序列级进行**。

GSPO定义序列级权重 $s_i(\theta)$ 为序列似然比的几何平均（长度归一化）： $$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\text{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left( \frac{1}{|y_i|} \sum_t \log \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\text{old}}(\cdot)} \right)$$并在序列级做PPO-clip: $\min(s_i A_i, \text{clip}(s_i) A_i)$。 可以把“整段序列”看成一个“大动作”，那么整条序列的优势 $A_{i}$ ：对同一 query x 采样 GGG 条回复，先算每条的 reward，再做组内归一化，把“归一化后的 reward”当作优势； $s_i(\theta)$  是整条序列一个数（由这条序列所有 token 的 log-ratio 平均决定）。**这里的 $i$ 就是一条序列 / 一条轨迹。**
它还强调：这样做符合“奖励给的是整段序列，所以off-policy指标/裁剪也应匹配序列级”。 一句话总结GSPO的本质：把“校正与裁剪的单位”从token提升到sequence，用长度归一化的序列ratio作为真正有意义的off-policy距离度量，从源头上切断token-level单样本权重导致的高方差累积。
>
### 4.4 SAPO：在“序列一致性”和“token 适应性”之间，做连续可调的软信赖域
SAPO 的最重要特点：它承认 GSPO 把单位提升到序列级后更稳定，但 **GSPO 的 hard clip 太“脆”**：序列里只要有少数极 off-policy 的 token 把序列指标推到剪切带外，**整条序列的梯度就会被压没**；这牺牲了大量“其实仍近似 on-policy 的 token”的学习信号。SAPO 用一个**温度控制的 sigmoid 软门控**替代 hard clip：偏离越大，权重越平滑地衰减，而不是直接清零。

>on-policy 的 token：如果 $r_t(\theta) \approx 1$，表示“新策略在这个前缀上对这个 token 的概率几乎没变”，这就叫这个 token 接近 on-policy（near on-policy）——它和生成数据时的分布很一致。 在这种情况下，这个 token 对更新的贡献（你可以叫它“学习信号”）通常是更可靠/噪声更小的，因为你没有用很离谱的 off-policy 权重去放大或扭曲。

>GSPO 的 hard clip 太“脆”，但是PPO/GRPO的梯度没那么脆：因为GSPO 用的是**序列级** ratio。

>整条序列的梯度就会被压没：在band也就是置信区域外，梯度 = 0。

SAPO 的本质：把 GSPO/GRPO 的“硬裁剪信赖域”变成“可微、连续、可调的软信赖域”，在大多数时候保持序列级对齐，在遇到离群 token 时只局部降权，从而同时提升稳定性与样本效率。

1) SAPO 的目标函数 $$J(\theta) = \mathbb{E}_{q\sim\mathcal{D}, \{y_i\}_{i=1}^G\sim\pi_{\theta_{\text{old}}}(\cdot|q)}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} f_{i,t}(r_{i,t}(\theta)) \bar{A}_{i,t} \right]$$ 这里: 
	- $q$: query/prompt; $\mathcal{D}$: query 集合 
	- 一次对同一个 $q$ 采样 $G$ 个回复 $y_1,\dots,y_G$ (来自行为策略 $\pi_{\theta_{\text{old}}}$) 
	- $|y_i|$: 第 $i$ 个回复的 token 数 
	- $\bar{A}_{i,t}$: 组内归一化 advantage (对同一条回复内所有 token 共享) 
	- $r_{i,t}(\theta)$: token 级重要性比率 (见下) 

2) token 比率 $r_{i,t}$ 与组内优势 $\bar{A}$  $$r_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t} | q, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t} | q, y_{i,<t})}, \quad \bar{A}_{i,t} = \bar{A}_i = \frac{R_i - \text{mean}(\{R_j\}_{j=1}^G)}{\text{std}(\{R_j\}_{j=1}^G)}$$
3) SAPO 的“软门控” $f_{i,t}$ $$f_{i,t}(x) = \sigma(\tau_{i,t}(x - 1)) \cdot \frac{4}{\tau_{i,t}}, \quad \tau_{i,t} = \begin{cases} \tau_{\text{pos}}, & \bar{A}_{i,t} > 0 \\ \tau_{\text{neg}}, & \text{otherwise} \end{cases}$$ 其中 $\sigma(z) = \frac{1}{1+e^{-z}}$ 是 sigmoid。 直觉: 
	- hard clip 是“过界就截断/置零” 
	- SAPO 是“离 1 越远就越平滑地衰减”(而且正/负 advantage 用不同温度，负向更容易不稳定就让它衰减更快)
4) 看 SAPO 真正更新什么：它的梯度形式 
		$$\nabla_\theta J(\theta) = \mathbb{E}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} w_{i,t}(\theta) r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(y_{i,t} | q, y_{i,<t}) \bar{A}_{i,t} \right]$$
       其中软门控产生的梯度权重是: $$w_{i,t}(\theta) = 4p_{i,t}(\theta)(1-p_{i,t}(\theta)), \quad p_{i,t}(\theta) = \sigma(\tau_{i,t}(r_{i,t}(\theta)-1))$$并且它有个非常好记的等价形式 (利用 $4\sigma(z)(1-\sigma(z)) = \text{sech}^2(z/2)$): $$w_{i,t}(\theta) = \text{sech}^2\left( \frac{\tau_{i,t}}{2}(r_{i,t}(\theta)-1) \right)$$这就是 SAPO 的关键: - PPO/GRPO 的“有效系数”大致是 (硬裁剪后的) $r_{i,t}$ 或者直接 0 - SAPO 的“有效系数”变成 $w_{i,t}(\theta)r_{i,t}(\theta)$: 不再 hard clip，而是用 $w$ 做连续衰减 - 在 $r_{i,t}=1$ 时 $w_{i,t}=1$，所以在 on-policy 点附近，梯度保持和未裁剪一致 (论文也强调这一点) 
5) SAPO 为什么说自己“sequence-coherent”? 
      论文还给了一个结论: 在“小步更新 + 序列内 token log-ratio 离散度低”等条件下，token gate 的平均会集中成一个序列级 gate: $$g(\log s_i(\theta)) = \text{sech}^2\left( \frac{\tau_i}{2} \log s_i(\theta) \right)$$这就是它宣称“像 GSPO 一样对齐序列奖励，但又能在有离群 token 时只压离群 token”的数学抓手。 
      如果你想把 SAPO 和 GSPO/GRPO 的公式放到同一个统一视角里，其实可以只看一件事:“梯度里乘在 $\nabla \log \pi$ 前面的那个系数” 
      - GRPO: hard clip 让很多 token 直接系数变 0 
      - GSPO: 整条序列一个 hard clip (序列被少数 token 推出边界就全 0) 
      - SAPO: 用 $w_{i,t}$ 把“是否更新”变成连续值 (更连续信赖域) 你要不要我用同一个“effective weight”把三者写成一行对照?