## 1.Double DQN
Double DQN（Double Deep Q-Network）是对 DQN 的一个小改动，但能显著缓解 **Q 值“系统性高估（overestimation bias）”**，训练更稳、策略更靠谱。

DQN 的 TD 目标（target）通常是： $$y = r + \gamma \max_{a'} Q_{\text{target}}(s', a')$$ 如果 $Q(\cdot)$ 的估计里带噪声（神经网络几乎必然有），那么“max”会倾向挑到“被高估”的那个动作 —— 即便真实最优动作不是它。挑到哪个偏高的动作后，DQN是直接用这个动作的偏高Q值来更新，Double DQN也是用这个动作，但是不用这个Q值，用的是另一个网络估计的Q值，缓解“二次伤害”的问题。
- 真实训练里Q不是直接监督的，而是靠bootstrap：前一状态的值靠下一状态的max Q回传； 
- 下一状态动作多、估计有噪声时，max很容易挑中被高估动作；
- 这个偏高target会一路传回更早状态，造成策略偏向次优分支； 
- Double DQN用“两张相关但不完全相同的估计器”把“选”和“评”拆开，显著减少这种系统性高估。

## 2.MAPPO（Multi-Agent PPO）
1) 本质：PPO + 中心化 critic（值函数） MAPPO 本质上就是：actor 还是每个 agent 各自（或参数共享）的 PPO，但 critic 用全局信息（全局状态或所有 agent 的观测/动作）来估值，从而缓解多智能体的非平稳性与信用分配问题。它是在合作任务里把 PPO 做成一个很强的 baseline。 - 执行时：每个 agent 用自己的局部观测 $o_t^i$ 输出动作 $a_t^i$（分散）。 - 训练时：用中心化值函数 $V_\phi(\cdot)$（或 $Q_\phi(\cdot)$）来算 advantage，通常输入全局状态 $s_t$ 或拼接的 $(o_t^1,\dots,o_t^n)$（集中）。 
2) 关键目标：PPO 的 clipped surrogate（对每个 agent） 对第 $i$ 个 agent，PPO 的 ratio： $$r_t^i(\theta) = \frac{\pi_\theta^i(a_t^i | o_t^i)}{\pi_{\text{old}}^i(a_t^i | o_t^i)}$$ actor 目标（同 PPO）： $$L_{\text{clip}}^i(\theta) = \mathbb{E}_t\left[ \min\left(r_t^i \hat{A}_t^i, \text{clip}(r_t^i, 1-\epsilon, 1+\epsilon)\hat{A}_t^i\right) \right]$$差别在 $\hat{A}_t^i$：MAPPO 常用**中心化 critic** 给出的 $V_\phi(\text{global})$ 计算 GAE/advantage。 你可以把 MAPPO 看成“IPPO（每个 agent 独立 PPO）+ centralized value function”。 
3) 训练流程（非常像 PPO） 1. 用当前策略 rollout 一批 episode（on-policy）。 2. 用中心化 critic 估 $V$，算 GAE 得到 $\hat{A}_t^i$。 3. 多个 epoch、mini-batch 做 PPO 更新（actor）+ MSE 更新（critic）。 4. 丢弃旧数据，再 rollout 下一批。 
4) PPO/MAPPO**会对同一批 rollout 数据做多 epoch / 多 mini-batch 更新**，是on-policy。

## 3.MADDPG (Multi-Agent Deep Deterministic Policy Gradient)
1) 本质：DDPG + “每个 agent 一个中心化 critic” 
       MADDPG 的核心是：每个 agent 有自己的 actor（分散执行），但 critic 在训练时**条件在所有 agent 的信息上**（至少包括所有动作，通常也包括所有观测/状态），从而把“其他 agent 策略在变导致环境非平稳”这件事吸进 critic 的输入里，让 Q 学习更稳定。 
       论文标题为 *Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments*，专门面向合作/竞争混合场景。 
2) 关键公式（以第 $i$ 个 agent 为例） 
	- **中心化 critic**： $$Q_\phi^i(x, a^1, \dots, a^n)$$ 这里 $x$ 常表示全局状态或拼接观测（实现可选）。 
	- **critic 的 TD 损失**： $$\mathcal{L}(\phi) = \mathbb{E}\left[(Q_\phi^i(x, a) - y)^2\right], \quad y = r^i + \gamma Q_{\phi'}^i(x', a'^1, \dots, a'^n)$$ 其中 $a'^j = \mu_{\theta'}^j(o^j)$ 来自 target actor。
	- **actor 梯度（确定性策略梯度）**： $$\nabla_{\theta^i} J \approx \mathbb{E}\left[\nabla_{a^i} Q_\phi^i(x, a) \nabla_{\theta^i} \mu_\theta^i(o^i)\right]$$ 这是 DDPG 的套路，只是 Q 现在需观测所有 agent 的动作。 
	- **经验回放 replay buffer**： MADDPG 是 off-policy 算法，采用 replay + target networks 机制，样本效率通常比 MAPPO 更好。 
3) 训练流程（典型） 
	1. 用当前 actors 与探索噪声收集 transition，存入 replay buffer。 
	2. 采样 batch： 
		- 用 target actors 生成下一时刻动作 $a'$；
		- 用 target critics 计算 TD target $y$； 
		- 更新每个 agent 的 critic。 
	3. 用当前 critic 对 actor 做确定性策略梯度更新。 
	4. 软更新 target networks。 
4) 对比（本质） 
- **MAPPO**：on-policy、PPO 框架稳定，中心化 V 函数精准计算 advantage → 合作任务表现稳健。 
- **MADDPG**：off-policy、replay 机制提升样本效率，中心化 Q 函数吸收“其他 agent 策略变动”的非平稳性 → 连续控制/混合博弈更常用。