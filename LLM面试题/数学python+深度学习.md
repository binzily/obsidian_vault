### 1. ==说一下你对kl散度的理解？kl散度和交叉熵的区别和联系是什么？== 

KL散度表达式：

$D_{KL}(P\parallel Q)=\sum_iP(i)\log\frac{P(i)}{Q(i)}$ 中，

第一个参数 P 是基准，第二个参数 Q 是我们的模型。，KL散度衡量的是：“**当我们身处 P 的世界里时，用 Q 的模型来解释这个世界，会让我们付出多大的代价。**”

KL 散度有一个非常重要的特性：**不对称性**。 `D_KL(P || Q) ≠ D_KL(Q || P)`

- `D_KL(P || Q)`：用“地图 Q”来描述“现实 P”所造成的信息损失。
- `D_KL(Q || P)`：用“现实 P”来描述“地图 Q”所造成的信息损失。

这两种损失通常是不同的。这就好比用“纽约地图”在“东京”导航，和用“东京地图”在“纽约”导航，犯的错误是不一样的。因此，我们称之为“散度”，而不是“距离”。`P(i)` 较大的项（真实世界中更重要的事件）所产生的**正向惩罚**，会**系统性地超过**那些 `P(i)` 较小的项所带来的负向奖励。



**交叉熵  = 熵  + KL 散度**：$H(P,Q)=H(P)+D_{KL}(P||Q)$

​					   $H(P,Q)=-\sum_iP(i)\log(Q(i))$

我们来拆解这个公式：

- **熵 `H(P)`**: 代表**真实分布 P** 的不确定性，当你说“我不确定 X 是什么”，你其实是在说：你只能用一组概率 p(x) 来描述它，熵是分布的函数（functional）。在信息论中，它表示编码来自 P（随机变量） 的事件所需要的**最优平均比特数**。这是理论上的**最短编码长度**。对于一个给定的真实分布 P，它的熵 `H(P)` 是一个**常数**。
- **KL 散度 `D_KL(P || Q)`**: 正如我们上面所说，这是使用**非最优的分布 Q** 来编码 P 时，所产生的**额外比特数**或**信息损失**。
- **交叉熵 `H(P, Q)`**: 代表使用**基于 Q 的编码方式**来编码来自**真实分布 P** 的事件时，所需要的**实际平均比特数**。
- 因为最小化交叉熵就等于最小化KL散度，且**计算更简单**，所以被用作**损失函数**。

### 2. ==如何选取模型中的超参数，有什么办法？==

- **随机搜索**：为每个超参数定义一个**分布**，在预设的尝试次数内（比如100次），**每次都从这些分布中随机采样**一组超参数组合来进行训练和评估。

- **贝叶斯优化**：随机选择几组（比如3-5组）超参数 `x`，分别用这几组超参数完整地训练模型，得到对应的性能得分 `f(x)`。我们现在有了一个小小的初始数据集 `D = {(x₁, f(x₁)), (x₂, f(x₂)), ...}`。 使用当前所有的数据 `D`，拟合一个高斯过程模型：普通函数在每个输入点 `x` 只会输出一个确定的值 `y`。而高斯过程在每个输入点 `x` 输出的是一个**概率分布**——具体来说是一个**高斯（正态）分布**。这个分布由**均值和方差定义**。我们得到了一个覆盖整个超参数空间的“信念地图”（均值和方差）。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。更新，重复。

- 基于**早停**的方法：代表算法:是**Successive Halving** 和 **Hyperband**。随机生成大量的超参数组合。

  给所有组合分配一个**最小的资源**（比如只训练1个epoch）。淘汰掉表现最差的一半组合。给剩下的一半组合分配**更多的资源**（比如再训练2个epoch）。再淘汰掉一半...重复这个“分配资源-淘汰差生”的过程，直到只剩下一个“冠军”组合，并对其进行完整训练。

### 3. ==正则化==

**正则化防止过拟合**：让很多个特征都贡献一点点（拥有较小的权重），而不是让少数几个特征拥有巨大的权重，使得权重分散。

| 特性         | L2 正则化 (Ridge)        | L1 正则化 (Lasso)                        |
| ------------ | ------------------------ | ---------------------------------------- |
| 惩罚项公式   | λ * ∑ wᵢ² (平方和)       | `λ * ∑                                   |
| 对权重的影响 | 使权重变小，趋近于0      | 使权重变小，很多会精确等于0              |
| 模型稀疏性   | 否，权重通常是小的非零值 | 是，产生稀疏的权重矩阵                   |
| 核心思想     | 鼓励权重分散、平滑       | 特征选择，保留最重要的特征               |
| 主要用途     | 通用的、首选的正则化方法 | 当特征数量巨大且怀疑很多特征是无用的时候 |

L1正则化：**惩罚是“一视同仁”的**：惩罚的力度与权重大小成正比。一个权重从 0.1 变为 0，和从 10.1 变为 10，所获得的“奖励”（惩罚的减少量）是相同的。

### 4. ==在参数初始化时，为什么不能全零初始化==

![500](assets/数学python+深度学习/file-20251204084206136.png)
全0初始化会一直是0；全1初始化网络变动会一直一样。可以把两层的连线看成权重W矩阵。
### 5. ==死亡ReLU==

在训练过程中，如果某一次梯度更新的幅度特别大（比如学习率设置得太高），可能会导致权重 `W` 和偏置 `b` 被更新成一组“糟糕”的值。这组“糟糕”的值，使得对于**之后所有输入到这个神经元的数据 `x`**，计算出的进入激活函数前的 `z` **始终小于0**。反向传播的梯度计算是**一路连乘**的。ReLU函数在负数区间的**导数（斜率）恰好为0**。

### 6. ==1×1卷积核的作用==

首先：一个卷积核，无论它的高和宽是多少（`1x1`, `3x3`, `5x5`），它的唯一使命就是：作为一个特征探测器，扫描整个输入数据，并将所有输入通道的信息“浓缩”成一张代表“该特征出现位置”的单通道输出图。对于多通道的图像，卷积核最后会把所有通道都加起来。**卷积核是3D的**，**一个卷积核卷完后得到的特征图是1D的**。

那么1×1卷积核的作用：

- **降维或者升维**。

- **增加非线性**：一个 1x1 的卷积，后面再接一个 ReLU，相当于在不改变特征图尺寸的情况下，对输入的每一个像素点进行了一次非线性的变换。

- **跨通道信息交互**：将输入的所有 `C` 个通道的值进行一次**加权求和**。

### 7. ==深度可分离卷积==

| 特性   | 标准卷积             | 深度可分离卷积                                      |
| ---- | ---------------- | -------------------------------------------- |
| 工作方式 | 一步到位，同时处理空间和通道   | 分两步走，先处理空间，再处理通道                             |
| 步骤   | 单一的标准卷积          | 1. 深度卷积 (Depthwise) 2. 逐点卷积 (Pointwise, 1x1) |
| 核心思想 | 耦合操作             | 解耦操作                                         |
| 主要优势 | 表达能力强            | 参数少，计算快，极其高效                                 |
| 代表模型 | VGG, ResNet (早期) | MobileNet, Xception, EfficientNet            |

### 8.==GAN是怎么训练的==

生成对抗网络(GAN)的训练过程包括两个部分:生成器的训练和判别器的训练。这两个部分交替进行,形成一
种“对抗”的训练过程。

**判别器**训练:首先,我们固定生成器的参数,训练判别器。具体来说,我们首先从真实数据集中采样一批真实样
本,然后从生成器中生成一批假样本。我们希望判别器能够正确地区分这两批样本,因此我们通过最大化判别器对
真实样本的输出和最小化判别器对假样本的输出来更新判别器的参数。

**生成器**训练:然后,我们固定判别器的参数,训练生成器。我们希望生成器能够生成能够“欺骗”判别器的假样
本,因此我们通过最大化判别器对生成器生成的假样本的输出来更新生成器的参数。

循环训练。

### 9.==word2vec提出了负采样策略，他的原理是什么，解决了什么样的问题？==

Word2Vec不是一个单一的算法，而是 Google 在 2013 年提出的一个**模型工具集**，其主要目的是将词语转换成向量。这种由模型产生的词语向量，通常被称为**词嵌入 (Word Embedding)**。

```
Word2Vec 怎么训练：

1. 输入： 也就是一对词（中心词，上下文词）。
    
2. 动作：
    
    - 去矩阵里查出这两个词的向量（一开始是乱数）。
        
    - 算这两个向量的内积（相似度）。
        
3. 目的：
    
    - 如果是真邻居，就拉近这两个向量（修改矩阵数值）。
        
    - 如果是假邻居（负采样），就推远这两个向量。
        
4. 产出： 训练结束后，那个被改了无数遍的矩阵（表格），就是我们梦寐以求的 Word2Vec 词向量。
```

Word2Vec 工具集中包含两种主要的训练模型：

- **CBOW (Continuous Bag-of-Words)**：**根据上下文词预测中心词**。比如，给出 `["the", "cat", "on", "the", "mat"]`，模型需要预测中间的词是 `sat`。
- **Skip-gram**: **根据中心词预测上下文词**。比如，给出中心词 `sat`，模型需要预测出周围的词可能是 `the`, `cat`, `on`, `the`, `mat`。Skip-gram 在处理罕见词时效果通常更好。

|**特性**|**Skip-Gram (中心 → 周围)**|**CBOW (周围 → 中心)**|
|---|---|---|
|**方向**|**一对多** (One-to-Many)|**多对一** (Many-to-One)|
|**逻辑**|给定当前词，猜它可能出现在什么环境里。|给定环境，猜中间缺了什么词。|
|**处理方式**|每个上下文词都独立产生误差，**独立更新**中心词。|上下文词先**求平均**，再一起去预测中心词。|
|**生僻词效果**|**更好**。因为每个样本都独立起作用，生僻词能被充分训练。|**较差**。因为上下文被平均了，生僻词的独特信息容易被淹没。|
|**训练速度**|**慢**。计算量大 (样本多)。|**快**。计算量小 (合并计算)。|
|**适用场景**|**大部分场景的首选**，特别是语料不够多时，或者对生僻词精度要求高时。|语料库超级巨大，且追求极速训练时。|

### 10.==如果让你设计一个命名实体识别任务，你会怎么设计？==

