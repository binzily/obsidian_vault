### 1.==GPT和BERT的区别==


### 2.==为什么现在大多都是decoder-only的架构==

Encoder的双向注意力会存在低秩问题，就生成任务而言，引入Encoder并无实质好处，Encoder-Decoder在某些场景下表现更好大概因为它多了一倍的参数，且Encoder-Decoder容易过拟合。
### 3.==ChatGPT的训练步骤==

#### RLHF 终极总览：从 SFT 到 PPO

##### 第一幕：准备阶段 - “剧组”的诞生

此阶段为 PPO 训练准备好所有必要的模型。

###### 阶段一：监督微调 (SFT - Supervised Fine-Tuning)

1. **起点**: 一个通用的**预训练模型 (Base LLM)**。
2. **目标**: 教会模型遵循指令，进行高质量的对话。
3. **方法**: 使用高质量的“指令-回答”数据集进行微调。
4. **产物**:
   - **演员 (Actor $π_θ$)**: SFT 模型的**副本**，将在 PPO 阶段持续学习和进化。
   - **参考模型 (Reference Model $π_SFT$)**: SFT 模型的**原始、冻结版本**，用于计算 KL 散度惩罚，防止 Actor “忘本”。
   - **评论家 (Critic $V_φ$)**: SFT 模型的**副本**，但其输出层被替换为一个**随机初始化的价值头 (Value Head)**，用于在 PPO 阶段学习评估状态的价值。

###### 阶段二：奖励模型 (Reward Modeling - RM)

1. **目标**: 训练一个能模仿人类偏好的“AI 裁判”。
2. **方法**:
   - 让 SFT 模型对一个指令生成多个回答。
   - 人类对这些回答进行排序。
   - 使用这些排序数据（人类偏好）训练一个新的模型。
3. **产物**:
   - **奖励模型 (Reward Model)**: 一个冻结的模型，其任务是为“指令+回答”的组合输出一个**标量分数**，代表人类喜好程度。

**至此，PPO 训练的四个关键模型集结完毕：Actor (`θ`), Critic (`φ`), Reference Model, Reward Model。**

##### 第二幕：PPO 训练循环

###### 第 1 步：数据收集 (Rollout)

1. 给定一个 `prompt`，**Actor ($π_θ$)** 开始生成回答。
2. 在生成的**每一步 `t`**:
   - Actor 采样一个动作 `a_t`。
   - Critic ($V_φ$) 预测当前状态的价值 $V_φ(s_t)$。
1. 生成结束后：
   - **Reward Model** 对完整的回答打一个最终的奖励分数 $R_final$。
   - **Reference Model (`π_SFT`)** 用于计算每一步的 KL 散度惩罚 $r_{KL}$。
1. 将轨迹数据 $(s_t, a_t, V_φ(s_t), r_KL, R_final)$ 存入缓冲区。

###### 第 2 步：优势计算 (Advantage Calculation)

1. **计算总奖励 $r_t$**:
   - 中间步骤的奖励主要是 KL 惩罚: $r_t = r_KL$。
   - 最后一步的奖励是 KL 惩罚与最终分数之和: $r_T = r_KL + R_final$。
1. **计算优势函数 $Â_t$ :
   - **给 Actor 看的**: 使用 GAE (Generalized Advantage Estimation) 等方法计算。它衡量了在状态 $s_t$ 下采取动作 $a_t$ 相对于 Critic 预期的“惊喜程度”。$Â_t > 0$ 是“神来之笔”，$Â_t < 0$ 是“败笔”。
1. **计算目标价值 $V_t^{target}$ :
   - **给 Critic 看的**: $V_t^{target} = Â_t + V_φ(s_t)$。作为 Critic 学习的标准答案。

###### 第 3 步：模型优化 (Opimization)

1. **更新 Critic (`φ`)**:
   - **目标**: 让 Critic 的价值判断更准。
   - **损失函数**: 最小化均方误差 $Loss_{Critic} = (V_φ(s_t) - V_t^{target})^2$。
1. **更新 Actor (`θ`)**:
   - **目标**: 增加“神来之笔”的概率，降低“败笔”的概率，同时保证学习的稳定性。
   - **方法**: 使用 PPO 的核心——剪裁替代目标函数。

##### 第三幕：PPO 核心 - Actor 更新详解

Actor 在更新时看不见奖励模型，它唯一的指导信号是计算出的**优势 `Â_t`**。

###### 1. 核心组件：概率比率 r_t(θ)

$r_{t}(θ)=π_{θ}{old}(a_{t}∣s_{t})π_{}θ(a_{t}∣s_{t})$

- 该比率衡量了**新策略**相对于旧策略在选择同一动作上的激进程度。
- $r_t$ > 1 表示新策略更倾向于该动作。
- $r_t < 1$ 表示新策略更不倾向于该动作。

###### 2. PPO 剪裁目标函数 L_CLIP(θ)

$L_{CLIP}(θ)=E_{t}[min(r_{t}(θ)A^t,clip(r_{t}(θ),1−ϵ,1+ϵ)A^t)]$

- ϵ 是一个超参数（如 0.2），定义了 `[0.8, 1.2]` 的信任区间。
- `clip` 函数将 $r_t(θ)$ 强制限制在 `[1-ε, 1+ε]` 区间内。
- `min` 函数确保我们总是选择一个更保守（更悲观）的目标进行优化，这就是 PPO 的精髓。

**函数行为分析**:

- **当优势 $\hat{A}_{t_{}}$ > 0(好动作)**:
  - 我们希望增大 $r_t(θ)$ 来增加奖励。$\hat{A}_t < 0$
  - 但 `clip` 函数为奖励的增长设置了一个**上限**。一旦 r_t(θ) 超过 1+ϵ，目标函数的值就不再增长，梯度变为 0。
  - **效果**: 防止因一个好动作而产生“得意忘形”的过度更新。
- **当优势 $\hat{A}_t < 0$ (坏动作)**:
  - 我们希望减小 $r_t(θ)$ 来减小惩罚。
  - `min` 函数会选择两个负值中更小的那个（绝对值更大）。
  - 如果 $r_t(θ)$ 变化过大，`clip` 项会施加一个更大的惩罚，迫使更新幅度保持在信任区域内。
  - **效果**: 保证了在惩罚信号下的更新稳定性，不会“一蹶不振”。

###### 3. 最终 Actor 损失函数

为了进行梯度**下降**，并鼓励探索，最终的损失函数如下：

Total_Actor_Loss=$−(L_{CLIP}(θ)+c⋅S[π_θ]))$

- 负号: 因为优化器通常是做最小化，而我们的目标是最大化 $L_{CLIP}$。
- **$S[π_θ]$**: 策略的**熵 (Entropy)**。将其加入目标函数（相当于在 Loss 中减去它）可以鼓励模型进行更多探索，防止过早收敛到次优策略。`c` 是熵奖励的系数。

### 4.==LLM中的分词技术==

BPE、wordpiece、sentence-piece：

| 特性 (Feature) | BPE          | WordPiece    | SentencePiece          |     |
| ------------ | ------------ | ------------ | ---------------------- | --- |
| 核心算法         | 贪婪合并最频繁对     | 贪婪合并最大化LM似然度 | 算法库，支持BPE/UnigramLM    |     |
| 预分词依赖        | 依赖 (通常基于空格)  | 依赖 (通常基于空格)  | 不依赖 (直接从原始字符训练)        |     |
| OOV 处理       | 通过拆分子词       | 通过拆分子词       | 保证无 OOV (回退到字符)        |     |
| 主要应用         | GPT, RoBERTa | BERT, T5     | LLama, Alpaca, 许多多语言模型 |     |
| 子词表示         | 词尾 </w> (可选) | 词中 ## (或 _)  | 词首 _ (表示原词首或空格)        |     |
| 语言适用性        | 擅长英文         | 擅长英文         | 擅长所有语言 (尤其东亚语言)        |     |

### 5.==为什么要有激活函数==

如果神经网络中**没有激活函数**（或者只使用线性激活函数），无论你堆叠多少层，整个网络最终都等价于**一个单一的线性变换**。

### 6.==为什么基于Transformer的架构需要多头注意力机制？==

| 特性       | 大单头矩阵                                         | 多头注意力机制                                               |
| ---------- | -------------------------------------------------- | ------------------------------------------------------------ |
| 投影方式   | 只有一组大的权重矩阵，将输入投影到同一个高维空间。 | 有多组独立的、小的权重矩阵，将输入分别投影到多个不同的低维子空间。 |
| 注意力计算 | 计算一个统一的注意力分数矩阵。                     | 并行计算多个独立的、各自专注的注意力分数矩阵。               |
| 关注点     | 试图找到一个“面面俱到”但可能“平庸”的单一关注模式。 | 每个头可以专注于一种特定类型的关系（如语法、位置、语义等）。 |
| 结果       | 输出一个基于单一“妥协”视角的表示。                 | 输出一个融合了多个“专家”视角的、更丰富、更鲁棒的表示。       |

### 7. ==对LLM进行数据预处理有哪些常见的做法？==

### 8. ==Top-p抽样和Beam搜索==

**Beam Search**：找到一个整体累积概率最高的词语序列（句子）。它不只是看眼前哪一个词的概率最高，而是试图规划出一条“全局最优”的路径。对于同样的输入和同样的 `beam_width`，输出结果几乎总是**相同**的，且容易陷入**重复循环**（例如，"I think I think I think..."），缺乏惊喜和创造力。

**Top-k**：这是一种随机抽样方法。在生成文本的每一步：模型首先计算出词汇表中所有词作为下一个词的概率分布。然后，它会筛选出概率最高的 K 个词（比如 `K=50`）。它会抛弃所有其他的词，然后将这 K 个词的概率进行重新归一化（使它们的概率总和为1）。最后，它会从这 K 个词中随机抽取一个作为下一个词，概率越高的词被抽中的几率也越大。

**Top-p**：它不选择固定数量`K`个词，而是选择一个概率阈值 `P` (比如 `P=0.95`)，然后从高到低累加词的概率，直到总和超过`P`为止。

### 9.==位置编码==

#### 正余弦编码：
正余弦编码有一个神奇的性质：对于任何固定的偏移量 `k`，位置 `pos+k` 的编码，可以由位置 `pos` 的编码通过一个线性变换来表示，这有助于模型学习到这种固定的模式。

在 NLP 和表示学习（Representation Learning）中，我们更倾向于把词向量看作高维空间中的**点**，而不是箭头。
融合方式为什么是相加：线性变换可以将相加的信息重新解耦（$W(A+B) = WA + WB$，A和B在高维空间往往是接近正交的，**让W和A/B其中一个方向一致就能提取信息**，如果不正交，我们就不能相加。如果不正交还硬要相加，那就是“混合之后再也分不开了”，即不能选择性让其中一方结果为0。这也引申除了多头注意力的多头的原因，即用**多个不同的矩阵W去提取不同的特征（只有一个W那只能提取一个A或B，两个就可以把A和B都提取出来）**。

**位置嵌入和位置编码**的不同之处在于一个可学习一个不可学习。

| 特性   | 原始 Transformer (正弦位置编码)    | 使用 RoPE 的现代 Transformer                    |     |
| ---- | -------------------------- | ------------------------------------------ | --- |
| 融合位置 | 在所有编码层开始之前，在最底部的输入层。       | 在每个注意力层的内部。                                |     |
| 融合对象 | 初始的词嵌入向量 x。                | 线性投影后的 Q 和 K 向量，**不作用于V**。                 |     |
| 融合方式 | 向量相加 (x + PE)。             | 旋转操作 (apply_rope(q, pos))。                 |     |
| 信息流  | (词嵌入 + PE) -> Wq/Wk -> q/k | 词嵌入 -> Wq/Wk -> q/k -> apply_rope -> q'/k' |     |
| 哲学   | "先给输入打上位置标签，再让各层去处理。"      | "先将输入转为查询和键，再在比较时精准注入位置关系。"                |     |

#### RoPE：
数学推导（极简版）
假设我们用复数来表示二维向量（这比矩阵更直观）：

- 位置 $m$ 的向量 $q$：我们在它原本的角度上，加上位置角度 $m\theta$。即 $q \cdot e^{im\theta}$。
    
- 位置 $n$ 的向量 $k$：我们在它原本的角度上，加上位置角度 $n\theta$。即 $k \cdot e^{in\theta}$。
    
现在做点积（在复数里相当于共轭相乘）：

$$Score = q_{rotated} \cdot k_{rotated}^*$$

$$= (q \cdot e^{im\theta}) \cdot (k \cdot e^{in\theta})^*$$

$$= q \cdot k^* \cdot e^{i(m-n)\theta}$$
为什么RoPE的旋转没有把语义搞砸：
1.RoPE 通常只作用于 Q 和 K，而不作用于 V。（最重要的一点）
2.那 Q 和 K 被转晕了，它们还能算准匹配度吗？本来‘猫’应该找‘鱼’，现在‘猫’被转了30度，它还能找到‘鱼’吗？能，如果“鱼”就在“猫”旁边（距离近，旋转差小），高匹配；如果“鱼”离“猫”十万八千里（距离远，旋转差大），低匹配（即便语义很合）。
3.假设 embedding 有 128 维，RoPE 是把它们两两分组进行旋转的。**这 64 组旋转的“速度”（频率）是完全不同的！**，前面的转的慢后面的转的快。

### 10.==是否可以在训练过程中使用可变的序列长度==

模型的静态参数和序列长度无关。实现手段为：padding和mask。

### 11.==归一化方式==

一个Batch的样本有很多个特征：

**Layer Norm**：对**单个样本**的**所有特征**（一行）进行归一化。

**Batch Norm**：对**一批样本**的**单个特征**（一列）进行归一化。

为什么 Transformer 用 LN 而不是 BN？

1. **NLP任务的序列长度多变**：对于变长的句子，在时间步维度上做 BN 操作很困难且不自然。而 LN 对每个词（Token）独立进行归一化，完美适应变长序列。
2. **小批次问题**：在训练大型语言模型时，由于显存限制，我们有时不得不使用非常小的批次（甚至 `batch_size=1`）。BN 在这种情况下效果很差，而 LN 不受影响。
3. **任务性质**：BN 的成功很大程度上源于它为每个批次引入了轻微的噪声（因为每个批次的均值和方差都不同），这在 CV 任务中起到了一种“正则化”的效果。而在 NLP 任务中，我们可能更需要模型对每个样本的内部特征分布进行稳定，而不是依赖批次内的其他样本。