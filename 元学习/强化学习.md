## 1.RLHF + PPO训练LLM的全流程：

### 1.1训练前的准备

**SFT** 阶段：首先需要一个已经完成预训练的 LLM ，它的训练任务是“Next Token Prediction”，SFT就是在这个 LLM 的基础上用“指令-回答”数据集监督微调，最后得到的模型就是 SFT 模型。
**RM** 阶段：复用SFT模型的架构来构建Reward Model。和Critic一样，它也有一个特殊的“奖励头”，输出一个标量值。训练数据都是一个 `(prompt, SFT模型回答_1, SFT模型回答_2, 排序结果)` 的组合。

| 模型                        | 来源                         | 在PPO阶段的状态           | 训练阶段            |
| ------------------------- | -------------------------- | ------------------- | --------------- |
| Actor $π_θ$               | 复制自SFT模型                   | 可训练，在PPO中不断更新       | PPO             |
| Critic $V_φ$              | 架构和权重来自SFT模型，但有一个随机初始化的价值头 | 可训练，在PPO中与Actor同步更新 | PPO             |
| Reference Model $π_{SFT}$ | 就是SFT模型本身                  | 冻结，参数固定不变           | SFT             |
| Reward Model (RM)         | 架构来自SFT模型，在人类排序数据上单独训练     | 冻结，参数固定不变           | Reward Modeling |
### 1.2训练主循环

###### 步骤一：数据收集 (Rollout / 与环境交互)
一句话：我们让当前的Actor模型 $π_θ$ 生成一大批对话数据。具体流程：
1. 从一个数据集中随机抽取一批 `prompt`。
2. 对于每一个 `prompt`，让Actor $π_θ$ 自回归地（一个词一个词地）生成一个完整的回答。
3. 在生成每一个词 $a_t$ 的过程中，我们需要记录下一整条“轨迹 (trajectory)”所需的所有信息：
    - **状态 $s_t$**: 生成 $a_t$ 之前的文本序列。
    - **动作 $a_t$**: 实际生成的那个词 (token)。
    - **动作概率 $log(π_θ(a_t|s_t))$**: Actor模型在状态 $s_t$ 下，选择动作 $a_t$ 的对数概率。
    - **价值估计 $V_φ(s_t)$**: Critic模型对当前状态 $s_t$ 的打分。
    - **KL散度奖励 $r_{KL}$**: 这是我们之前讲的“过程分”，用于稳定训练。
4. 当一个回答生成完毕（遇到结束符 $<eos>$），我们计算它的最终奖励：
    - 将完整的 `prompt + 回答` 输入给 RM，得到一个最终分数 $R_{final}$。
    - 将 KL散度奖励 和 最终奖励 结合。通常是这样：
        - 对于中间的每一步 `t`，奖励 $r_t = -β * KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$。
        - 对于最后一步 `T`，奖励 $r_T = R_{final} + r_{KL}$。
5. 重复以上过程，直到收集到足够多的轨迹数据（例如，几千条完整的对话）。现在，我们手上有一个巨大的经验池，里面装满了 $(s_t, a_t, log(π_θ(a_t|s_t)), V_φ(s_t), r_t)$ 这些数据点。
###### 步骤二：优势计算

==优势的定义是 `A(s,a) = Q(s,a) - V(s)`。问题在于，真实的`Q`值我们永远不知道。我们只能用不同的方法去**估计**它。==
两种极端的估计方法：
1. TD估计 (只看一步): $Â_{TD}(t) = r_t + γV(s_{t+1}) - V(s_t) = δ_t$
    - 优点: 方差低。因为它主要依赖于相对稳定的函数`V`，而不是充满随机性的真实奖励。
    - 缺点: 偏差高。因为$V(s_{t+1})$身就是一个不准确的估计，这个偏差会引入到$Â$的计算中。
2. 蒙特卡洛估计 (看到底): $Â_{MC}(t) = (r_t + γr_{t+1} + γ^2r_{t+2} + ...) - V(s_t)$
    - 优点: 无偏。因为它完全基于真实发生过的奖励序列，没有任何估计成分。
    - 缺点: 方差高。因为真实的奖励序列每次都可能因随机性而剧烈波动。

###### 步骤三：模型优化
1. Actor Loss：$L_{CLIP}(θ) = E[ min( r_t(θ) * Â_t, clip(r_t(θ), 1-ε, 1+ε) * Â_t ) ]$
2. Critic Loss：$L_{VF}(φ) = E[ (V_φ(s_t) - V_t^{target})^2]$
3. Entropy of the policy Bonus：$S = E[ -Σ_a π_θ(a|s_t) * log(π_θ(a|s_t)) ]$，最大化熵（鼓励探索）。
