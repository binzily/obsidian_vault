**PEFT**参数高效微调：相对于全量微调，只微调一小部分参数。

**Causal LM**：因果语言模型，根据前文的上下文信息来预测下一个单词或字符的语言模型。它假设当前的单词或字符与之前的历史信息存在因果关系，通过学习大量文本数据中的这种因果模式，来生成符合语法和语义规则的文本。

**显存占用**：

- **激活值（Activations）**：在模型前向传播过程中，每层神经元的输出值即为激活值。随着模型层数增加和输入数据量增大，激活值占用的显存会显著增多。尤其是对于一些结构复杂的大模型，如多层 Transformer 结构，激活值可能会占用大量显存，是显存占用的重要部分。
- **梯度（Gradients）**：梯度数据与模型参数形状相同。
- **优化器状态（Optimizer States）**：如 Adam 优化器需要保存一阶矩和二阶矩估计值，估计值的形状和模型参数形状相同。
- **中间计算结果**：模型训练过程中，会产生一些中间计算结果。例如，在计算注意力机制时，会有键 - 值对的计算结果、多头注意力融合后的结果等；在进行张量运算时，也会产生一些临时的中间张量，这些都需要占用显存。
- **数据加载缓存**：为了提高训练效率，数据加载器通常会缓存一部分已加载的数据，以便快速提供给模型进行训练。当训练数据样本较大或批量较大时，数据加载缓存也会占用一定显存。
- **正则化相关数据**：若模型使用了正则化方法，如 L1、L2 正则化，在计算正则化项时可能会产生一些中间数据，另外，像 Dropout 等正则化技术，在训练过程中也需要记录一些随机丢弃神经元的信息，这也会带来一定的显存占用。

# 1.LoRA

[动手实现 LoRA - LoRA from scratch - 知乎](https://zhuanlan.zhihu.com/p/702419731)

​	LoRA 其实就是用两个更低秩的矩阵 A 和 B来近似原参数矩阵 W 的效果，将**原矩阵冻结**以后，前向结果是原矩阵与新的近似矩阵结果的和，但反向的时候只更新 A 和 B 两个低秩矩阵，从而大大降低了需要训练的参数，因此属于参数高效微调（PEFT，Parameter Efficient Fine-tuning）的一种。

所以，我们如果要对一个模型应用 LoRA，其实就是把 **W 替换为 W+BA**，非常简单！有哪些线性层可以替换呢？

​	一般的q，k，v和FFN等线性层都是可以替换的。除此以外，其实作为因果模型，最后一般都有一个形状为 `(vocab_size, hidden_size)` 的因果头，一般叫 `lm_head`，它的作用就是把最后输出的 hidden_state 映射为词表范围，用于输出。它也是线性层，但我们一般不会去对它做 LoRA 替换。huggingface 的 Peft 库中的实现也是默认不考虑它的。