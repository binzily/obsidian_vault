# 1.OpenFlamingo

**openflamingo = open_clip + LM**

### 1.1模型架构

![image-20250226153851715](C:\Users\26344\AppData\Roaming\Typora\typora-user-images\image-20250226153851715.png)

模型由**不参与训练**的视觉模型和语言模型 + 一些待训练的层组成。

如果抛开图片的部分不谈，模型就是一个简单的语言模型，加上图片部分，相当于给模型加以提示，得到句子输出。

# 2.CLIP

​	CLIP（对比语言 - 图像预训练）是一个在各种（图像、文本）对数据上训练的神经网络。它可以通过自然语言指令，在给定图像的情况下预测最相关的文本片段，而无需直接针对该任务进行优化，这与 GPT - 2 和 GPT - 3 的零样本能力类似。具体应用有：**openai CLIP ViT-L/14**等，CLIP 是一个较为宽泛的模型架构，在图像编码器方面可以有多种选择，如 ResNet 系列或 Vision Transformer（ViT）系列等；文本编码器则通常使用 Transformer。

### 2.1 模型架构

![image-20250227091555328](C:\Users\26344\AppData\Roaming\Typora\typora-user-images\image-20250227091555328.png)

（1）对比预训练：

- 文本处理：假设拥有如描述小狗“Pepper the aussie pup”这类的文本，将其送入文本编码器（Text Encoder）。文本编码器如同“文字翻译官”，把文字信息转化为计算机能理解的数字形式（特征向量），生成T1、T2、T3…TN等一系列文本特征。 - 

- 图像处理：与此同时，有一组小狗图片，这些图片被送入图像编码器（Image Encoder）。图像编码器类似“图像翻译官”，将图片转化为数字形式（特征向量），得到I1、I2、I3…IN等一系列图像特征。  - 

- 对比学习：将每个图像特征与每个文本特征进行配对组合，并==计算它们之间的相似度==（以乘法符号示意）。理想状态下，描述同一张图片的文本和图像对应的特征向量相似度较高，不同的则较低。借助这种对比学习方式，使模型学习到图像和文本间的关联。

（2）从标签文本创建数据集分类器：

- 存在一系列类别标签，例如“plane”（飞机）、“car”（汽车）、“dog”（狗）、“bird”（鸟）等。把每个标签与固定句式“A photo of a {object}.”（一张{物体}的照片）组合，形成具体的文本描述。 

- 这些新的文本描述再经由文本编码器，转化为对应的文本特征T1、T2、T3…TN，这些特征集合构成一个数据集分类器，用于后续判断图像所属类别。 、

（3）零样本预测：

- 选取一张新图片，如一只黑狗在草地上的图片，将其输入图像编码器，得到图像特征I1。 - 把该图像特征I1与之前创建的数据集分类器中的所有文本特征（T1、T2、T3…TN）进行相似度计算。

- 若计算结果中，与描述“dog”（狗）的文本特征相似度最高，便预测这张图片是“一张狗的照片” ，实现了在未见过这类图片样本训练的情况下进行分类预测，即零样本预测。 

# 3.Flamingo

Flamingo 和 CLIP（Contrastive Language - Image Pretraining）有以下区别和联系：

**区别**:

- **模型架构**
  - **CLIP**：是一种基于对比学习的视觉 - 语言模型，它由一个图像编码器和一个文本编码器组成，通常使用 Transformer 架构，通过对比图像 - 文本对与其他负样本对的相似性来学习图像和文本的联合表示。
  - **Flamingo**：是在 Transformer 架构基础上，针对视觉语言任务进行改进的模型，它引入了一些新的机制，如多模态融合策略等，能更好地处理视觉和语言信息的交互。
- **训练数据**
  - **CLIP**：预训练数据来自大规模的互联网图像 - 文本对，数据来源广泛但相对较为通用，涵盖各种场景和主题。
  - **Flamingo**：训练数据通常也包含大量图像 - 文本对，但可能在数据筛选、标注或增强方面有其独特之处，以更好地适应特定任务或提高模型性能。
- **任务侧重**
  - **CLIP**：主要用于图像 - 文本的匹配、图像分类（通过文本描述进行零样本分类）等任务，侧重于学习图像和文本之间的语义对齐关系。
  - **Flamingo**：除了具备 CLIP 的一些基本能力外，更侧重于复杂的视觉语言理解和生成任务，例如根据图像生成自然语言描述、回答与图像相关的复杂问题等，在多模态交互的深度和灵活性方面表现更优。

**联系**:

- **都是多模态模型**：Flamingo 和 CLIP 都属于多模态模型范畴，旨在将视觉和语言两种模态的信息进行融合和理解，以实现更强大的人工智能任务。
- **相互借鉴与发展**：CLIP 的成功为多模态研究提供了重要的思路和方法，Flamingo 在一定程度上借鉴了 CLIP 的一些理念，如使用大规模预训练数据和对比学习等思想，并在此基础上进行创新和改进，以推动视觉语言模型的发展。
- **应用场景有交集**：在一些实际应用中，Flamingo 和 CLIP 都可以用于图像检索、智能客服（涉及图像和文本交互的场景）等领域，只是在具体任务的表现和适用场景上可能略有不同。

# 4. RoboFlamingo

==输入：视觉 + 语言 + （状态）；输出：动作指令 + （）==

![image-20250227172746466](C:\Users\26344\AppData\Roaming\Typora\typora-user-images\image-20250227172746466.png)

### 4.1 训练过程

#### 4.1.1 数据预处理

- 视觉数据处理
  - **归一化**：对图像数据进行归一化操作，将像素值缩放到一个特定的范围（如 [0, 1]），以帮助模型更好地收敛。例如，对于 RGB 图像，将每个通道的像素值除以 255。
  - **裁剪和缩放**：将图像裁剪或缩放到统一的尺寸，以便输入到模型中。这可以通过使用图像处理库（如 OpenCV 或 PIL）中的函数来实现。
- 语言数据处理
  - **分词**：使用分词器（如 Hugging Face 的`tokenizer`）将自然语言指令进行分词，将其转换为模型可以处理的词块（tokens）。
  - **编码**：将分词后的词块转换为对应的整数编码，这些编码可以作为模型的输入。
- 机器人状态和动作数据处理
  - **归一化**：对于机器人的关节角度、末端执行器位置等状态数据以及动作数据，进行归一化处理，使其数值范围适合模型训练。

#### 4.1.2 模型架构准备

RoboFlamingo 通常基于 Flamingo 模型进行扩展，融合了视觉编码器、语言编码器和用于机器人控制的相关模块。

- **视觉编码器**：用于处理视觉数据，提取图像中的特征。常见的视觉编码器如 ResNet、ViT 等。
- **语言编码器**：处理自然语言指令，提取文本特征。例如，使用 Transformer 架构的语言模型（如 GPT、BERT 等）。
- **融合模块**：将视觉特征和语言特征进行融合，以便模型能够综合利用这两种信息。这可能通过注意力机制等方式实现。
- **动作预测模块**：根据融合后的特征，预测机器人应该执行的动作。

#### 4.1.3 训练目标和损失函数定义

- **训练目标**：==训练 RoboFlamingo 的目标是让模型能够根据输入的视觉信息和语言指令，准确地预测机器人应该执行的动作序列==。
- **损失函数**：通常使用交叉熵损失（Cross Entropy Loss）来衡量模型预测的动作序列与真实动作序列之间的差异。对于回归任务（如预测机器人关节角度），可能会使用均方误差损失（Mean Squared Error Loss）。

#### 4.1.4 训练过程

- **数据加载**：将预处理好的数据按照一定的批次大小（batch size）加载到模型中。例如，使用 PyTorch 的`DataLoader`来实现数据的批量加载。
- **前向传播**：将视觉数据、语言数据输入到模型中，通过视觉编码器、语言编码器提取特征，然后经过融合模块和动作预测模块，得到模型的预测结果。
- **损失计算**：根据模型的预测结果和真实的动作数据，计算损失函数的值。
- **反向传播和参数更新**：使用优化器（如 Adam、SGD 等）根据损失函数的值进行反向传播，计算梯度，并更新模型的参数，以减小损失。