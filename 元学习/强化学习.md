## 0.Policy Gradient
### 0.1 先把LLM 放进 RL 记号里：token = action，前缀 = state
对一个固定 prompt $x$： 
- 状态：$s_t = (x, y_{<t})$（prompt + 已生成前缀） 
- 动作：$a_t = y_t$（第 $t$ 个 token） 
- 轨迹：$\tau = (s_1, a_1, \dots, s_T, a_T)$ 等价于整段 $y$ 
语言模型策略就是 $\pi_\theta(a_t | s_t) = \pi_\theta(y_t | x, y_{<t})$

### 0.2 原始公式推导
令策略（actor）是参数分布 $\pi_\theta(a | s)$。优化目标写成“轨迹回报的期望”： $$J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)]$$其中 $\tau = (s_0,a_0,\dots,s_T,a_T,s_{T+1})$ 是一条轨迹（在 LLM 里就是：给定 prompt 后采样出一整段序列；$a_t$ 对应 token，$s_t$ 对应“prompt+已生成前缀”）。 
轨迹概率可以展开为（环境转移不含 $\theta$）： $$P(\tau | \theta) = \rho_0(s_0) \prod_{t=0}^T P(s_{t+1} | s_t,a_t) \pi_\theta(a_t | s_t)$$
用log-derivative trick 把 $\nabla J(\theta)$ 写成期望（积分）：$$J(\theta) = \int P(\tau | \theta) R(\tau) d\tau$$ 求梯度： $$\nabla_\theta J(\theta) = \int \nabla_\theta P(\tau | \theta) R(\tau) d\tau$$ 用 log-derivative trick: $$\nabla_\theta P(\tau | \theta) = P(\tau | \theta) \nabla_\theta \log P(\tau | \theta)$$ 代入得: $$\nabla_\theta J(\theta) = \int P(\tau | \theta) \nabla_\theta \log P(\tau | \theta) R(\tau) d\tau = \mathbb{E}_{\tau\sim\pi_\theta}\left[\nabla_\theta \log P(\tau | \theta) R(\tau)\right]$$ 而因为环境部分不依赖 $\theta$, 所以 $$\nabla_\theta \log P(\tau | \theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t)$$ 于是得到最“原始”的 REINFORCE 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_\tau\left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]$$
**还完全是 on-policy**：期望是对“当前策略 $π_{θ}$​ 诱导出的轨迹分布 $p_\theta(\tau)$”取的。
“重要性采样（IS）”出现的唯一原因是：**你想用别的分布采到的样本来估计这个期望**——典型就是用旧策略 $\pi_{\text{old}}​$ 采到的轨迹来更新新策略 $\pi_\theta$​。
1) 从 on-policy 期望到 IS：就是一个换测度恒等式 对任意函数 $f(\tau)$，只要 $p_{\text{old}}(\tau) > 0 \Rightarrow p_\theta(\tau) > 0$（支持集覆盖），都有： $$\mathbb{E}_{\tau\sim p_\theta}[f(\tau)] = \sum_\tau p_\theta(\tau)f(\tau) = \sum_\tau p_{\text{old}}(\tau)\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)}f(\tau) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[w(\tau)f(\tau)\right]$$ 这一步就是重要性采样的定义：用权重 $w(\tau) = \frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}$ 把“在 $p_{\text{old}}$ 下的期望”变成“在 $p_\theta$ 下的期望”。
2) 把 $f(\tau)$ 换成你的 policy-gradient integrand 令 $$f(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)$$ 代回去就得到 off-policy 形式: $$\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim p_{\text{old}}}\left[\underbrace{\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)}}_{w(\tau)} \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)\right]$$ 这就是“从 on-policy policy gradient 到重要性采样”的那一步：只是把期望的采样分布从 $p_\theta$ 换成 $p_{\text{old}}$。
3) 为什么 $w(\tau)$ 会变成“连乘的 ratio”（而且环境转移会抵消） 
       轨迹分布可写成（环境转移不依赖 $\theta$）： $$p_\theta(\tau) = \rho_0(s_0) \prod_{t=0}^T \pi_\theta(a_t | s_t) P(s_{t+1} | s_t,a_t)$$因此比值： $$\frac{p_\theta(\tau)}{p_{\text{old}}(\tau)} = \prod_{t=0}^T \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$所有 $P(\cdot)$ 都消掉了，所以 IS 权重只依赖两套策略，不依赖环境动态。Sutton & Barto 在讲 off-policy MC 时把这个“转移概率抵消”写得很明确，并给出了乘积形式的 ratio（他们用 $\mu$ 表示行为策略/旧策略）。 映射到  LLM: $s_t = (x,y_{<t})$，$a_t = y_t$，于是 $$w_{\text{seq}}(y) = \prod_t \frac{\pi_\theta(y_t | x,y_{<t})}{\pi_{\text{old}}(y_t | x,y_{<t})}$$这就是你一直写的“序列级重要性比率”。问题在于：**用这个 $w_{seq}(y)$ 做梯度估计方差巨大**（尤其长序列时），**TRPO/PPO 走的是另一条路：改目标函数、做“局部近似 + 信赖域”**，而不是严格轨迹 IS。
4) TRPO/PPO 的目标是：**不要用这个高方差的轨迹级 $w(\tau)$**，改用更稳定的“局部近似 + 信赖域”。
       TRPO 先定义策略表现（折扣回报）$\eta(\pi)$，以及 $Q^\pi, V^\pi, A^\pi$（优势） 然后给出一个关键恒等式（performance difference）： $$\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{\tau\sim\tilde{\pi}}\left[\sum_{t\ge0} \gamma^t A^\pi(s_t,a_t)\right]$$
       直觉：**新策略的期望回报 = 旧策略期望回报 + 在新策略采样到的轨迹上，旧策略优势函数的折扣和，我们需要做的是优化后者**。
       并进一步“把沿时间步求和”改写成“按状态出现次数加权”： $$\eta(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A^\pi(s,a)$$直接优化这个式子很难（策略一变，后续会走到哪些状态的分布就全变了）。
       符号解释： 
    - $\tilde{\pi}$：你想更新得到的新策略 
    - $\pi$：当前/旧策略（后面常写 $\pi_{\text{old}}$）
    - $\rho_{\tilde{\pi}}(s)$：折扣访问频率，直观就是“在 $\tilde{\pi}$ 下状态 $s$ 被访问到的加权次数” 
    - $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$：动作相对平均水平的好坏 
	到这里为止，TRPO 还没引入任何重要性采样比率。
       
   5) TRPO 的“局部近似 surrogate”从哪来？ 
         TRPO 说：我先忽略“状态访问分布变化”，把 $\rho_{\tilde{\pi}}$ 用 $\rho_\pi$ 近似（$\rho_\pi$ 是“旧策略跑出来的数据分布”，你手里真的有样本可以估计它，所以 surrogate 可算、可做 SGD。），得到局部 surrogate： $$L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s \rho_\pi(s) \sum_a \tilde{\pi}(a|s) A_\pi(s,a)$$并且它证明：这个 surrogate 在 $\theta = \theta_0$ 处与真目标一阶相同（梯度相同、函数值相同）。 这就是理论支撑：TRPO 优化的不是“严格等于真目标”的东西，而是“在旧策略附近一阶正确”的 surrogate。 所以只要你每次更新别走太远，优化 surrogate 就能稳定提升真目标。 这也解释了“trust region / KL 约束”为啥必要：它是用来保证“别走太远”。
>这两个式子唯一差别是把 $\rho_{\tilde{\pi}}$ 换成了 $\rho_\pi$；而在 $\tilde{\pi} = \pi$ 处，$\rho_{\tilde{\pi}} = \rho_\pi$，并且“$\rho$ 对 $\theta$ 的导数项”会乘上一个在每个状态都为 0 的量：$\sum_a \pi(a|s) A^\pi(s,a) = 0$，所以导数差异消失。

6) 从 TRPO surrogate 到 PPO 里熟悉的 $r_t \hat{A}_t$ 现在看 surrogate 里的关键项： $$\sum_a \tilde{\pi}(a|s)A_\pi(s,a)$$这里的s特指某一个s，但是在TRPO的真等式本来外层就对状态加权。
       整理一下（$\tilde{\pi} \equiv \pi_\theta$）：$$\sum_a \pi_\theta(a|s)A(s,a) = \sum_a \pi_{\text{old}}(a|s)\frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)}A(s,a) = \mathbb{E}_{a\sim\pi_{\text{old}}(\cdot|s)}\left[r(a,s)A(s,a)\right]$$ PPO 论文在讲 TRPO 背景时就直接写了：TRPO 最大化 $\hat{\mathbb{E}}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \hat{A}_t\right]$ 并加 KL 约束。 并且 PPO 论文也把这个 surrogate 写成： $$L^{\text{CPI}}(\theta) = \hat{\mathbb{E}}_t\left[r_t(\theta)\hat{A}_t\right]$$**这里的“t”本质就是把 rollout 收集到的一堆 $(s_t, a_t)$（LLM 里就是 token 位点）摊平后的样本索引，这里可以token-level loss（GRPO）也可以sample level loss（DAPO）。** 
       然后再引入 clip。 到这里你应该能看到：PPO 的 $r_t \hat{A}_t$ 不是从 $\prod r_t$ 直接“化简”出来的；**它来自 TRPO 的“局部 surrogate + 在每个 state 上对 action 的期望用单步 IS 改写”**。

## 1.RLHF + PPO训练LLM的全流程：

### 1.1训练前的准备

**SFT** 阶段：首先需要一个已经完成预训练的 LLM ，它的训练任务是“Next Token Prediction”，SFT就是在这个 LLM 的基础上用“指令-回答”数据集监督微调，最后得到的模型就是 SFT 模型。
**RM** 阶段：复用SFT模型的架构来构建Reward Model。和Critic一样，它也有一个特殊的“奖励头”，输出一个标量值。训练数据都是一个 `(prompt, SFT模型回答_1, SFT模型回答_2, 排序结果)` 的组合。

| 模型                        | 来源                         | 在PPO阶段的状态           | 训练阶段            |
| ------------------------- | -------------------------- | ------------------- | --------------- |
| Actor $π_θ$               | 复制自SFT模型                   | 可训练，在PPO中不断更新       | PPO             |
| Critic $V_φ$              | 架构和权重来自SFT模型，但有一个随机初始化的价值头 | 可训练，在PPO中与Actor同步更新 | PPO             |
| Reference Model $π_{SFT}$ | 就是SFT模型本身                  | 冻结，参数固定不变           | SFT             |
| Reward Model (RM)         | 架构来自SFT模型，在人类排序数据上单独训练     | 冻结，参数固定不变           | Reward Modeling |
### 1.2训练主循环

###### 步骤一：数据收集 (Rollout / 与环境交互)
一句话：我们让当前的Actor模型 $π_θ$ 生成一大批对话数据。具体流程：
1. 从一个数据集中随机抽取一批 `prompt`。
2. 对于每一个 `prompt`，让Actor $π_θ$ 自回归地（一个词一个词地）生成一个完整的回答。
3. 在生成每一个词 $a_t$ 的过程中，我们需要记录下一整条“轨迹 (trajectory)”所需的所有信息：
    - **状态 $s_t$**: 生成 $a_t$ 之前的文本序列。
    - **动作 $a_t$**: 实际生成的那个词 (token)。
    - **动作概率 $log(π_θ(a_t|s_t))$**: Actor模型在状态 $s_t$ 下，选择动作 $a_t$ 的对数概率。
    - **价值估计 $V_φ(s_t)$**: Critic模型对当前状态 $s_t$ 的打分。
    - **KL散度奖励 $r_{KL}$**: 这是我们之前讲的“过程分”，用于稳定训练。
4. 当一个回答生成完毕（遇到结束符 $<eos>$），我们计算它的最终奖励：
    - 将完整的 `prompt + 回答` 输入给 RM，得到一个最终分数 $R_{final}$。
    - 将 KL散度奖励 和 最终奖励 结合。通常是这样：
        - 对于中间的每一步 `t`，奖励 $r_t = -β * KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$。
        - 对于最后一步 `T`，奖励 $r_T = R_{final} + r_{KL}$。
5. 重复以上过程，直到收集到足够多的轨迹数据（例如，几千条完整的对话）。现在，我们手上有一个巨大的经验池，里面装满了 $(s_t, a_t, log(π_θ(a_t|s_t)), V_φ(s_t), r_t)$ 这些数据点。
###### 步骤二：优势计算
==“构造优势”本身就是在做方差降低：把回报做“去均值/中心化”的 baseline（控制变量） 技巧。==
==优势的定义是 `A(s,a) = Q(s,a) - V(s)`。问题在于，真实的`Q`值我们永远不知道。我们只能用不同的方法去**估计**它。==
两种极端的估计方法：
1. TD估计 (只看一步): $Â_{TD}(t) = r_t + γV(s_{t+1}) - V(s_t) = δ_t$
    - 优点: 方差低。因为它主要依赖于相对稳定的函数`V`，而不是充满随机性的真实奖励。
    - 缺点: 偏差高。因为$V(s_{t+1})$身就是一个不准确的估计，这个偏差会引入到$Â$的计算中。
2. 蒙特卡洛估计 (看到底): $Â_{MC}(t) = (r_t + γr_{t+1} + γ^2r_{t+2} + ...) - V(s_t)$
    - 优点: 无偏。因为它完全基于真实发生过的奖励序列，没有任何估计成分。
    - 缺点: 方差高。因为真实的奖励序列每次都可能因随机性而剧烈波动。
我们可以定义一个“2步优势估计”：$Â^2(t) = r_t + γr_{t+1} + γ^2V(s_{t+2}) - V(s_t)$。  
可以定义一个“k步优势估计”：$Â^{k}(t) = (Σ_{l=0}^{k-1} γ^l r_{t+l}) + γ^k V(s_{t+k}) - V(s_t)$
GAE的公式 $Â_t = δ_t + (γλ)δ_{t+1} + (γλ)^2δ_{t+2} + ...$ 
                  $Â_t = δ_t + (γλ) * Â_{t+1}$
                  $\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_{\text{old}}(s_{t+1}) - V_{\text{old}}(s_t)$
                  PS：λ=1意思是把所有长度的 TD 残差都权重一样地累加
- `t`时刻的GAE优势，不仅仅是`t`时刻的TD-Error $δ_t$。
- 它还包含了对未来`t+1`时刻TD-Error $δ_{t+1}$的一次折扣考虑。
- 它还包含了对未来`t+2`时刻TD-Error $δ_{t+2}$的二次折扣考虑。
- ...以此类推，直到序列结束。
###### 步骤三：模型优化
**Rollout 之后：先计算 returns/advantages；然后在同一个 PPO 更新循环里，actor 和 critic 在每个 mini-batch 上一起更新（或交替更新）。**
1. Actor Loss：$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$
2. Critic Loss：$L_{VF}(φ) = E[ (V_φ(s_t) - V_t^{target})^2]$ = $\mathbb{E}\left[\left(V_φ(s_t) - R_t\right)^2\right]$
			   $V_t^{\text{target}} = \hat{R}_t = \hat{A}_t + V_{old}(s_t)$
3. Entropy of the policy Bonus：$S = E[ -Σ_a π_θ(a|s_t) * log(π_θ(a|s_t)) ]$，最大化熵（鼓励探索）。

>重要性采样（==RLHF的PPO中用到的重要性采样是”序列级“==）：$$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$$
>Q:理论上用 $\mathbb{E}_{a\sim\pi_\theta(\cdot|s_t)}[\hat{A}(s_t,a)] = \sum_a \pi_\theta(a|s_t)\hat{A}(s_t,a)$ （没有重要性采样）当作Actor的优化目标是完全可以的。那为什么不这样写而是引入重要性采样呢？
>A：$\hat{A}(s_t,a)$ 是 $\pi_{old}$ 得到的，$\hat{A}(s_t,a)$ 里所有数据（reward, value, log‑probs）都是**基于旧策略分布**，但是目标函数应该是**对当前策略分布的期望**。

>Q：LLM用到的RLHF是policy based还是value based？
>A: 本质上是policy-based（策略梯度）为主，但训练时通常带一个 **critic/value function** 当 baseline ——也就是 **actor–critic**。
>**用到了 value**，但不是 “value-based RL” 的那种（以学 Q 为主、用 Q 决策）。

>Q:为什么LLM很少用纯value-based（比如Q-learning）？
>A：动作空间是“词表大小（几万）× 每个位置”的巨大离散空间，且序列很长，直接学全动作的 Q(s,a)并做 max/bootstrapping 通常更难、更不稳定；策略梯度更自然。
>Q：什么是bootstraping？
>A：bootstraping（自举），用自己当前学到的价值函数预测，来当作“未来回报”的一部分目标，而不是等到把整段轨迹都走完再算真实回报。比如​用$r_t + \gamma V(s_{t+1})$ 或 GAE 来构造$\hat{R}_t$和$\hat{A}_t$。
>bootstraping的优点就是方差小；缺点就是有偏差。

> **on-policy**：你用“当前策略 $\pi_\theta$​”采样的数据，也用来更新“当前策略”。指**不长期用老数据**，不是指“每个 SGD step 都完全 on-policy”。比如在一轮rollout中，采样到的数据会被分为很多mini-batch。$\pi_{\text{old}}$ 就是rollout时的策略，$\pi_\theta$ 在一个mini-batch就会更新一次，第一个mini-batch时，重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ = 1。**第一轮更新后， $\pi_\theta$​ 已经变了，但数据还是旧策略采的**。因此后面那些更新，从严格意义上讲就“off-policy 了”，GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”
> **off-policy**：你用“别的策略”（旧策略/行为策略）采样的数据，拿来更新“当前策略”。
## 2.GRPO

在 **GRPO（Group Relative Policy Optimization）** 里确实**没有 critic / value function**，所以它不走 “$A=\hat R - V(s)$” 这条路，而是用 **“同一个 prompt 下多次采样得到的一组回答（group）”** 来构造一个 **baseline**，从而得到优势（advantage）。
对同一个prompt x，从当前策略π_θ采样K个回答y₁,…,y_K。 
1. 先算每个回答的标量奖励（通常含KL惩罚）： $r_i = R(x,y_i) - \beta \text{KL}(\pi_\theta(\cdot|x) \parallel \pi_{\text{ref}}(\cdot|x))$
2. 用组内baseline（最常见是均值）： $b = \frac{1}{K} \sum_{j=1}^K r_j$ 
3. 定义优势（相对同组其他回答“好多少”）： $\hat{A}_i = r_i - b$ 很多实现还会做 组内标准化（更稳）： $\hat{A}_i = \frac{r_i - \mu}{\sigma + \epsilon}, \quad \mu = \frac{1}{K} \sum_{j=1}^K r_j, \quad \sigma = \sqrt{\frac{1}{K} \sum (r_j - \mu)^2}$ 
>直觉：同一个prompt的这组回答里，比平均好就正优势，比平均差就负优势。这样就不需要一个单独的critic来预测V(s)当baseline了。

## 3.DPO
#### DPO 的核心：用“成对偏好”替代 advantage
数据是一对回答：$(x, y^+, y^-)$，表示在prompt x下人更喜欢$y^+$而不是$y^-$。 
DPO定义一个“偏好logit”（可以把它当成一种相对优势）： $\Delta_\theta = \left[\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x)\right] - \left[\log \pi_{\text{ref}}(y^+|x) - \log \pi_{\text{ref}}(y^-|x)\right]$ 
然后用logistic loss： $L_{\text{DPO}}(\theta) = -\mathbb{E}\left[\log \sigma(\beta \Delta_\theta)\right]$ 
- $\sigma$: sigmoid 
- $\beta$: 温度/强度超参（控制偏好推动的力度） 
- ref项让更新不要偏离参考模型太多（效果上类似RLHF里的KL约束）

#### “那优势在哪？”
如果你非要找一个“优势”的影子，它就是这个 成对差值的logit: 
- 在PPO/GRPO里：优势是“这个动作/回答比baseline好多少” 
- 在DPO里：你不需要baseline，因为标签直接告诉你$y^+$应该比$y^-$好 所以DPO直接优化“偏好差”: $\Delta_\theta$ 越大越好 
- 可以把$\Delta_\theta$看成一种 pairwise advantage（“偏好优势”），但它不是$Q-V$那种per-state的优势，也不需要critic去估$V$。

==强化学习中神经网络本质：把你在环境里交互得到的经验（状态、动作、奖励、下一个状态）压缩成一个可微分的函数，用来**决策**（怎么选动作）和/或**评估**（这个状态/动作有多好），从而最大化长期回报。==

## 4.强化学习主流算法总结
现在主流的大模型RL微调，基本都跑在PPO家族上：一开始是标准PPO，再到DPO这类“去掉value、走偏好”的方向，后面DeepSeek提了GRPO，Qwen又在这个基础上提出了DAPO、GSPO，再往前一步就是现在的SAPO。整个演进有一个共同的主线，就是在解决两件事：**训练稳定性和样本效率**，尤其是在长CoT、MoE这种比较折腾的场景下。

GRPO 的创新是用组内相对比较 + token 级 PPO 框架，去掉 value 和 RM；它的问题是 **reward 粒度和更新粒度不一致**，尤其是在长序列和 MoE 上不稳定。
>用于“任务好坏”的主奖励（RM/正确性）是序列级的，而参数更新却要对每个 token 的 logprob 做梯度；
>于是主奖励会被“广播”到所有 token，上下文里哪个 token 真正导致成功/失败很难归因。
>KL/熵这些 token 级项，更多是“正则/约束/探索”，不是“任务信用分配”。

### 4.1 PPO>DPO>GRPO>DAPO>GSPO>SAPO
DAPO、GSPO、SAPO都在解决同一类痛点：**长序列 + 多个 mini-batch 更新会带来 off-policy；如果用“token 级重要性比率 + token 级裁剪”，方差会沿序列累积，容易不稳定甚至崩溃**。GSPO 论文把这个问题说得很直：GRPO 把重要性权重用在每个 token 上，本质上“**用单一样本去做分布校正**”会引入高方差噪声，并随长序列累积；而奖励是整段给的，所以“优化单位应匹配奖励单位”。

>"分布校正"：你手里有的数据是按 **旧分布/旧策略** $\pi_{\text{old}}$​ 采出来的，但你想优化的是 **新分布/新策略** $\pi_\theta$。重要性采样的做法“**用权重把分布从 old 校正到 new**”，所以又RLHF中PPO算法的重要性比率 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ 。
>
>"单一样本做分布校正"：在每个 token 位置只采到一个 token，就试图用它的概率比率去代表整个位点的分布差异；理论上需要大量样本平均才稳定，现实里会变成高方差噪声。
>在 LLM-RL（GRPO 那套）里，每个 token 位置 t 的“动作”就是生成的那个 $token \ a_t$​。但在某个前缀状态 $s_t$ 下，你通常只采到 **一个** token（一次只走了一条路径）。于是你用 **这一条路径上这一个 token** 的比率$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$来做“分布校正”，**只采样一次就用重要性采样本身就会带来偏差（或者说是引入了噪声）。**

在 LLM 的 RL 里，一次 rollout 得到的是整段序列 $y_{1:T}$，而你常见的训练做法是：**一大批 rollout 数据切成多个 mini-batch，做多次梯度更新**——这一步直接把学习推向 off-policy：数据来自旧策略 $\pi_{\text{old}}$​，但你在优化新策略 $\pi_\theta$​。GSPO 论文把这点写得很直：把 rollout batch 切成多个 mini-batch 更新“**不可避免引入 off-policy**”。
#### 4.1.1 从最基本的理论：重要性采样在“随机变量本身的分布”上做换测度 
1) 设你要的目标是（prompt $x$ 固定）： $$J(\theta) = \mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[R(x,y)]$$这里 $y = y_{1:T}$ 是整段序列，奖励 $R(x,y)$ 也是整段函数。 如果你手里只有旧策略 $\pi_{\text{old}}$ 采样来的序列 $y$，那要把期望换成在旧分布下： $$\mathbb{E}_{y\sim\pi_\theta}[R] = \sum_y \pi_\theta(y|x)R(x,y) = \sum_y \pi_{\text{old}}(y|x)\underbrace{\frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}}_{\text{正确的IS权重}}R(x,y) = \mathbb{E}_{y\sim\pi_{\text{old}}}\left[w_{\text{seq}}(y)R(x,y)\right]$$所以理论上对应的权重就是 $$w_{\text{seq}}(y) = \frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}$$这一步就是经典重要性采样恒等式，没有额外假设。 在 RL 文献里，这个 $w$ 叫 trajectory-wise / ordinary importance sampling ratio：用“轨迹在目标策略下发生的概率”除以“轨迹在行为策略下发生的概率”。Thomas & Brunskill 的 high-confidence OPE 论文就把无偏估计写成：对每条轨迹用 $\Pr(\tau|\theta)/\Pr(\tau|\theta_i)$ 乘以回报，并进一步展开成沿时间步的概率比连乘。

2) 为什么它在序列生成里会变成“token 比率的连乘”（也就是你说的序列级 ratio） LLM 自回归生成的序列概率本来就按链式法则分解： $$\pi_\theta(y|x) = \prod_{t=1}^T \pi_\theta(y_t | x, y_{<t})$$ 因此 $$w_{\text{seq}}(y) = \frac{\prod_t \pi_\theta(y_t|x,y_{<t})}{\prod_t \pi_{\text{old}}(y_t|x,y_{<t})} = \prod_{t=1}^T \underbrace{\frac{\pi_\theta(y_t|x,y_{<t})}{\pi_{\text{old}}(y_t|x,y_{<t})}}_{r_t}$$ 这就是“序列级重要性比率 = token 比率连乘”的严格推导。 Sutton & Barto 在讲 off-policy Monte Carlo 时也明确把重要性采样比率写成沿轨迹的动作概率比连乘（$\prod \pi/\mu$），并讨论它会导致方差极端甚至无穷。
3) PPO/GRPO 的 loss 不是把“整段一个标量权重 $w_{\text{seq}}$”乘到整段梯度上；它是把每个 token 的梯度分别乘上各自的 $r_t$（以及各自的 clip/min 分支）再求和：$$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$
### 4.2 DAPO：不改“token-level ratio”的骨架，而是用一套工程化手段把它“撑稳、撑久”
它是在 token-level PPO/GRPO 框架不变的情况下，把“熵塌缩、有效梯度减少、长序列权重失真、截断奖励噪声”这些最容易把系统推向崩溃的因素逐个拆弹。
	1) Decoupled Clip / Clip-Higher：上界放宽，保住探索与熵 
	DAPO观察到：PPO常用的对称clip（如ε = 0.2）对“低概率探索token”极不友好——上界1+ε会把它们的概率提升死死卡住，从而让熵过快塌陷、样本趋同。于是它把clip下界ε low和上界ε high解耦，并提高上界给探索留空间。 
	本质：不是在“纠正off-policy”，而是在避免clip机制把探索通道掐死，从而让长链推理的策略不会早早变成确定性、走向崩。 
	2) Dynamic Sampling：把“零梯度/弱梯度batch”从数据管道里清掉 
	在verifiable reward（对/错）里，经常出现某些prompt的一组采样全对或全错→组内标准化优势变成0→整个prompt对梯度没贡献。DAPO选择“多采样+过滤”把这些样本从batch里剔除，保证每个batch里有效梯度数量稳定，降低噪声敏感性。 
	本质：你解决的不是“ratio方差”，而是有效样本数下降导致的梯度方差飙升。 
	3) Token-level Policy Gradient Loss：让长样本该有的权重回到梯度里 
	DAPO指出GRPO的sample-level loss（先对token平均再对样本平均）会让长序列里每个token的贡献被稀释：高质量长推理学不进去，低质量超长胡言乱语又惩罚不够，导致熵和长度不健康膨胀。于是它改成token-level方式聚合，让长序列在梯度里有更合理的影响，并能更强地压制样本里的坏模式。 
	本质：这是在修“长序列下的梯度分配/信用分配失真”，让训练动力学更健康。 
	4) Overlong Reward Shaping：减少截断带来的奖励噪声 
	生成被截断时，如果直接给惩罚，会把“本来正确但太长”的推理也当作错误，产生强噪声扰动；DAPO用过滤/软惩罚等方式降低这种噪声，显著稳定训练。

### 4.3 GSPO：直接把“off-policy 校正单位”改成序列级，让优化单位对齐奖励单位
它认为 GRPO 的崩溃来自更根本的“算法目标 ill-posed”：token-level importance weight 在每个位置只用单样本，无法完成分布校正，噪声随长度累积并被 clip 放大，导致崩溃。
因此 GSPO 做了一个原则性改变：**重要性比率用序列似然定义，clip/优化也在序列级进行**。

GSPO定义序列级权重 $s_i(\theta)$ 为序列似然比的几何平均（长度归一化）： $$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\text{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left( \frac{1}{|y_i|} \sum_t \log \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\text{old}}(\cdot)} \right)$$并在序列级做PPO-clip: $\min(s_i A_i, \text{clip}(s_i) A_i)$。 可以把“整段序列”看成一个“大动作”，那么整条序列的优势 $A_{i}$ ：对同一 query x 采样 GGG 条回复，先算每条的 reward，再做组内归一化，把“归一化后的 reward”当作优势； $s_i(\theta)$  是整条序列一个数（由这条序列所有 token 的 log-ratio 平均决定）。**这里的 $i$ 就是一条序列 / 一条轨迹。**
它还强调：这样做符合“奖励给的是整段序列，所以off-policy指标/裁剪也应匹配序列级”。 一句话总结GSPO的本质：把“校正与裁剪的单位”从token提升到sequence，用长度归一化的序列ratio作为真正有意义的off-policy距离度量，从源头上切断token-level单样本权重导致的高方差累积。
>
### 4.4 SAPO：在“序列一致性”和“token 适应性”之间，做连续可调的软信赖域
SAPO 的最重要特点：它承认 GSPO 把单位提升到序列级后更稳定，但 **GSPO 的 hard clip 太“脆”**：序列里只要有少数极 off-policy 的 token 把序列指标推到剪切带外，**整条序列的梯度就会被压没**；这牺牲了大量“其实仍近似 on-policy 的 token”的学习信号。SAPO 用一个**温度控制的 sigmoid 软门控**替代 hard clip：偏离越大，权重越平滑地衰减，而不是直接清零。

>on-policy 的 token：如果 $r_t(\theta) \approx 1$，表示“新策略在这个前缀上对这个 token 的概率几乎没变”，这就叫这个 token 接近 on-policy（near on-policy）——它和生成数据时的分布很一致。 在这种情况下，这个 token 对更新的贡献（你可以叫它“学习信号”）通常是更可靠/噪声更小的，因为你没有用很离谱的 off-policy 权重去放大或扭曲。

>GSPO 的 hard clip 太“脆”，但是PPO/GRPO的梯度没那么脆：因为GSPO 用的是**序列级** ratio。

>整条序列的梯度就会被压没：在band也就是置信区域外，梯度 = 0。

SAPO 的本质：把 GSPO/GRPO 的“硬裁剪信赖域”变成“可微、连续、可调的软信赖域”，在大多数时候保持序列级对齐，在遇到离群 token 时只局部降权，从而同时提升稳定性与样本效率。

1) SAPO 的目标函数 $$J(\theta) = \mathbb{E}_{q\sim\mathcal{D}, \{y_i\}_{i=1}^G\sim\pi_{\theta_{\text{old}}}(\cdot|q)}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} f_{i,t}(r_{i,t}(\theta)) \bar{A}_{i,t} \right]$$ 这里: 
	- $q$: query/prompt; $\mathcal{D}$: query 集合 
	- 一次对同一个 $q$ 采样 $G$ 个回复 $y_1,\dots,y_G$ (来自行为策略 $\pi_{\theta_{\text{old}}}$) 
	- $|y_i|$: 第 $i$ 个回复的 token 数 
	- $\bar{A}_{i,t}$: 组内归一化 advantage (对同一条回复内所有 token 共享) 
	- $r_{i,t}(\theta)$: token 级重要性比率 (见下) 

2) token 比率 $r_{i,t}$ 与组内优势 $\bar{A}$  $$r_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t} | q, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t} | q, y_{i,<t})}, \quad \bar{A}_{i,t} = \bar{A}_i = \frac{R_i - \text{mean}(\{R_j\}_{j=1}^G)}{\text{std}(\{R_j\}_{j=1}^G)}$$
3) SAPO 的“软门控” $f_{i,t}$ $$f_{i,t}(x) = \sigma(\tau_{i,t}(x - 1)) \cdot \frac{4}{\tau_{i,t}}, \quad \tau_{i,t} = \begin{cases} \tau_{\text{pos}}, & \bar{A}_{i,t} > 0 \\ \tau_{\text{neg}}, & \text{otherwise} \end{cases}$$ 其中 $\sigma(z) = \frac{1}{1+e^{-z}}$ 是 sigmoid。 直觉: 
	- hard clip 是“过界就截断/置零” 
	- SAPO 是“离 1 越远就越平滑地衰减”(而且正/负 advantage 用不同温度，负向更容易不稳定就让它衰减更快)
4) 看 SAPO 真正更新什么：它的梯度形式 
		$$\nabla_\theta J(\theta) = \mathbb{E}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} w_{i,t}(\theta) r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(y_{i,t} | q, y_{i,<t}) \bar{A}_{i,t} \right]$$
       其中软门控产生的梯度权重是: $$w_{i,t}(\theta) = 4p_{i,t}(\theta)(1-p_{i,t}(\theta)), \quad p_{i,t}(\theta) = \sigma(\tau_{i,t}(r_{i,t}(\theta)-1))$$并且它有个非常好记的等价形式 (利用 $4\sigma(z)(1-\sigma(z)) = \text{sech}^2(z/2)$): $$w_{i,t}(\theta) = \text{sech}^2\left( \frac{\tau_{i,t}}{2}(r_{i,t}(\theta)-1) \right)$$这就是 SAPO 的关键: - PPO/GRPO 的“有效系数”大致是 (硬裁剪后的) $r_{i,t}$ 或者直接 0 - SAPO 的“有效系数”变成 $w_{i,t}(\theta)r_{i,t}(\theta)$: 不再 hard clip，而是用 $w$ 做连续衰减 - 在 $r_{i,t}=1$ 时 $w_{i,t}=1$，所以在 on-policy 点附近，梯度保持和未裁剪一致 (论文也强调这一点) 
5) SAPO 为什么说自己“sequence-coherent”? 
      论文还给了一个结论: 在“小步更新 + 序列内 token log-ratio 离散度低”等条件下，token gate 的平均会集中成一个序列级 gate: $$g(\log s_i(\theta)) = \text{sech}^2\left( \frac{\tau_i}{2} \log s_i(\theta) \right)$$这就是它宣称“像 GSPO 一样对齐序列奖励，但又能在有离群 token 时只压离群 token”的数学抓手。 
      如果你想把 SAPO 和 GSPO/GRPO 的公式放到同一个统一视角里，其实可以只看一件事:“梯度里乘在 $\nabla \log \pi$ 前面的那个系数” 
      - GRPO: hard clip 让很多 token 直接系数变 0 
      - GSPO: 整条序列一个 hard clip (序列被少数 token 推出边界就全 0) 
      - SAPO: 用 $w_{i,t}$ 把“是否更新”变成连续值 (更连续信赖域) 你要不要我用同一个“effective weight”把三者写成一行对照?

>SAPO 目标函数的求导：
>1) 先把梯度推进期望里 SAPO 的目标: $$J(\theta) = \mathbb{E}_{q\sim\mathcal{D}, \{y_i\}\sim\pi_{\theta_{\text{old}}}(\cdot|q)}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} f_{i,t}(r_{i,t}(\theta)) \bar{A}_{i,t} \right]$$ 其中 $q, \{y_i\}$ 的采样分布是 $\pi_{\theta_{\text{old}}}$（固定不随 $\theta$ 变），所以对 $\theta$ 求导时可以把 $\nabla_\theta$ 放进期望与求和里: $$\nabla_\theta J(\theta) = \mathbb{E}\left[ \frac{1}{G} \sum_i \frac{1}{|y_i|} \sum_t \nabla_\theta \left(f_{i,t}(r_{i,t}(\theta)) \bar{A}_{i,t}\right) \right].$$ 通常（论文里也是这样）把 $\bar{A}_{i,t}$ 当作常数对待（stop-grad），因为它由奖励/组内标准化算出来，不从 actor 这条链回传。 于是: $$\nabla_\theta \left(f_{i,t}(r_{i,t}(\theta)) \bar{A}_{i,t}\right) = \bar{A}_{i,t} \nabla_\theta f_{i,t}(r_{i,t}(\theta)).$$2) 第一层链式法则: $\nabla_\theta f_{i,t}(r_{i,t}(\theta)) = f_{i,t}'(r) \cdot \nabla_\theta r$ SAPO 定义: $$f_{i,t}(x) = \sigma(\tau_{i,t}(x - 1)) \cdot \frac{4}{\tau_{i,t}}$$ 令 $z = \tau_{i,t}(x - 1)$，则 $$\frac{d}{dx} f_{i,t}(x) = \frac{4}{\tau_{i,t}} \cdot \sigma'(z) \cdot \tau_{i,t} = 4\sigma'(z).$$ 而 sigmoid 导数 $\sigma'(z) = \sigma(z)(1 - \sigma(z))$，所以 $$f_{i,t}'(x) = 4\sigma(\tau_{i,t}(x - 1))(1 - \sigma(\tau_{i,t}(x - 1))).$$ 把 $x = r_{i,t}(\theta)$ 代回去，定义论文里的: $$p_{i,t}(\theta) = \sigma(\tau_{i,t}(r_{i,t}(\theta) - 1)), \quad w_{i,t}(\theta) = 4p_{i,t}(\theta)(1 - p_{i,t}(\theta)),$$ 就得到 $$\nabla_\theta f_{i,t}(r_{i,t}(\theta)) = w_{i,t}(\theta) \nabla_\theta r_{i,t}(\theta).$$3) 第二层链式法则: $\nabla_\theta r_{i,t}(\theta) = r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(\cdot)$ token ratio 定义是 $$r_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t} | q, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t} | q, y_{i,<t})}.$$ 分母不依赖 $\theta$，所以 $$\nabla_\theta r_{i,t}(\theta) = \frac{1}{\pi_{\theta_{\text{old}}}(\cdot)} \nabla_\theta \pi_\theta(\cdot) = \frac{\pi_\theta(\cdot)}{\pi_{\theta_{\text{old}}}(\cdot)} \nabla_\theta \log \pi_\theta(\cdot) = r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(y_{i,t} | q, y_{i,<t}).$$4) 合并两层链式法则： $$\nabla_\theta f_{i,t}(r_{i,t}(\theta)) = w_{i,t}(\theta) r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(y_{i,t} | q, y_{i,<t}).$$ 再乘回 $\bar{A}_{i,t}$，并放回外层的 $\frac{1}{G} \sum_i \frac{1}{|y_i|} \sum_t$ 与期望里，就得到： $$\nabla_\theta J(\theta) = \mathbb{E}\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} w_{i,t}(\theta) r_{i,t}(\theta) \nabla_\theta \log \pi_\theta(y_{i,t} | q, y_{i,<t}) \bar{A}_{i,t} \right],$$ 这正是 SAPO 文中“Differentiating (5) yields ...”的加权 log-policy gradient 。


## 5.强化学习算法大一统

## 5.1“对象→公式”的结构化总表（表内只放公式编号）

| 优化的对象（Object）                                              | 这类算法的“祖宗公式”编号                      | 你拆新算法时要重点找它改了什么                                           | 典型算法族（含 LLM RL）          |
| ---------------------------------------------------------- | ---------------------------------- | --------------------------------------------------------- | ------------------------ |
| **Policy 直接最大化回报**（纯策略梯度）                                  | PG-0 ~ PG-4                        | 回报用 **R(τ)** 还是 **G_t**？有没有 baseline/advantage？梯度估计器怎么写？  | REINFORCE、Vanilla PG     |
| **Actor-Critic（策略 + 价值）**                                  | AC-1 + TD-1/TD-2 + (可选)GAE-1/GAE-2 | Critic 学什么（V/Q/soft V）？Actor 用什么 advantage？是否 off-policy？ | A2C/A3C、GAE、PPO(A2C风格) ( |
| **Value/V 函数为对象（评估）**                                      | BELL-V + TD-1                      | 你是在解 Bellman 固定点，还是在最小化 TD/残差？                            | TD(0)/TD(λ)              |
| **Q 函数为对象（控制）**                                            | BELL-Q + BELL-Q* + QL-1 + DQN-1    | max/softmax？target network？off-policy？                    | Q-learning、DQN           |
| **Trust Region / Proximal（“别走太远”）**                        | TRPO-1 或 PPO-1                     | 约束是 **KL≤δ** 还是 clip？ratio 用 token 还是 sequence？           | TRPO、PPO                 |
| **Off-policy 修正（重要性采样系）**                                  | IS-1 或 VTRACE-1~3                  | 修正单位是 step/token 还是 trajectory/sequence？截断怎么做？            | IS、V-trace(IMPALA)       |
| **最大熵/熵正则（MaxEnt）**                                        | ENT-1 + SAC-1~3                    | reward 里加熵还是加 KL？soft Bellman 怎么写？                        | SAC                      |
| **KL 正则（RLHF/LLM 常见）**                                     | RLHF-1（以及 RM-1）                    | KL 是 **惩罚项** 还是 **reward shaping**？reference 是谁？          | InstructGPT PPO/RLHF     |
| **分布式价值（Distributional）**                                  | DIST-1 + DIST-2                    | 对象不再是 E[return]，而是 return 的分布 Z                           | C51/Distributional RL    |
| **Offline 保守/正则化（防止 OOD Q 爆炸）**                            | CQL-1                              | “保守项”长什么样（log-sum-exp 等）？和 Bellman 误差怎么拼？                 | CQL 等                    |
| **Occupancy Measure / LP（换个变量做 RL）**                       | OCC-1 + LP-1                       | 把策略换成 occupancy（流量守恒约束）                                   | MDP 线性规划/对偶              |
| **Model-based（模型派）**                                       | MB-1 + MB-2                        | 先学模型（监督学习目标）还是直接用模型做规划/控制？                                | Dyna/MPC/MBPO 风格         |
| **LLM：GRPO（token-level ratio + group advantage）**          | GRPO-1~3                           | advantage 怎么组内归一？ratio 单位是 token；KL 怎么加？                  | DeepSeekMath GRPO        |
| **LLM：GSPO（sequence-level ratio + sequence clip）**         | GSPO-1~2                           | **把 ratio 从 token 换成 sequence**（还做 length-normalize）      | Qwen GSPO                |
| **LLM：SAPO（soft/adaptive clip gate）**                      | SAPO-1~3                           | clip 的“min”不再硬切：用 λ 做软门控                                  | SAPO                     |
| **LLM：DAPO（$ε_{low}/ε_{high}$ + 动态采样 + token-level loss）** | DAPO-1~3                           | clip 上下界解耦、采样过滤条件、loss 聚合方式改成 token-level                 | DAPO                     |

---

## 5.2 公式库（可直接复制进 Obsidian；每条公式都用单个 `$$...$$`）

### 5.2.1 Policy / Policy Gradient（策略为对象）
**PG-0（最原始的目标）**  
$$  
J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\left[R(\tau)\right]  
$$

**PG-1（“祖宗式”——trajectory-level REINFORCE 形式，OK 的）**  
$$  
\nabla_\theta J(\theta)  
=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)R(\tau)\right]  
$$

**PG-2（reward-to-go 版本：把 R(τ) 换成从 t 开始的回报 $G_t$，方差更小）**  
$$  
\nabla_\theta J(\theta)  
=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)G_t\right] 
\quad  
,\ \ G_t=\sum_{k=t}^{T}\gamma^{k-t}r_k  
$$

**PG-3（加 baseline b(s) 不改期望、降方差）**  
$$  
\nabla_\theta J(\theta)  
=\mathbb{E}\left[\sum_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_t\mid s_t)\big(G_t-b(s_t)\big)\right]  
$$

**PG-4（Policy Gradient Theorem 常见写法：用 $Q^\pi / A^\pi$）**  
$$  
\nabla_\theta J(\theta)  
=\mathbb{E}_{s\sim d^{\pi_\theta},,a\sim\pi_\theta}\left[\nabla_\theta\log\pi_\theta(a\mid s)Q^{\pi_\theta}(s,a)\right]  
$$

---

### 5.2.2 Value / Bellman（价值为对象）

**BELL-V（Bellman expectation：评估 $V^\pi$）**  
$$  
V^\pi(s)=\mathbb{E}_{a\sim\pi(\cdot\mid s),s'\sim P(\cdot\mid s,a)}\left[r(s,a)+\gamma V^\pi(s')\right]  
$$

**BELL-Q（Bellman expectation：评估 $Q^\pi$）**  
$$  
Q^\pi(s,a)=\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\left[r(s,a)+\gamma\mathbb{E}_{a'\sim\pi(\cdot\mid s')}\left[Q^\pi(s',a')\right]\right]  
$$

**BELL-Q*（Bellman optimality：最优 Q）**  
$$Q^*(s,a) = \mathbb{E}_{s'\sim P(\cdot|s,a)} \left[ r(s,a) + \gamma \max_{a'} Q^*(s',a') \right] $$ 

**TD-1（TD 误差 δ）**  
$$  
\delta_t=r_t+\gamma V_\phi(s_{t+1})-V_\phi(s_t)  
$$

**TD-2（一个最经典的 critic 目标：最小化 TD 误差平方）**  
$$  
\min_\phi\ \mathbb{E}\left[\delta_t^2\right]  
$$

---

### 5.2.3 Q-learning / DQN（Q 为对象）

**QL-1（Q-learning 更新的标准型）**  
$$  
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\Big(r_t+\gamma\max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)\Big)  
$$

**DQN-1（DQN：用 target network 做回归）**  
$$  
\min_\theta\ \mathbb{E}_{(s,a,r,s')\sim D}\left[\Big(r+\gamma\max_{a'}Q_{\theta^-}(s',a')-Q_\theta(s,a)\Big)^2\right]  
$$

---

### 5.2.4 Actor-Critic（“actor 用 advantage，critic 学 V/Q”） 
**AC-1（actor 的通用更新：logπ × advantage）**
对应的是随机策略（**policy 是一个分布**）
$$  
\widehat{g}  
=\widehat{\mathbb{E}}_t\left[\nabla_\theta\log\pi_\theta(a_t\mid s_t)\widehat{A}_t\right]  
$$

---

### 5.2.5 Deterministic Actor-Critic（DDPG/DPG）

**DPG-1（确定性策略梯度）**  
对应的是确定性策略（**policy 是一个函数 $a=\mu_\theta(s)$**）
确定性策略是$a=\mu_\theta(s)$，所以它的“要最大化什么”常写成： $$ J(\mu_\theta) = \mathbb{E}_{s\sim\rho^{\mu_\theta}} \left[ Q^{\mu_\theta} \left( s, \mu_\theta(s) \right) \right] $$
求导：
$$  
\nabla_{\theta^\mu}J  
\approx  
\mathbb{E}_{s\sim\rho^\beta}\left[\nabla_a Q(s,a\mid\theta^Q)\big|_{a=\mu(s)}\ \nabla_{\theta^\mu}\mu(s\mid\theta^\mu)\right]  
$$

---

### 5.2.6 Trust Region / PPO（约束/近端）

**TRPO-1（约束形式：maximize surrogate s.t. 平均 KL ≤ δ）**
TRPO/PPO 的 surrogate objective 在 old policy 附近的一阶信息，就是标准 policy gradient。
5.2.4那条 $\widehat{g}$​ 只回答了：**往哪个方向走**（direction）。  
TRPO/PPO 额外回答：**一步能走多远、怎么避免策略崩掉**（step size / trust region）。
$$  
\max_\theta\ \widehat{\mathbb{E}}_t\left[\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}\widehat{A}_t\right]  
\quad  
\text{s.t. }\widehat{\mathbb{E}}_t\left[\mathrm{KL}\big(\pi_{\theta_{\text{old}}}(\cdot\mid s_t),\pi_\theta(\cdot\mid s_t)\big)\right]\le \delta  
$$

**PPO-1（clip 形式：$L^{CLIP}$）**  
$$  
L^{\text{CLIP}}(\theta)=\widehat{\mathbb{E}}_t\left[\min\Big(r_t(\theta)\widehat{A}_t,\ \mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\widehat{A}_t\Big)\right]  
$$

**PPO-2（ratio 定义）**  
$$  
r_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}  
$$

---

### 5.2.7 GAE（优势估计器：把“critic 的 TD 残差”串起来）

**GAE-1（GAE 主公式）**  
$$  
\widehat{A}^{\text{GAE}(\gamma,\lambda)}_t=\sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}  
$$

**GAE-2（δ 的定义：和 TD-1 一致）**  
$$  
\delta_t=R_t+\gamma V(s_{t+1})-V(s_t)  
$$

---

### 5.2.8 Off-policy 修正：IS / V-trace

#### **IS-1（重要性采样的基本恒等式）**  
$$  
\mathbb{E}_{z\sim \pi_{\text{tar}}}[f(z)]
=
\mathbb{E}_{z\sim \pi_{\text{beh}}}\left[\frac{\pi_{\text{tar}}(z)}{\pi_{\text{beh}}(z)}f(z)\right]  
$$

#### **VTRACE-1（V-trace target：$v_s$）**  
$$  
v_s
=
V(x_s)  
+  
\sum_{t=s}^{s+n-1}\gamma^{t-s}\left(\prod_{i=s}^{t-1}c_i\right)\rho_t\Big(r_t+\gamma V(x_{t+1})-V(x_t)\Big)  
$$
令$n=3$，则： $$ v_s = V(x_s) + \underbrace{\rho_s \Delta_s}_{t=s} + \underbrace{\gamma c_s \rho_{s+1} \Delta_{s+1}}_{t=s+1} + \underbrace{\gamma^2 c_s c_{s+1} \rho_{s+2} \Delta_{s+2}}_{t=s+2} $$ 其中 $$ \Delta_t = r_t + \gamma V(x_{t+1}) - V(x_t) $$ 你会看到它就是: 第 0 步 TD 误差 + 第 1 步 TD 误差 (打折并衰减) + 第 2 步 TD 误差 (再衰减) …… 衰减链条就是那串 $\prod c_i$。

**VTRACE-2（截断权重）**  
$$  
\rho_t=\min\left(\bar\rho,\frac{\pi(a_t\mid x_t)}{\mu(a_t\mid x_t)}\right),  
\qquad  
c_t=\min\left(\bar c,\frac{\pi(a_t\mid x_t)}{\mu(a_t\mid x_t)}\right)  
$$
$\rho_t$​ 是 **off-policy 纠偏（截断的重要性采样比）**,$c_{i}$​ 是 **“trace cutting（迹截断）系数”**：控制“把后面第 t 步的误差往回传到 s 的力度”

---

### 5.2.9 最大熵 / SAC（把“熵/KL”当成正则对象） 

**ENT-1（最大熵目标的典型写法）**  
经典 RL（不带熵）目标是最大化期望回报。最大熵 RL 在每一步额外加一个“随机性奖励”（熵奖励）：
$$  
J(\pi)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t\Big(r(s_t,a_t)+\alpha\mathcal{H}(\pi(\cdot\mid s_t))\Big)\right]  
$$
一旦目标变了，V/Q 的定义也会变（这就是“不熟”的地方） 最大熵 RL 里，值函数把“未来每一步的熵奖励”也算进去了。OpenAI Spinning Up 给了清晰定义： （核心等价关系）把熵展开： $$ \mathcal{H}(\pi(\cdot|s)) = \mathbb{E}_{a\sim\pi}[-\log \pi(a|s)] $$于是有： $$ V^\pi(s) = \mathbb{E}_{a\sim\pi}\left[ Q^\pi(s,a) - \alpha \log \pi(a|s) \right] $$这条式子就是“soft”的来源：不是简单 $V = \mathbb{E}[Q]$，而是 $V = \mathbb{E}[Q - \alpha \log \pi]$。 

**SAC-1（soft Q 的 Bellman 回归 target）**  
$$  
\widehat{Q}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\big[V_{\bar\psi}(s_{t+1})\big]  
$$

**SAC-2（Q 回归损失）**  
$$  
J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim D}\left[\frac12\Big(Q_\theta(s_t,a_t)-\widehat{Q}(s_t,a_t)\Big)^2\right]  
$$

**SAC-3（V 的回归损失：V 逼近 soft value）**  
$$  
J_V(\psi)=\mathbb{E}_{s_t\sim D}\left[\frac12\Big(V_\psi(s_t)-\mathbb{E}_{a_t\sim\pi_\phi}\big[Q_\theta(s_t,a_t)-\log\pi_\phi(a_t\mid s_t)\big]\Big)^2\right]  
$$

---

### 5.2.10 RLHF / LLM 常见：KL 正则 + 奖励模型

#### **RM-1（奖励模型 pairwise loss：偏好学习）**  
$$  
\mathrm{loss}(\theta)
=
-\frac{1}{\binom{K}{2}},  
\mathbb{E}_{(x,y_w,y_l)\sim D}\Big[\log\sigma\big(r_\theta(x,y_w)-r_\theta(x,y_l)\big)\Big]  
$$

#### **RLHF-1（InstructGPT 里写出来的“奖励 + per-token KL 惩罚 +（可选）预训练混合”）**  
$$  
\mathrm{objective}(\phi)
=
\mathbb{E}_{(x,y)\sim D_{\pi^{\text{RL}}_\phi}}  
\Big[r_\theta(x,y)-\beta\log\frac{\pi^{\text{RL}}_\phi(y\mid x)}{\pi^{\text{SFT}}(y\mid x)}\Big]  
+  
\gamma\mathbb{E}_{x\sim D_{\text{pretrain}}}\big[\log\pi^{\text{RL}}_\phi(x)\big]  
$$

---

### 5.2.11 Distributional RL（对象变成“回报分布”） 

>1.这里的$Z(s,a)$ 不是一个数，而是一个随机变量/分布，它表示：从 (x,a) 出发、之后按策略走下去得到的**折扣累计回报**的“随机结果”。Bellemare 等把它叫 **random return / value distribution**；式子里的 $D$ 不是“数值相等”，而是“分布相同”。
>2.假设在某个$s,a$下，立刻奖励是： - +1（概率0.5） - -1（概率0.5） 且$\gamma=0$ 那么： 
>	- 传统RL学到：$$Q(s,a) = \mathbb{E}[r] = 0$$    - 分布式RL学到：$$Z(s,a) = \begin{cases} +1, & p=0.5 \\ -1, & p=0.5 \end{cases}$$均值一样是0，但分布告诉你“这个动作有一半会亏一半会赚”。这就是分布提供的额外信息。

>$T^\pi$ 是“分布式 Bellman 算子（policy evaluation operator）”：把一个分布函数 $Z$ 映射成另一个分布函数。 
>论文里它是这么搭出来的（更接近原文结构）： 先定义“下一步状态-动作带来的随机性”： $$ (P^\pi Z)(x,a) \stackrel{D}{=} Z(X',A'),\ X'\sim P(\cdot|x,a),\ A'\sim\pi(\cdot|X') $$ 再定义： $$ (T^\pi Z)(x,a) \stackrel{D}{=} R(x,a) + \gamma (P^\pi Z)(x,a) $$ 这两步是 Bellemare 等人在 policy evaluation 部分给出的标准定义。

**DIST-1（分布式 Bellman：随机变量等分布，真实的回报随机变量（value distribution）满足一个分布意义的递推）**  
$$  
Z^\pi(x,a)\ \stackrel{D}{=}\ R(x,a)+\gamma Z(X',A')  
$$

**DIST-2（分布式 Bellman operator 形式）**  
它的意思是：给你一个“候选分布函数” $Z$，我通过“一步奖励 + 折扣后的下一步分布”构造一个新的分布函数 $T^\pi Z$。这就是论文里对 $T^\pi$ 的正式定义（他们还先定义了 $P^\pi$ 再写 $T^\pi$）。
$$  
(T^\pi Z)(x,a)\ \stackrel{D}{=}\ R(x,a)+\gamma Z(X',A'),\quad X'\sim P(\cdot\mid x,a),\ A'\sim\pi(\cdot\mid X')  
$$

---

### 5.2.12 Offline RL：CQL（“把 Q 压保守”）

**CQL-1（离散动作版：Bellman 误差 + log-sum-exp 保守项）**  
>$\pi_\beta(a|s)$：真实的（未知的）行为策略，产生数据集 $D$。“$\beta$ 表示 behavior policy，数据集从它来采样”。 
 $\hat{\pi}_\beta(a|s)$：用有限数据集 $D$ 对 $\pi_\beta$ 做出来的经验估计。论文同一段话里写了“在所有状态上，让它表示经验行为策略”。 
$$  
\min_Q\  
\alpha\mathbb{E}_{s\sim D}\Big[\log\sum_a \exp(Q(s,a))-\mathbb{E}_{a\sim \hat\pi_\beta(\cdot\mid s)}[Q(s,a)]\Big]  
+  
\frac12\mathbb{E}_{(s,a,s')\sim D}\Big[\big(Q(s,a)-\widehat{\mathcal{B}}^{\pi_k}\widehat{Q}_k(s,a)\big)^2\Big]  
$$

1.保守正则项（前面那坨） $$ \alpha \mathbb{E}_{s\sim D} \left[ \log \sum_{a} \exp(Q(s,a)) - \mathbb{E}_{a\sim\hat{\pi}_\beta(\cdot|s)} \left[ Q(s,a) \right] \right] $$$\log \sum_a \exp(Q(s,a)) = \text{softmax} / \text{soft-max}$： 只要有某个动作的$Q$特别大（常见于OOD动作被网络胡乱高估），这一项就会变大。 你在最小化它 ⇨ 等价于“把所有动作里最夸张的$Q$压下去”，尤其是那些数据没见过的。 
$-\mathbb{E}_{a\sim\hat{\pi}_\beta(\cdot|s)} \left[ Q(s,a) \right]$ 是“给数据动作开后门”： 因为是负号，最小化整体时，会倾向于让数据里出现过的动作$Q$更大（至少别被压下去）。 所以前面这一坨的净效果就是： “数据内动作 Q 相对更高，数据外动作 Q 相对更低（保守）”。 
2.Bellman 误差（后面那坨）
$$ \frac{1}{2} \mathbb{E} \left[ \left( Q - \hat{B}^\pi \hat{Q}^k \right)^2 \right] $$ 就是让$Q$至少仍然“像”一个可自举的价值函数（不然你光压$Q$，会变成全是小数，策略也学不到东西）。 

> [!暂时没看]
> ### 5.2.13 Occupancy / LP（把策略换成“流量变量”）

**OCC-1（discounted occupancy 的常用定义）**  
$$  
\rho_\pi(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^t\Pr_\pi(s_t=s,a_t=a)  
$$

**LP-1（MDP 对偶/占用测度形式之一：Stanford 讲义里的 µ 版本）**  
$$  
\max_{{\mu^a\ge 0}}\ \sum_{a\in A}(r^a)^\top\mu^a  
\quad  
\text{s.t. }\sum_{a\in A}\big(I-\gamma(P^a)^\top\big)\mu^a=e  
$$

---

### 5.2.14 Model-based（模型派最核心就两步：学模型 + 用模型算回报）

**MB-1（学动力学：最大似然/监督学习形式）**  
$$  
\min_\phi\ \mathbb{E}_{(s,a,s')\sim D}\big[-\log p_\phi(s',r\mid s,a)\big]  
$$

**MB-2（用模型做规划：在模型里最大化回报）**  
$$  
\max_{\pi}\ \mathbb{E}_{\tau\sim \pi,\ p_\phi}\left[\sum_{t=0}^{T}\gamma^t r_\phi(s_t,a_t)\right]  
$$

---

## 5.3 现代 LLM RL：GRPO / GSPO / SAPO / DAPO（你要拆的新东西都在“PPO-CLIP 模板”里换零件）

### 5.3.1 GRPO（DeepSeekMath：token-level ratio + group advantage + KL）
#### **GRPO-1（GRPO 主目标：论文式子结构）**  
$$  
J_{\text{GRPO}}(\theta)

\mathbb{E}_{(q,a)\sim D,\ {o_i}_{i=1}^G\sim \pi_{\theta_{\text{old}}}(\cdot\mid q)}  
\left[  
\frac1G\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}  
\min\Big(r_{i,t}(\theta)\widehat{A}_{i,t},\ \mathrm{clip}(r_{i,t}(\theta),1-\epsilon,1+\epsilon)\widehat{A}_{i,t}\Big)  
-\beta,D_{\mathrm{KL}}(\pi_\theta\Vert \pi_{\mathrm{ref}})  
\right]  
$$

#### **GRPO-2（token-level ratio）**  
$$  
r_{i,t}(\theta)=\frac{\pi_\theta(o_{i,t}\mid q,o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,<t})}  
$$

#### **GRPO-3（组内优势：同一条 response 的所有 token 共享同一个 advantage）**  
$$  
\widehat{A}_{i,t}=\widehat{A}_i

\frac{r_i-\mathrm{mean}({r_j}_{j=1}^G)}{\mathrm{std}({r_j}_{j=1}^G)}  
$$

---

### 5.3.2 GSPO（Qwen：sequence-level ratio + sequence clip + length normalize）

#### **GSPO-1（sequence-level clip objective）**  
$$  
J_{\text{GSPO}}(\theta)

\mathbb{E}_{x\sim D,\ {y_i}_{i=1}^G\sim \pi_{\theta_{\text{old}}}(\cdot\mid x)}  
\left[  
\frac1G\sum_{i=1}^G  
\min\Big(s_i(\theta)\widehat{A}_i,\ \mathrm{clip}(s_i(\theta),1-\epsilon,1+\epsilon)\widehat{A}_i\Big)  
\right]  
$$

#### **GSPO-2（sequence ratio 的 length-normalized 形式：几何平均）**  
$$ s_i(\theta) = \left( \frac{\pi_\theta(y_i|x)}{\pi_{\theta_{old}}(y_i|x)} \right)^{1/|y_i|} = \exp\left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})} \right) $$

---

### 5.3.3 SAPO（Soft Adaptive：把 hard clip 的 min 改成“软门控混合”） 

#### **SAPO-1（统一 surrogate：用 f_{i,t} 代替硬 min）**  
$$  
J_{\text{SAPO}}(\theta)

\mathbb{E}_{x\sim D,\ {y_i}_{i=1}^G\sim \pi_{\theta_{\text{old}}}(\cdot\mid x)}  
\left[  
\frac1G\sum_{i=1}^G\frac1{|y_i|}\sum_{t=1}^{|y_i|} f_{i,t}!\left(r_{i,t}(\theta)\right),\widehat{A}_{i,t}  
\right]  
$$

#### **SAPO-2（f：在 clipped / unclipped 两个方向之间做“软切换”）**  
$$  
f_{i,t}!\left(r_{i,t}(\theta)\right)

(1-\lambda_t),\min!\Big(r_{i,t},\ \mathrm{clip}(r_{i,t},1-\epsilon,1+\epsilon)\Big)  
+  
\lambda_t,\max!\Big(r_{i,t},\ \mathrm{clip}(r_{i,t},1-\epsilon,1+\epsilon)\Big)  
$$

#### **SAPO-3（门控 λ_t：随优势自适应）**  
$$  
\lambda_t=\frac12\tanh!\big(\eta(\widehat{A}_{i,t}-b_t)\big)+\frac12  
$$

---

### 5.3.4 DAPO（Decoupled clip + Dynamic sampling + Token-level loss）

#### **DAPO-1（decoupled clip：把上下 clip 分成 ε_low / ε_high）**  
$$  
J_{\text{DAPO}}(\theta)

\mathbb{E}_{(q,a)\sim D,\ {o_i}_{i=1}^G\sim \pi_{\theta_{\text{old}}}(\cdot\mid q)}  
\left[  
\sum_{i=1}^G\sum_{t=1}^{|o_i|}  
\min\Big(r_{i,t}(\theta)\widehat{A}_{i,t},\ \mathrm{clip}(r_{i,t}(\theta),1-\epsilon_{\text{low}},1+\epsilon_{\text{high}})\widehat{A}_{i,t}\Big)  
\right]  
$$

#### **DAPO-2（Dynamic sampling 的“过滤条件”写成约束：保证组内既有对也有错，避免全 0 梯度）**  
$$  
0\ <\ \big|{,o_i\mid \mathrm{is_equivalent}(a,o_i),}\big|\ <\ G  
$$

#### **DAPO-3（Token-level policy gradient loss：强调“按 token 汇总”，而不是先对每条 sample 平均再平均）**  
$$  
J_{\text{DAPO}}(\theta)

\mathbb{E}\left[  
\sum_{i=1}^G\sum_{t=1}^{|o_i|}  
\min\Big(r_{i,t}(\theta)\widehat{A}_{i,t},\ \mathrm{clip}(r_{i,t}(\theta),1-\epsilon_{\text{low}},1+\epsilon_{\text{high}})\widehat{A}_{i,t}\Big)  
\right]  
$$

