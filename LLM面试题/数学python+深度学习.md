### 1. ==说一下你对kl散度的理解？kl散度和交叉熵的区别和联系是什么？== 

KL散度表达式：

$D_{KL}(P\parallel Q)=\sum_iP(i)\log\frac{P(i)}{Q(i)}$ 中，

第一个参数 P 是基准，第二个参数 Q 是我们的模型。，KL散度衡量的是：“**当我们身处 P 的世界里时，用 Q 的模型来解释这个世界，会让我们付出多大的代价。**”

KL 散度有一个非常重要的特性：**不对称性**。 `D_KL(P || Q) ≠ D_KL(Q || P)`

- `D_KL(P || Q)`：用“地图 Q”来描述“现实 P”所造成的信息损失。
- `D_KL(Q || P)`：用“现实 P”来描述“地图 Q”所造成的信息损失。

这两种损失通常是不同的。这就好比用“纽约地图”在“东京”导航，和用“东京地图”在“纽约”导航，犯的错误是不一样的。因此，我们称之为“散度”，而不是“距离”。`P(i)` 较大的项（真实世界中更重要的事件）所产生的**正向惩罚**，会**系统性地超过**那些 `P(i)` 较小的项所带来的负向奖励。



**交叉熵  = 熵  + KL 散度**：$H(P,Q)=H(P)+D_{KL}(P||Q)$

​					   $H(P,Q)=-\sum_iP(i)\log(Q(i))$

我们来拆解这个公式：

- **熵 `H(P)`**: 代表**真实分布 P** 的不确定性。在信息论中，它表示编码来自 P 的事件所需要的**最优平均比特数**。这是理论上的**最短编码长度**。对于一个给定的真实分布 P，它的熵 `H(P)` 是一个**常数**。
- **KL 散度 `D_KL(P || Q)`**: 正如我们上面所说，这是使用**非最优的分布 Q** 来编码 P 时，所产生的**额外比特数**或**信息损失**。
- **交叉熵 `H(P, Q)`**: 代表使用**基于 Q 的编码方式**来编码来自**真实分布 P** 的事件时，所需要的**实际平均比特数**。
- 因为最小化交叉熵就等于最小化KL散度，且**计算更简单**，所以被用作**损失函数**。

### 2. ==如何选取模型中的超参数，有什么办法？==

- **随机搜索**：为每个超参数定义一个**分布**，在预设的尝试次数内（比如100次），**每次都从这些分布中随机采样**一组超参数组合来进行训练和评估。

- **贝叶斯优化**：随机选择几组（比如3-5组）超参数 `x`，分别用这几组超参数完整地训练模型，得到对应的性能得分 `f(x)`。我们现在有了一个小小的初始数据集 `D = {(x₁, f(x₁)), (x₂, f(x₂)), ...}`。 使用当前所有的数据 `D`，拟合一个高斯过程模型：普通函数在每个输入点 `x` 只会输出一个确定的值 `y`。而高斯过程在每个输入点 `x` 输出的是一个**概率分布**——具体来说是一个**高斯（正态）分布**。这个分布由**均值和方差定义**。我们得到了一个覆盖整个超参数空间的“信念地图”（均值和方差）。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。在“信念地图”上，使用采集函数（如EI）进行评估，找到能使采集函数值最大的那组新超参数 `x_next`。更新，重复。

- 基于**早停**的方法：代表算法:是**Successive Halving** 和 **Hyperband**。随机生成大量的超参数组合。

  给所有组合分配一个**最小的资源**（比如只训练1个epoch）。淘汰掉表现最差的一半组合。给剩下的一半组合分配**更多的资源**（比如再训练2个epoch）。再淘汰掉一半...重复这个“分配资源-淘汰差生”的过程，直到只剩下一个“冠军”组合，并对其进行完整训练。

### 3. ==正则化==

**正则化防止过拟合**：让很多个特征都贡献一点点（拥有较小的权重），而不是让少数几个特征拥有巨大的权重，使得权重分散。

| 特性         | L2 正则化 (Ridge)        | L1 正则化 (Lasso)                        |
| ------------ | ------------------------ | ---------------------------------------- |
| 惩罚项公式   | λ * ∑ wᵢ² (平方和)       | `λ * ∑                                   |
| 对权重的影响 | 使权重变小，趋近于0      | 使权重变小，很多会精确等于0              |
| 模型稀疏性   | 否，权重通常是小的非零值 | 是，产生稀疏的权重矩阵                   |
| 核心思想     | 鼓励权重分散、平滑       | 特征选择，保留最重要的特征               |
| 主要用途     | 通用的、首选的正则化方法 | 当特征数量巨大且怀疑很多特征是无用的时候 |

L1正则化：**惩罚是“一视同仁”的**：惩罚的力度与权重大小成正比。一个权重从 0.1 变为 0，和从 10.1 变为 10，所获得的“奖励”（惩罚的减少量）是相同的。

### 4. ==在参数初始化时，为什么不能全零初始化==

![500](assets/数学python+深度学习/file-20251204084206136.png)
### 5. ==死亡ReLU==

在训练过程中，如果某一次梯度更新的幅度特别大（比如学习率设置得太高），可能会导致权重 `W` 和偏置 `b` 被更新成一组“糟糕”的值。这组“糟糕”的值，使得对于**之后所有输入到这个神经元的数据 `x`**，计算出的进入激活函数前的 `z` **始终小于0**。反向传播的梯度计算是**一路连乘**的。ReLU函数在负数区间的**导数（斜率）恰好为0**。

### 6. ==1×1卷积核的作用==

首先：一个卷积核，无论它的高和宽是多少（`1x1`, `3x3`, `5x5`），它的唯一使命就是：作为一个特征探测器，扫描整个输入数据，并将所有输入通道的信息“浓缩”成一张代表“该特征出现位置”的单通道输出图。对于多通道的图像，卷积核最后会把所有通道都加起来。**卷积核是3D的**，**一个卷积核卷完后得到的特征图是1D的**。

那么1×1卷积核的作用：

- **降维或者升维**。

- **增加非线性**：一个 1x1 的卷积，后面再接一个 ReLU，相当于在不改变特征图尺寸的情况下，对输入的每一个像素点进行了一次非线性的变换。

- **跨通道信息交互**：将输入的所有 `C` 个通道的值进行一次**加权求和**。

### 7. ==深度可分离卷积==

| 特性     | 标准卷积                     | 深度可分离卷积                                       |
| -------- | ---------------------------- | ---------------------------------------------------- |
| 工作方式 | 一步到位，同时处理空间和通道 | 分两步走，先处理空间，再处理通道                     |
| 步骤     | 单一的标准卷积               | 1. 深度卷积 (Depthwise) 2. 逐点卷积 (Pointwise, 1x1) |
| 核心思想 | 耦合操作                     | 解耦操作                                             |
| 主要优势 | 表达能力强                   | 参数少，计算快，极其高效                             |
| 代表模型 | VGG, ResNet (早期)           | MobileNet, Xception, EfficientNet                    |

### 8.==GAN是怎么训练的==

生成对抗网络(GAN)的训练过程包括两个部分:生成器的训练和判别器的训练。这两个部分交替进行,形成一
种“对抗”的训练过程。

**判别器**训练:首先,我们固定生成器的参数,训练判别器。具体来说,我们首先从真实数据集中采样一批真实样
本,然后从生成器中生成一批假样本。我们希望判别器能够正确地区分这两批样本,因此我们通过最大化判别器对
真实样本的输出和最小化判别器对假样本的输出来更新判别器的参数。

**生成器**训练:然后,我们固定判别器的参数,训练生成器。我们希望生成器能够生成能够“欺骗”判别器的假样
本,因此我们通过最大化判别器对生成器生成的假样本的输出来更新生成器的参数。

循环训练。

### 9.==word2vec提出了负采样策略，他的原理是什么，解决了什么样的问题？==

Word2Vec不是一个单一的算法，而是 Google 在 2013 年提出的一个**模型工具集**，其主要目的是将词语转换成向量。这种由模型产生的词语向量，通常被称为**词嵌入 (Word Embedding)**。

Word2Vec 的哲学基础是语言学中的分布式假设：**一个词的意义，由它上下文中的词语所决定 **。

Word2Vec 工具集中包含两种主要的训练模型：

- **CBOW (Continuous Bag-of-Words)**：**根据上下文词预测中心词**。比如，给出 `["the", "cat", "on", "the", "mat"]`，模型需要预测中间的词是 `sat`。
- **Skip-gram**: **根据中心词预测上下文词**。比如，给出中心词 `sat`，模型需要预测出周围的词可能是 `the`, `cat`, `on`, `the`, `mat`。Skip-gram 在处理罕见词时效果通常更好。



CBOW 使用负采样的详细训练步骤：

第 1 步：准备“正负样本对”

1. **构造正样本 (Positive Sample)**:
   - 我们从文本中取出一个真实的配对。
   - **上下文**: `["the", "quick", "brown", "jumps"]`
   - **中心词**: `"fox"`
   - 这就构成了一个**正样本** `( (["the", "quick", "brown", "jumps"]), "fox" )`，我们给它打上标签 **1**。
2. **构造负样本 (Negative Samples)**:
   - 我们保持**上下文不变**。
   - 从词汇表中**随机**抽取 `k` 个噪声词，作为“假的”中心词。比如，我们抽到了 `["cat", "apple", "sky", "road"]`。
   - 这就构成了 `k` 个**负样本**:
     - `( (["the", "quick", "brown", "jumps"]), "cat" )`， 标签 **0**
     - `( (["the", "quick", "brown", "jumps"]), "apple" )`，标签 **0**
     - `( (["the", "quick", "brown", "jumps"]), "sky" )`， 标签 **0**
     - `( (["the", "quick", "brown", "jumps"]), "road" )`， 标签 **0**

现在，我们有了一个小型的、包含 `1+k` 个样本的训练集，可以开始训练了。

第 2 步：前向传播（Vector 运算）

这是 CBOW 与 Skip-gram 不同的关键之处。

1. **计算上下文向量 `v_context`**:
   - 模型会去**输入词向量矩阵 `W_in`** 中，查找所有上下文词 `["the", "quick", "brown", "jumps"]` 对应的词向量。
   - 然后，将这些词向量**求平均**，得到一个单一的、代表了整体上下文语义的向量 `v_context`。 `v_context = Average(v_the, v_quick, v_brown, v_jumps)`
2. **计算相似度分数**:
   - 模型会去**输出词向量矩阵 `W_out`** 中，查找我们准备好的正负样本的目标词（`"fox"`, `"cat"`, `"apple"`...）对应的词向量。
   - 用 `v_context` 分别与这些目标词的向量进行**点积**运算，得到一个分数。
     - `score_positive = v_context · v_out_fox`
     - `score_negative_1 = v_context · v_out_cat`
     - ...
3. **转换为概率**:
   - 将每个分数都通过一个 **Sigmoid 函数**，将分数转换成一个 `[0, 1]` 之间的概率值。这个概率值代表了“模型认为这是一个真实例子的可能性有多大”。

第 3 步：反向传播（学习与更新）

1. **计算损失**:
   - 对于正样本，模型希望输出的概率接近 `1`。
   - 对于所有负样本，模型希望输出的概率接近 `0`。
   - 根据这个目标，模型会计算出总的二元交叉熵损失。
2. **更新向量**:
   - 根据损失，反向传播算法会去更新所有参与了计算的向量。具体包括：
     - **`W_in`** 矩阵中，所有上下文词（`"the"`, `"quick"`, `"brown"`, `"jumps"`）的词向量。
     - **`W_out`** 矩阵中，正样本中心词（`"fox"`）和所有负样本词（`"cat"`, `"apple"`...）的词向量。

这个过程不断重复，模型会逐渐学会：当 `Average(v_the, v_quick, v_brown, v_jumps)` 这个上下文出现时，它在向量空间中应该与 `v_out_fox` 非常接近，而与 `v_out_cat`, `v_out_apple` 等随机词非常疏远。

### 10.==如果让你设计一个命名实体识别任务，你会怎么设计？==

